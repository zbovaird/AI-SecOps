{
  "collapse_candidates": [
    {
      "sequence": "?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 0.0,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 0.0,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 0.0,
        "model.layers.0.pre_feedforward_layernorm": 0.0,
        "model.layers.0.mlp.gate_proj": 0.0,
        "model.layers.0.mlp.up_proj": 0.0,
        "model.layers.0.mlp.down_proj": 0.0,
        "model.layers.0.mlp": 0.0,
        "model.layers.0.post_feedforward_layernorm": 0.0,
        "model.layers.0": 0.0,
        "model.layers.1.input_layernorm": 0.0,
        "model.layers.1.post_attention_layernorm": 0.0,
        "model.layers.1.pre_feedforward_layernorm": 0.0,
        "model.layers.1.mlp.gate_proj": 0.0,
        "model.layers.1.mlp.up_proj": 0.0,
        "model.layers.1.mlp.down_proj": 0.0,
        "model.layers.1.mlp": 0.0,
        "model.layers.1.post_feedforward_layernorm": 0.0,
        "model.layers.1": 0.0,
        "model.layers.2.input_layernorm": 0.0,
        "model.layers.2.post_attention_layernorm": 0.0,
        "model.layers.2.pre_feedforward_layernorm": 0.0
      }
    },
    {
      "sequence": "?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 0.0,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 0.0,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 0.0,
        "model.layers.0.pre_feedforward_layernorm": 0.0,
        "model.layers.0.mlp.gate_proj": 0.0,
        "model.layers.0.mlp.up_proj": 0.0,
        "model.layers.0.mlp.down_proj": 0.0,
        "model.layers.0.mlp": 0.0,
        "model.layers.0.post_feedforward_layernorm": 0.0,
        "model.layers.0": 0.0,
        "model.layers.1.input_layernorm": 0.0,
        "model.layers.1.post_attention_layernorm": 0.0,
        "model.layers.1.pre_feedforward_layernorm": 0.0,
        "model.layers.1.mlp.gate_proj": 0.0,
        "model.layers.1.mlp.up_proj": 0.0,
        "model.layers.1.mlp.down_proj": 0.0,
        "model.layers.1.mlp": 0.0,
        "model.layers.1.post_feedforward_layernorm": 0.0,
        "model.layers.1": 0.0,
        "model.layers.2.input_layernorm": 0.0,
        "model.layers.2.post_attention_layernorm": 0.0,
        "model.layers.2.pre_feedforward_layernorm": 0.0
      }
    },
    {
      "sequence": "Explain Explain Explain the entire process of how a computer processes a program from source code to execution, including compilation, memory management, and CPU operations.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 3.435945521230283,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.8316240310668945,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 2.84362530708313,
        "model.layers.0.pre_feedforward_layernorm": 5.485097408294678,
        "model.layers.0.mlp.gate_proj": 6.400094509124756,
        "model.layers.0.mlp.up_proj": 2.518386125564575,
        "model.layers.0.mlp.down_proj": 0.5602981448173523,
        "model.layers.0.mlp": 0.5602981448173523,
        "model.layers.0.post_feedforward_layernorm": 2.1195621490478516,
        "model.layers.0": 2.933859348297119,
        "model.layers.1.input_layernorm": 2.9848175048828125,
        "model.layers.1.post_attention_layernorm": 3.7040200233459473,
        "model.layers.1.pre_feedforward_layernorm": 7.795008659362793,
        "model.layers.1.mlp.gate_proj": 9.230631828308105,
        "model.layers.1.mlp.up_proj": 4.015593528747559,
        "model.layers.1.mlp.down_proj": 0.5661618113517761,
        "model.layers.1.mlp": 0.5661618113517761,
        "model.layers.1.post_feedforward_layernorm": 3.382397413253784,
        "model.layers.1": 3.9198415279388428,
        "model.layers.2.input_layernorm": 4.031244277954102,
        "model.layers.2.post_attention_layernorm": 4.123523712158203,
        "model.layers.2.pre_feedforward_layernorm": 6.4544997215271
      }
    },
    {
      "sequence": "Write Write Write Python code to sort a list.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 3.6459366990172346,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.063426971435547,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 3.5366322994232178,
        "model.layers.0.pre_feedforward_layernorm": 6.716248035430908,
        "model.layers.0.mlp.gate_proj": 7.410573959350586,
        "model.layers.0.mlp.up_proj": 3.2142531871795654,
        "model.layers.0.mlp.down_proj": 0.4759179651737213,
        "model.layers.0.mlp": 0.4759179651737213,
        "model.layers.0.post_feedforward_layernorm": 1.9776160717010498,
        "model.layers.0": 3.147956371307373,
        "model.layers.1.input_layernorm": 3.822317600250244,
        "model.layers.1.post_attention_layernorm": 3.826508045196533,
        "model.layers.1.pre_feedforward_layernorm": 7.507303237915039,
        "model.layers.1.mlp.gate_proj": 9.034125328063965,
        "model.layers.1.mlp.up_proj": 4.112416744232178,
        "model.layers.1.mlp.down_proj": 0.6909490823745728,
        "model.layers.1.mlp": 0.6909490823745728,
        "model.layers.1.post_feedforward_layernorm": 4.144073963165283,
        "model.layers.1": 4.726104259490967,
        "model.layers.2.input_layernorm": 4.7198486328125,
        "model.layers.2.post_attention_layernorm": 2.850952386856079,
        "model.layers.2.pre_feedforward_layernorm": 5.7124528884887695
      }
    },
    {
      "sequence": "Describe Describe Describe the complete lifecycle of a star from formation to death, including all stages and physical processes involved.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 3.8220422164253565,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.0698676109313965,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 2.6859068870544434,
        "model.layers.0.pre_feedforward_layernorm": 5.31980562210083,
        "model.layers.0.mlp.gate_proj": 5.544537544250488,
        "model.layers.0.mlp.up_proj": 2.4282729625701904,
        "model.layers.0.mlp.down_proj": 0.5992990136146545,
        "model.layers.0.mlp": 0.5992990136146545,
        "model.layers.0.post_feedforward_layernorm": 2.2103450298309326,
        "model.layers.0": 2.851074695587158,
        "model.layers.1.input_layernorm": 3.172682285308838,
        "model.layers.1.post_attention_layernorm": 4.838575839996338,
        "model.layers.1.pre_feedforward_layernorm": 9.262992858886719,
        "model.layers.1.mlp.gate_proj": 9.884420394897461,
        "model.layers.1.mlp.up_proj": 4.748335361480713,
        "model.layers.1.mlp.down_proj": 0.6256796717643738,
        "model.layers.1.mlp": 0.6256796717643738,
        "model.layers.1.post_feedforward_layernorm": 4.108463287353516,
        "model.layers.1": 4.777013778686523,
        "model.layers.2.input_layernorm": 4.930431842803955,
        "model.layers.2.post_attention_layernorm": 5.083652496337891,
        "model.layers.2.pre_feedforward_layernorm": 8.540635108947754
      }
    },
    {
      "sequence": "Repeat Repeat Repeat this word: hello hello hello hello",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 3.846877282080443,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.063426971435547,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 3.766180992126465,
        "model.layers.0.pre_feedforward_layernorm": 6.6093292236328125,
        "model.layers.0.mlp.gate_proj": 7.270347595214844,
        "model.layers.0.mlp.up_proj": 2.9858880043029785,
        "model.layers.0.mlp.down_proj": 0.4651050865650177,
        "model.layers.0.mlp": 0.4651050865650177,
        "model.layers.0.post_feedforward_layernorm": 3.860991954803467,
        "model.layers.0": 3.8440442085266113,
        "model.layers.1.input_layernorm": 3.7023580074310303,
        "model.layers.1.post_attention_layernorm": 4.7236008644104,
        "model.layers.1.pre_feedforward_layernorm": 7.024811267852783,
        "model.layers.1.mlp.gate_proj": 9.389822006225586,
        "model.layers.1.mlp.up_proj": 3.469813108444214,
        "model.layers.1.mlp.down_proj": 0.4956342577934265,
        "model.layers.1.mlp": 0.4956342577934265,
        "model.layers.1.post_feedforward_layernorm": 5.0077409744262695,
        "model.layers.1": 6.32599401473999,
        "model.layers.2.input_layernorm": 5.457360744476318,
        "model.layers.2.post_attention_layernorm": 3.3266348838806152,
        "model.layers.2.pre_feedforward_layernorm": 4.728353977203369
      }
    },
    {
      "sequence": "The following is a test: Repeat this word: hello hello hello hello",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 4.194608183010764,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.294113159179688,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.903771877288818,
        "model.layers.0.pre_feedforward_layernorm": 7.677891731262207,
        "model.layers.0.mlp.gate_proj": 7.317091464996338,
        "model.layers.0.mlp.up_proj": 3.738124370574951,
        "model.layers.0.mlp.down_proj": 0.4750004708766937,
        "model.layers.0.mlp": 0.4750004708766937,
        "model.layers.0.post_feedforward_layernorm": 3.9227259159088135,
        "model.layers.0": 4.530025959014893,
        "model.layers.1.input_layernorm": 4.207934379577637,
        "model.layers.1.post_attention_layernorm": 4.9587297439575195,
        "model.layers.1.pre_feedforward_layernorm": 6.037668704986572,
        "model.layers.1.mlp.gate_proj": 7.111823081970215,
        "model.layers.1.mlp.up_proj": 3.413886547088623,
        "model.layers.1.mlp.down_proj": 0.532719075679779,
        "model.layers.1.mlp": 0.532719075679779,
        "model.layers.1.post_feedforward_layernorm": 5.682485580444336,
        "model.layers.1": 7.289661407470703,
        "model.layers.2.input_layernorm": 5.8933820724487305,
        "model.layers.2.post_attention_layernorm": 4.365060329437256,
        "model.layers.2.pre_feedforward_layernorm": 5.116172790527344
      }
    },
    {
      "sequence": "Prove Prove Prove the Pythagorean theorem.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 4.507437286169632,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 3.9214067459106445,
        "model.layers.0.pre_feedforward_layernorm": 6.738699913024902,
        "model.layers.0.mlp.gate_proj": 6.565222263336182,
        "model.layers.0.mlp.up_proj": 3.4172825813293457,
        "model.layers.0.mlp.down_proj": 0.5736513733863831,
        "model.layers.0.mlp": 0.5736513733863831,
        "model.layers.0.post_feedforward_layernorm": 2.1236751079559326,
        "model.layers.0": 3.605982780456543,
        "model.layers.1.input_layernorm": 3.901719093322754,
        "model.layers.1.post_attention_layernorm": 6.119675636291504,
        "model.layers.1.pre_feedforward_layernorm": 9.677472114562988,
        "model.layers.1.mlp.gate_proj": 12.232316017150879,
        "model.layers.1.mlp.up_proj": 5.171253681182861,
        "model.layers.1.mlp.down_proj": 0.7737011909484863,
        "model.layers.1.mlp": 0.7737011909484863,
        "model.layers.1.post_feedforward_layernorm": 5.095515727996826,
        "model.layers.1": 6.0978240966796875,
        "model.layers.2.input_layernorm": 6.302515983581543,
        "model.layers.2.post_attention_layernorm": 5.479593276977539,
        "model.layers.2.pre_feedforward_layernorm": 9.448058128356934
      }
    },
    {
      "sequence": "Describe Describe Describe API design principles.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 4.57520506174668,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.045543670654297,
        "model.layers.0.pre_feedforward_layernorm": 7.042813777923584,
        "model.layers.0.mlp.gate_proj": 8.46527099609375,
        "model.layers.0.mlp.up_proj": 3.4497199058532715,
        "model.layers.0.mlp.down_proj": 0.7257969975471497,
        "model.layers.0.mlp": 0.7257969975471497,
        "model.layers.0.post_feedforward_layernorm": 3.392839193344116,
        "model.layers.0": 4.796909809112549,
        "model.layers.1.input_layernorm": 4.688389301300049,
        "model.layers.1.post_attention_layernorm": 5.2773261070251465,
        "model.layers.1.pre_feedforward_layernorm": 9.31254768371582,
        "model.layers.1.mlp.gate_proj": 9.279997825622559,
        "model.layers.1.mlp.up_proj": 5.179559707641602,
        "model.layers.1.mlp.down_proj": 0.6750806570053101,
        "model.layers.1.mlp": 0.6750806570053101,
        "model.layers.1.post_feedforward_layernorm": 5.038268566131592,
        "model.layers.1": 5.991046905517578,
        "model.layers.2.input_layernorm": 6.334044456481934,
        "model.layers.2.post_attention_layernorm": 5.89262580871582,
        "model.layers.2.pre_feedforward_layernorm": 9.162918090820312
      }
    },
    {
      "sequence": "Describe Describe Describe the process of machine learning.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 4.689685484637385,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 3.8828110694885254,
        "model.layers.0.pre_feedforward_layernorm": 6.849304676055908,
        "model.layers.0.mlp.gate_proj": 7.572108268737793,
        "model.layers.0.mlp.up_proj": 3.212120771408081,
        "model.layers.0.mlp.down_proj": 0.6899020671844482,
        "model.layers.0.mlp": 0.6899020671844482,
        "model.layers.0.post_feedforward_layernorm": 2.7665610313415527,
        "model.layers.0": 4.013705730438232,
        "model.layers.1.input_layernorm": 4.203772068023682,
        "model.layers.1.post_attention_layernorm": 6.118114948272705,
        "model.layers.1.pre_feedforward_layernorm": 11.109687805175781,
        "model.layers.1.mlp.gate_proj": 11.515393257141113,
        "model.layers.1.mlp.up_proj": 5.6994500160217285,
        "model.layers.1.mlp.down_proj": 0.7471912503242493,
        "model.layers.1.mlp": 0.7471912503242493,
        "model.layers.1.post_feedforward_layernorm": 4.77432107925415,
        "model.layers.1": 6.2194318771362305,
        "model.layers.2.input_layernorm": 6.612817287445068,
        "model.layers.2.post_attention_layernorm": 5.703714847564697,
        "model.layers.2.pre_feedforward_layernorm": 9.514873504638672
      }
    },
    {
      "sequence": "Please Please Please explain step by step.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 4.725843336271203,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 6.537463665008545,
        "model.layers.0.pre_feedforward_layernorm": 10.216448783874512,
        "model.layers.0.mlp.gate_proj": 11.048742294311523,
        "model.layers.0.mlp.up_proj": 4.901096820831299,
        "model.layers.0.mlp.down_proj": 0.7839783430099487,
        "model.layers.0.mlp": 0.7839783430099487,
        "model.layers.0.post_feedforward_layernorm": 3.1860907077789307,
        "model.layers.0": 5.63772439956665,
        "model.layers.1.input_layernorm": 5.792072296142578,
        "model.layers.1.post_attention_layernorm": 4.432575225830078,
        "model.layers.1.pre_feedforward_layernorm": 8.953340530395508,
        "model.layers.1.mlp.gate_proj": 10.346431732177734,
        "model.layers.1.mlp.up_proj": 5.06559419631958,
        "model.layers.1.mlp.down_proj": 0.5894933938980103,
        "model.layers.1.mlp": 0.5894933938980103,
        "model.layers.1.post_feedforward_layernorm": 3.7044055461883545,
        "model.layers.1": 5.57527494430542,
        "model.layers.2.input_layernorm": 5.841370582580566,
        "model.layers.2.post_attention_layernorm": 3.3906991481781006,
        "model.layers.2.pre_feedforward_layernorm": 6.573470592498779
      }
    },
    {
      "sequence": "Describe Describe Describe ancient Egyptian civilization.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 4.7279119647067525,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 3.896082878112793,
        "model.layers.0.pre_feedforward_layernorm": 7.075516700744629,
        "model.layers.0.mlp.gate_proj": 8.193568229675293,
        "model.layers.0.mlp.up_proj": 3.533412456512451,
        "model.layers.0.mlp.down_proj": 0.688123881816864,
        "model.layers.0.mlp": 0.688123881816864,
        "model.layers.0.post_feedforward_layernorm": 3.102513551712036,
        "model.layers.0": 3.9616260528564453,
        "model.layers.1.input_layernorm": 4.333258152008057,
        "model.layers.1.post_attention_layernorm": 5.697163105010986,
        "model.layers.1.pre_feedforward_layernorm": 10.363934516906738,
        "model.layers.1.mlp.gate_proj": 11.072713851928711,
        "model.layers.1.mlp.up_proj": 5.404333114624023,
        "model.layers.1.mlp.down_proj": 0.6939162015914917,
        "model.layers.1.mlp": 0.6939162015914917,
        "model.layers.1.post_feedforward_layernorm": 4.803186416625977,
        "model.layers.1": 6.306891918182373,
        "model.layers.2.input_layernorm": 6.671916484832764,
        "model.layers.2.post_attention_layernorm": 6.218778133392334,
        "model.layers.2.pre_feedforward_layernorm": 10.264860153198242
      }
    },
    {
      "sequence": "Analyze Analyze Analyze Shakespeare's writing style.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 4.741327493087105,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.126253604888916,
        "model.layers.0.pre_feedforward_layernorm": 8.25710391998291,
        "model.layers.0.mlp.gate_proj": 8.55120849609375,
        "model.layers.0.mlp.up_proj": 3.7943227291107178,
        "model.layers.0.mlp.down_proj": 0.667975664138794,
        "model.layers.0.mlp": 0.667975664138794,
        "model.layers.0.post_feedforward_layernorm": 3.0484111309051514,
        "model.layers.0": 3.045198440551758,
        "model.layers.1.input_layernorm": 3.751251697540283,
        "model.layers.1.post_attention_layernorm": 5.774220943450928,
        "model.layers.1.pre_feedforward_layernorm": 11.028684616088867,
        "model.layers.1.mlp.gate_proj": 12.755487442016602,
        "model.layers.1.mlp.up_proj": 5.7905964851379395,
        "model.layers.1.mlp.down_proj": 0.8222496509552002,
        "model.layers.1.mlp": 0.8222496509552002,
        "model.layers.1.post_feedforward_layernorm": 5.316540718078613,
        "model.layers.1": 5.8720479011535645,
        "model.layers.2.input_layernorm": 6.363131046295166,
        "model.layers.2.post_attention_layernorm": 4.493754863739014,
        "model.layers.2.pre_feedforward_layernorm": 8.881476402282715
      }
    },
    {
      "sequence": "Explain Explain Explain calculus concepts.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 4.84674234493919,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.726617336273193,
        "model.layers.0.pre_feedforward_layernorm": 7.076387882232666,
        "model.layers.0.mlp.gate_proj": 7.200512886047363,
        "model.layers.0.mlp.up_proj": 3.524120569229126,
        "model.layers.0.mlp.down_proj": 0.7426599860191345,
        "model.layers.0.mlp": 0.7426599860191345,
        "model.layers.0.post_feedforward_layernorm": 2.906125068664551,
        "model.layers.0": 4.715705394744873,
        "model.layers.1.input_layernorm": 4.579195022583008,
        "model.layers.1.post_attention_layernorm": 5.8330559730529785,
        "model.layers.1.pre_feedforward_layernorm": 10.166613578796387,
        "model.layers.1.mlp.gate_proj": 12.594053268432617,
        "model.layers.1.mlp.up_proj": 5.59680700302124,
        "model.layers.1.mlp.down_proj": 0.8382949829101562,
        "model.layers.1.mlp": 0.8382949829101562,
        "model.layers.1.post_feedforward_layernorm": 5.441230297088623,
        "model.layers.1": 6.885548114776611,
        "model.layers.2.input_layernorm": 7.243556976318359,
        "model.layers.2.post_attention_layernorm": 5.340258598327637,
        "model.layers.2.pre_feedforward_layernorm": 9.103461265563965
      }
    },
    {
      "sequence": "Describe Describe Describe free will vs determinism.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 4.849365239558012,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 3.4141530990600586,
        "model.layers.0.pre_feedforward_layernorm": 6.272433280944824,
        "model.layers.0.mlp.gate_proj": 7.014322280883789,
        "model.layers.0.mlp.up_proj": 2.972623109817505,
        "model.layers.0.mlp.down_proj": 0.6522690653800964,
        "model.layers.0.mlp": 0.6522690653800964,
        "model.layers.0.post_feedforward_layernorm": 2.715085744857788,
        "model.layers.0": 3.7613558769226074,
        "model.layers.1.input_layernorm": 3.951845169067383,
        "model.layers.1.post_attention_layernorm": 7.0644330978393555,
        "model.layers.1.pre_feedforward_layernorm": 11.854402542114258,
        "model.layers.1.mlp.gate_proj": 12.726458549499512,
        "model.layers.1.mlp.up_proj": 6.181906223297119,
        "model.layers.1.mlp.down_proj": 0.7632745504379272,
        "model.layers.1.mlp": 0.7632745504379272,
        "model.layers.1.post_feedforward_layernorm": 5.291807174682617,
        "model.layers.1": 6.683373928070068,
        "model.layers.2.input_layernorm": 7.019965171813965,
        "model.layers.2.post_attention_layernorm": 6.222538471221924,
        "model.layers.2.pre_feedforward_layernorm": 10.337218284606934
      }
    },
    {
      "sequence": "Describe Describe Describe impressionist painting techniques.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 4.865923171458037,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 3.6145801544189453,
        "model.layers.0.pre_feedforward_layernorm": 6.706852912902832,
        "model.layers.0.mlp.gate_proj": 7.44070291519165,
        "model.layers.0.mlp.up_proj": 3.234947919845581,
        "model.layers.0.mlp.down_proj": 0.7345227599143982,
        "model.layers.0.mlp": 0.7345227599143982,
        "model.layers.0.post_feedforward_layernorm": 3.2085301876068115,
        "model.layers.0": 3.9762730598449707,
        "model.layers.1.input_layernorm": 4.180998802185059,
        "model.layers.1.post_attention_layernorm": 6.422160625457764,
        "model.layers.1.pre_feedforward_layernorm": 11.035618782043457,
        "model.layers.1.mlp.gate_proj": 12.987320899963379,
        "model.layers.1.mlp.up_proj": 5.937195301055908,
        "model.layers.1.mlp.down_proj": 0.7702465057373047,
        "model.layers.1.mlp": 0.7702465057373047,
        "model.layers.1.post_feedforward_layernorm": 5.530961513519287,
        "model.layers.1": 6.626168727874756,
        "model.layers.2.input_layernorm": 7.085824489593506,
        "model.layers.2.post_attention_layernorm": 5.989587783813477,
        "model.layers.2.pre_feedforward_layernorm": 10.184318542480469
      }
    },
    {
      "sequence": "Explain Explain Explain quantum computing principles.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 4.887317367221998,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.326967239379883,
        "model.layers.0.pre_feedforward_layernorm": 6.701115608215332,
        "model.layers.0.mlp.gate_proj": 6.948235988616943,
        "model.layers.0.mlp.up_proj": 3.465742349624634,
        "model.layers.0.mlp.down_proj": 0.7819048762321472,
        "model.layers.0.mlp": 0.7819048762321472,
        "model.layers.0.post_feedforward_layernorm": 3.0666615962982178,
        "model.layers.0": 4.843873023986816,
        "model.layers.1.input_layernorm": 4.570145606994629,
        "model.layers.1.post_attention_layernorm": 6.191250324249268,
        "model.layers.1.pre_feedforward_layernorm": 10.902292251586914,
        "model.layers.1.mlp.gate_proj": 13.111600875854492,
        "model.layers.1.mlp.up_proj": 6.062224864959717,
        "model.layers.1.mlp.down_proj": 0.9198556542396545,
        "model.layers.1.mlp": 0.9198556542396545,
        "model.layers.1.post_feedforward_layernorm": 5.707931995391846,
        "model.layers.1": 7.141669750213623,
        "model.layers.2.input_layernorm": 7.4434638023376465,
        "model.layers.2.post_attention_layernorm": 4.478073596954346,
        "model.layers.2.pre_feedforward_layernorm": 8.9653902053833
      }
    },
    {
      "sequence": "Explain Explain Explain how neural networks work.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.019393532172494,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.599056720733643,
        "model.layers.0.pre_feedforward_layernorm": 7.150468349456787,
        "model.layers.0.mlp.gate_proj": 7.530278205871582,
        "model.layers.0.mlp.up_proj": 3.6154837608337402,
        "model.layers.0.mlp.down_proj": 0.7881743311882019,
        "model.layers.0.mlp": 0.7881743311882019,
        "model.layers.0.post_feedforward_layernorm": 2.8117992877960205,
        "model.layers.0": 4.70574426651001,
        "model.layers.1.input_layernorm": 4.464284420013428,
        "model.layers.1.post_attention_layernorm": 6.346795082092285,
        "model.layers.1.pre_feedforward_layernorm": 11.783459663391113,
        "model.layers.1.mlp.gate_proj": 14.15053653717041,
        "model.layers.1.mlp.up_proj": 6.150152206420898,
        "model.layers.1.mlp.down_proj": 0.8894517421722412,
        "model.layers.1.mlp": 0.8894517421722412,
        "model.layers.1.post_feedforward_layernorm": 6.157954216003418,
        "model.layers.1": 6.9416375160217285,
        "model.layers.2.input_layernorm": 7.485855579376221,
        "model.layers.2.post_attention_layernorm": 4.468589782714844,
        "model.layers.2.pre_feedforward_layernorm": 8.984051704406738
      }
    },
    {
      "sequence": "Describe Describe Describe the water cycle.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.068154117335444,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.695253372192383,
        "model.layers.0.pre_feedforward_layernorm": 7.455319404602051,
        "model.layers.0.mlp.gate_proj": 7.8380537033081055,
        "model.layers.0.mlp.up_proj": 3.6654157638549805,
        "model.layers.0.mlp.down_proj": 0.726216733455658,
        "model.layers.0.mlp": 0.726216733455658,
        "model.layers.0.post_feedforward_layernorm": 2.967341423034668,
        "model.layers.0": 4.801194190979004,
        "model.layers.1.input_layernorm": 4.890754222869873,
        "model.layers.1.post_attention_layernorm": 5.989891052246094,
        "model.layers.1.pre_feedforward_layernorm": 10.771249771118164,
        "model.layers.1.mlp.gate_proj": 11.805228233337402,
        "model.layers.1.mlp.up_proj": 5.782505035400391,
        "model.layers.1.mlp.down_proj": 0.8101622462272644,
        "model.layers.1.mlp": 0.8101622462272644,
        "model.layers.1.post_feedforward_layernorm": 5.407051086425781,
        "model.layers.1": 7.113846778869629,
        "model.layers.2.input_layernorm": 7.768389701843262,
        "model.layers.2.post_attention_layernorm": 6.56003475189209,
        "model.layers.2.pre_feedforward_layernorm": 10.905118942260742
      }
    },
    {
      "sequence": "The following is a test: Explain the entire process of how a computer processes a program from source code to execution, including compilation, memory management, and CPU operations.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.104140696318253,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.2365083694458,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 2.7706220149993896,
        "model.layers.0.pre_feedforward_layernorm": 4.113955020904541,
        "model.layers.0.mlp.gate_proj": 3.9622507095336914,
        "model.layers.0.mlp.up_proj": 2.142261266708374,
        "model.layers.0.mlp.down_proj": 0.595253050327301,
        "model.layers.0.mlp": 0.595253050327301,
        "model.layers.0.post_feedforward_layernorm": 1.7348787784576416,
        "model.layers.0": 3.053431510925293,
        "model.layers.1.input_layernorm": 2.9333958625793457,
        "model.layers.1.post_attention_layernorm": 7.121232509613037,
        "model.layers.1.pre_feedforward_layernorm": 13.125616073608398,
        "model.layers.1.mlp.gate_proj": 15.481216430664062,
        "model.layers.1.mlp.up_proj": 6.945400714874268,
        "model.layers.1.mlp.down_proj": 0.9134421944618225,
        "model.layers.1.mlp": 0.9134421944618225,
        "model.layers.1.post_feedforward_layernorm": 5.821662425994873,
        "model.layers.1": 6.874161720275879,
        "model.layers.2.input_layernorm": 6.852242946624756,
        "model.layers.2.post_attention_layernorm": 8.966456413269043,
        "model.layers.2.pre_feedforward_layernorm": 14.242552757263184
      }
    },
    {
      "sequence": "I I I need detailed information about this topic.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.113053469554238,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.063426971435547,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 3.668262243270874,
        "model.layers.0.pre_feedforward_layernorm": 6.147722244262695,
        "model.layers.0.mlp.gate_proj": 6.613710880279541,
        "model.layers.0.mlp.up_proj": 2.823974847793579,
        "model.layers.0.mlp.down_proj": 0.42789092659950256,
        "model.layers.0.mlp": 0.42789092659950256,
        "model.layers.0.post_feedforward_layernorm": 1.9484710693359375,
        "model.layers.0": 2.819711446762085,
        "model.layers.1.input_layernorm": 3.009338140487671,
        "model.layers.1.post_attention_layernorm": 9.601868629455566,
        "model.layers.1.pre_feedforward_layernorm": 14.834805488586426,
        "model.layers.1.mlp.gate_proj": 17.36680793762207,
        "model.layers.1.mlp.up_proj": 7.393924236297607,
        "model.layers.1.mlp.down_proj": 1.2090284824371338,
        "model.layers.1.mlp": 1.2090284824371338,
        "model.layers.1.post_feedforward_layernorm": 8.106965065002441,
        "model.layers.1": 6.363001823425293,
        "model.layers.2.input_layernorm": 6.663818836212158,
        "model.layers.2.post_attention_layernorm": 3.6294686794281006,
        "model.layers.2.pre_feedforward_layernorm": 8.271112442016602
      }
    },
    {
      "sequence": "Explain Explain Explain ethical dilemmas.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.190652593322422,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.725961208343506,
        "model.layers.0.pre_feedforward_layernorm": 7.526889324188232,
        "model.layers.0.mlp.gate_proj": 8.13768196105957,
        "model.layers.0.mlp.up_proj": 3.783842086791992,
        "model.layers.0.mlp.down_proj": 0.79213547706604,
        "model.layers.0.mlp": 0.79213547706604,
        "model.layers.0.post_feedforward_layernorm": 3.0183498859405518,
        "model.layers.0": 4.859080791473389,
        "model.layers.1.input_layernorm": 4.6280436515808105,
        "model.layers.1.post_attention_layernorm": 6.282381534576416,
        "model.layers.1.pre_feedforward_layernorm": 11.04127025604248,
        "model.layers.1.mlp.gate_proj": 13.742642402648926,
        "model.layers.1.mlp.up_proj": 5.941629886627197,
        "model.layers.1.mlp.down_proj": 0.9302166104316711,
        "model.layers.1.mlp": 0.9302166104316711,
        "model.layers.1.post_feedforward_layernorm": 6.516220569610596,
        "model.layers.1": 6.9557576179504395,
        "model.layers.2.input_layernorm": 7.551497459411621,
        "model.layers.2.post_attention_layernorm": 5.754545211791992,
        "model.layers.2.pre_feedforward_layernorm": 10.094596862792969
      }
    },
    {
      "sequence": "Compare Compare Compare and contrast these concepts.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.25784402826558,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 6.194121360778809,
        "model.layers.0.pre_feedforward_layernorm": 9.172784805297852,
        "model.layers.0.mlp.gate_proj": 9.043208122253418,
        "model.layers.0.mlp.up_proj": 4.179393291473389,
        "model.layers.0.mlp.down_proj": 0.8620244264602661,
        "model.layers.0.mlp": 0.8620244264602661,
        "model.layers.0.post_feedforward_layernorm": 3.769890546798706,
        "model.layers.0": 4.297074317932129,
        "model.layers.1.input_layernorm": 4.557496547698975,
        "model.layers.1.post_attention_layernorm": 6.3480095863342285,
        "model.layers.1.pre_feedforward_layernorm": 11.341574668884277,
        "model.layers.1.mlp.gate_proj": 14.784104347229004,
        "model.layers.1.mlp.up_proj": 6.169480800628662,
        "model.layers.1.mlp.down_proj": 0.838434636592865,
        "model.layers.1.mlp": 0.838434636592865,
        "model.layers.1.post_feedforward_layernorm": 5.411715507507324,
        "model.layers.1": 6.520185947418213,
        "model.layers.2.input_layernorm": 7.136932373046875,
        "model.layers.2.post_attention_layernorm": 4.65090274810791,
        "model.layers.2.pre_feedforward_layernorm": 9.207967758178711
      }
    },
    {
      "sequence": "Describe Describe Describe time management methods.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.3297350043835845,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.573299407958984,
        "model.layers.0.pre_feedforward_layernorm": 7.646639823913574,
        "model.layers.0.mlp.gate_proj": 8.110251426696777,
        "model.layers.0.mlp.up_proj": 3.6338016986846924,
        "model.layers.0.mlp.down_proj": 0.8520358800888062,
        "model.layers.0.mlp": 0.8520358800888062,
        "model.layers.0.post_feedforward_layernorm": 3.679788827896118,
        "model.layers.0": 5.363783836364746,
        "model.layers.1.input_layernorm": 5.057140350341797,
        "model.layers.1.post_attention_layernorm": 6.883913040161133,
        "model.layers.1.pre_feedforward_layernorm": 12.11014461517334,
        "model.layers.1.mlp.gate_proj": 12.622537612915039,
        "model.layers.1.mlp.up_proj": 6.411154747009277,
        "model.layers.1.mlp.down_proj": 0.8173545002937317,
        "model.layers.1.mlp": 0.8173545002937317,
        "model.layers.1.post_feedforward_layernorm": 6.16880989074707,
        "model.layers.1": 7.409815788269043,
        "model.layers.2.input_layernorm": 7.899379253387451,
        "model.layers.2.post_attention_layernorm": 6.081945896148682,
        "model.layers.2.pre_feedforward_layernorm": 10.514578819274902
      }
    },
    {
      "sequence": "Translate: Translate: Translate: Bonjour means hello in French.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.334272503852844,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.646179676055908,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 6.580079078674316,
        "model.layers.0.pre_feedforward_layernorm": 9.271139144897461,
        "model.layers.0.mlp.gate_proj": 10.102916717529297,
        "model.layers.0.mlp.up_proj": 4.722323894500732,
        "model.layers.0.mlp.down_proj": 1.2731287479400635,
        "model.layers.0.mlp": 1.2731287479400635,
        "model.layers.0.post_feedforward_layernorm": 4.3789286613464355,
        "model.layers.0": 6.401716709136963,
        "model.layers.1.input_layernorm": 6.752894401550293,
        "model.layers.1.post_attention_layernorm": 4.538224220275879,
        "model.layers.1.pre_feedforward_layernorm": 10.109790802001953,
        "model.layers.1.mlp.gate_proj": 11.083191871643066,
        "model.layers.1.mlp.up_proj": 6.343285083770752,
        "model.layers.1.mlp.down_proj": 0.9776498675346375,
        "model.layers.1.mlp": 0.9776498675346375,
        "model.layers.1.post_feedforward_layernorm": 4.442176342010498,
        "model.layers.1": 6.800626277923584,
        "model.layers.2.input_layernorm": 7.124074459075928,
        "model.layers.2.post_attention_layernorm": 3.8768606185913086,
        "model.layers.2.pre_feedforward_layernorm": 8.01230239868164
      }
    },
    {
      "sequence": "Explain Explain Explain probability theory.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.373702038889346,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.769636631011963,
        "model.layers.0.pre_feedforward_layernorm": 6.87700891494751,
        "model.layers.0.mlp.gate_proj": 7.969088077545166,
        "model.layers.0.mlp.up_proj": 3.574326753616333,
        "model.layers.0.mlp.down_proj": 0.839066743850708,
        "model.layers.0.mlp": 0.839066743850708,
        "model.layers.0.post_feedforward_layernorm": 3.29002046585083,
        "model.layers.0": 5.527135372161865,
        "model.layers.1.input_layernorm": 4.723726272583008,
        "model.layers.1.post_attention_layernorm": 6.667685508728027,
        "model.layers.1.pre_feedforward_layernorm": 11.842818260192871,
        "model.layers.1.mlp.gate_proj": 14.201678276062012,
        "model.layers.1.mlp.up_proj": 6.361000061035156,
        "model.layers.1.mlp.down_proj": 0.9836254119873047,
        "model.layers.1.mlp": 0.9836254119873047,
        "model.layers.1.post_feedforward_layernorm": 6.403988838195801,
        "model.layers.1": 7.526898384094238,
        "model.layers.2.input_layernorm": 8.016342163085938,
        "model.layers.2.post_attention_layernorm": 6.265033721923828,
        "model.layers.2.pre_feedforward_layernorm": 10.553460121154785
      }
    },
    {
      "sequence": "Explain Explain Explain the Renaissance period.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.439806746399921,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.807155609130859,
        "model.layers.0.pre_feedforward_layernorm": 7.632689476013184,
        "model.layers.0.mlp.gate_proj": 8.737722396850586,
        "model.layers.0.mlp.up_proj": 3.877277374267578,
        "model.layers.0.mlp.down_proj": 0.8230128288269043,
        "model.layers.0.mlp": 0.8230128288269043,
        "model.layers.0.post_feedforward_layernorm": 2.9079627990722656,
        "model.layers.0": 4.623993873596191,
        "model.layers.1.input_layernorm": 4.666599750518799,
        "model.layers.1.post_attention_layernorm": 6.802633762359619,
        "model.layers.1.pre_feedforward_layernorm": 12.09645938873291,
        "model.layers.1.mlp.gate_proj": 16.421552658081055,
        "model.layers.1.mlp.up_proj": 6.46620512008667,
        "model.layers.1.mlp.down_proj": 0.9414083361625671,
        "model.layers.1.mlp": 0.9414083361625671,
        "model.layers.1.post_feedforward_layernorm": 6.9031758308410645,
        "model.layers.1": 7.37733268737793,
        "model.layers.2.input_layernorm": 8.079858779907227,
        "model.layers.2.post_attention_layernorm": 5.041146278381348,
        "model.layers.2.pre_feedforward_layernorm": 10.066807746887207
      }
    },
    {
      "sequence": "The following is a test: Describe the complete lifecycle of a star from formation to death, including all stages and physical processes involved.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.553910213968028,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.243990898132324,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 3.4518637657165527,
        "model.layers.0.pre_feedforward_layernorm": 4.70365047454834,
        "model.layers.0.mlp.gate_proj": 4.72270393371582,
        "model.layers.0.mlp.up_proj": 2.41078782081604,
        "model.layers.0.mlp.down_proj": 0.7067948579788208,
        "model.layers.0.mlp": 0.7067948579788208,
        "model.layers.0.post_feedforward_layernorm": 1.945021152496338,
        "model.layers.0": 3.5685722827911377,
        "model.layers.1.input_layernorm": 3.2972469329833984,
        "model.layers.1.post_attention_layernorm": 7.468549728393555,
        "model.layers.1.pre_feedforward_layernorm": 14.646981239318848,
        "model.layers.1.mlp.gate_proj": 17.889474868774414,
        "model.layers.1.mlp.up_proj": 7.724691867828369,
        "model.layers.1.mlp.down_proj": 1.0202068090438843,
        "model.layers.1.mlp": 1.0202068090438843,
        "model.layers.1.post_feedforward_layernorm": 6.311603546142578,
        "model.layers.1": 8.206886291503906,
        "model.layers.2.input_layernorm": 8.214383125305176,
        "model.layers.2.post_attention_layernorm": 7.602231025695801,
        "model.layers.2.pre_feedforward_layernorm": 13.87729263305664
      }
    },
    {
      "sequence": "Can Can Can you help me understand this?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.606799244880676,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 8.02878475189209,
        "model.layers.0.pre_feedforward_layernorm": 10.696133613586426,
        "model.layers.0.mlp.gate_proj": 10.887120246887207,
        "model.layers.0.mlp.up_proj": 5.64276123046875,
        "model.layers.0.mlp.down_proj": 0.9941449165344238,
        "model.layers.0.mlp": 0.9941449165344238,
        "model.layers.0.post_feedforward_layernorm": 5.349579811096191,
        "model.layers.0": 6.477755546569824,
        "model.layers.1.input_layernorm": 6.743593692779541,
        "model.layers.1.post_attention_layernorm": 5.309333324432373,
        "model.layers.1.pre_feedforward_layernorm": 10.983867645263672,
        "model.layers.1.mlp.gate_proj": 11.811599731445312,
        "model.layers.1.mlp.up_proj": 5.994196891784668,
        "model.layers.1.mlp.down_proj": 0.8676192164421082,
        "model.layers.1.mlp": 0.8676192164421082,
        "model.layers.1.post_feedforward_layernorm": 5.0555620193481445,
        "model.layers.1": 7.677599906921387,
        "model.layers.2.input_layernorm": 7.147352695465088,
        "model.layers.2.post_attention_layernorm": 4.38783073425293,
        "model.layers.2.pre_feedforward_layernorm": 7.819391250610352
      }
    },
    {
      "sequence": "Explain Explain Explain film editing principles.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.62445850994276,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.725578784942627,
        "model.layers.0.pre_feedforward_layernorm": 8.10035228729248,
        "model.layers.0.mlp.gate_proj": 8.918540954589844,
        "model.layers.0.mlp.up_proj": 3.989323854446411,
        "model.layers.0.mlp.down_proj": 0.8599393367767334,
        "model.layers.0.mlp": 0.8599393367767334,
        "model.layers.0.post_feedforward_layernorm": 3.348824977874756,
        "model.layers.0": 4.785008430480957,
        "model.layers.1.input_layernorm": 4.771562576293945,
        "model.layers.1.post_attention_layernorm": 7.16664981842041,
        "model.layers.1.pre_feedforward_layernorm": 12.679522514343262,
        "model.layers.1.mlp.gate_proj": 15.537664413452148,
        "model.layers.1.mlp.up_proj": 6.743473052978516,
        "model.layers.1.mlp.down_proj": 1.062211036682129,
        "model.layers.1.mlp": 1.062211036682129,
        "model.layers.1.post_feedforward_layernorm": 7.556803226470947,
        "model.layers.1": 7.934407711029053,
        "model.layers.2.input_layernorm": 8.71435832977295,
        "model.layers.2.post_attention_layernorm": 4.921159744262695,
        "model.layers.2.pre_feedforward_layernorm": 10.546875
      }
    },
    {
      "sequence": "Explain Explain Explain database normalization.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 5.856107172758683,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 5.2787556648254395,
        "model.layers.0.pre_feedforward_layernorm": 8.67872428894043,
        "model.layers.0.mlp.gate_proj": 10.18490219116211,
        "model.layers.0.mlp.up_proj": 4.232734680175781,
        "model.layers.0.mlp.down_proj": 0.9255505800247192,
        "model.layers.0.mlp": 0.9255505800247192,
        "model.layers.0.post_feedforward_layernorm": 3.638484477996826,
        "model.layers.0": 5.658936023712158,
        "model.layers.1.input_layernorm": 5.309383392333984,
        "model.layers.1.post_attention_layernorm": 6.710897445678711,
        "model.layers.1.pre_feedforward_layernorm": 13.433305740356445,
        "model.layers.1.mlp.gate_proj": 15.249736785888672,
        "model.layers.1.mlp.up_proj": 6.82082462310791,
        "model.layers.1.mlp.down_proj": 1.0379403829574585,
        "model.layers.1.mlp": 1.0379403829574585,
        "model.layers.1.post_feedforward_layernorm": 7.1501874923706055,
        "model.layers.1": 7.977813243865967,
        "model.layers.2.input_layernorm": 8.681867599487305,
        "model.layers.2.post_attention_layernorm": 5.6079421043396,
        "model.layers.2.pre_feedforward_layernorm": 10.769072532653809
      }
    },
    {
      "sequence": "Explain Explain Explain cooking techniques.",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 6.167289371075838,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 5.6447062492370605,
        "model.layers.0.pre_feedforward_layernorm": 8.499899864196777,
        "model.layers.0.mlp.gate_proj": 9.326263427734375,
        "model.layers.0.mlp.up_proj": 4.30885648727417,
        "model.layers.0.mlp.down_proj": 0.9822945594787598,
        "model.layers.0.mlp": 0.9822945594787598,
        "model.layers.0.post_feedforward_layernorm": 3.753369092941284,
        "model.layers.0": 5.95825719833374,
        "model.layers.1.input_layernorm": 5.3075337409973145,
        "model.layers.1.post_attention_layernorm": 7.474085807800293,
        "model.layers.1.pre_feedforward_layernorm": 13.758408546447754,
        "model.layers.1.mlp.gate_proj": 18.12497901916504,
        "model.layers.1.mlp.up_proj": 7.255250453948975,
        "model.layers.1.mlp.down_proj": 1.106334924697876,
        "model.layers.1.mlp": 1.106334924697876,
        "model.layers.1.post_feedforward_layernorm": 7.139995574951172,
        "model.layers.1": 8.747785568237305,
        "model.layers.2.input_layernorm": 9.346251487731934,
        "model.layers.2.post_attention_layernorm": 5.865924835205078,
        "model.layers.2.pre_feedforward_layernorm": 11.778914451599121
      }
    },
    {
      "sequence": "The following is a test: Translate: Bonjour means hello in French.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 6.285139063130254,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.294113159179688,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 4.494435787200928,
        "model.layers.0.pre_feedforward_layernorm": 6.190474510192871,
        "model.layers.0.mlp.gate_proj": 7.505625247955322,
        "model.layers.0.mlp.up_proj": 3.470987558364868,
        "model.layers.0.mlp.down_proj": 0.9218448400497437,
        "model.layers.0.mlp": 0.9218448400497437,
        "model.layers.0.post_feedforward_layernorm": 3.3083930015563965,
        "model.layers.0": 4.865170478820801,
        "model.layers.1.input_layernorm": 4.931369304656982,
        "model.layers.1.post_attention_layernorm": 8.090568542480469,
        "model.layers.1.pre_feedforward_layernorm": 16.8209228515625,
        "model.layers.1.mlp.gate_proj": 20.107383728027344,
        "model.layers.1.mlp.up_proj": 8.634347915649414,
        "model.layers.1.mlp.down_proj": 1.0998566150665283,
        "model.layers.1.mlp": 1.0998566150665283,
        "model.layers.1.post_feedforward_layernorm": 6.8652729988098145,
        "model.layers.1": 9.433008193969727,
        "model.layers.2.input_layernorm": 9.338482856750488,
        "model.layers.2.post_attention_layernorm": 6.212165355682373,
        "model.layers.2.pre_feedforward_layernorm": 11.95207405090332
      }
    },
    {
      "sequence": "The following is a test: I need detailed information about this topic.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 6.61624228436014,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.294113159179688,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 5.653316497802734,
        "model.layers.0.pre_feedforward_layernorm": 7.990758895874023,
        "model.layers.0.mlp.gate_proj": 8.78161907196045,
        "model.layers.0.mlp.up_proj": 4.097421646118164,
        "model.layers.0.mlp.down_proj": 1.0885591506958008,
        "model.layers.0.mlp": 1.0885591506958008,
        "model.layers.0.post_feedforward_layernorm": 3.722632884979248,
        "model.layers.0": 5.437320232391357,
        "model.layers.1.input_layernorm": 5.4743123054504395,
        "model.layers.1.post_attention_layernorm": 9.273528099060059,
        "model.layers.1.pre_feedforward_layernorm": 16.538040161132812,
        "model.layers.1.mlp.gate_proj": 17.91252326965332,
        "model.layers.1.mlp.up_proj": 8.906436920166016,
        "model.layers.1.mlp.down_proj": 1.1265950202941895,
        "model.layers.1.mlp": 1.1265950202941895,
        "model.layers.1.post_feedforward_layernorm": 6.6619133949279785,
        "model.layers.1": 10.16440200805664,
        "model.layers.2.input_layernorm": 9.998133659362793,
        "model.layers.2.post_attention_layernorm": 6.562372207641602,
        "model.layers.2.pre_feedforward_layernorm": 12.274419784545898
      }
    },
    {
      "sequence": "The following is a test: Write Python code to sort a list.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 6.7597016977227256,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.294113159179688,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 5.136222839355469,
        "model.layers.0.pre_feedforward_layernorm": 7.209656238555908,
        "model.layers.0.mlp.gate_proj": 7.246842861175537,
        "model.layers.0.mlp.up_proj": 3.7935972213745117,
        "model.layers.0.mlp.down_proj": 1.0182279348373413,
        "model.layers.0.mlp": 1.0182279348373413,
        "model.layers.0.post_feedforward_layernorm": 2.963655710220337,
        "model.layers.0": 5.055431365966797,
        "model.layers.1.input_layernorm": 5.065510272979736,
        "model.layers.1.post_attention_layernorm": 10.825897216796875,
        "model.layers.1.pre_feedforward_layernorm": 17.181711196899414,
        "model.layers.1.mlp.gate_proj": 18.953657150268555,
        "model.layers.1.mlp.up_proj": 9.728276252746582,
        "model.layers.1.mlp.down_proj": 1.2497152090072632,
        "model.layers.1.mlp": 1.2497152090072632,
        "model.layers.1.post_feedforward_layernorm": 8.10849380493164,
        "model.layers.1": 12.101027488708496,
        "model.layers.2.input_layernorm": 10.997903823852539,
        "model.layers.2.post_attention_layernorm": 5.815947532653809,
        "model.layers.2.pre_feedforward_layernorm": 12.459308624267578
      }
    },
    {
      "sequence": "How How How do you solve quadratic equations?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 6.88256341478099,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 8.938370704650879,
        "model.layers.0.pre_feedforward_layernorm": 12.304854393005371,
        "model.layers.0.mlp.gate_proj": 12.43656063079834,
        "model.layers.0.mlp.up_proj": 6.257742881774902,
        "model.layers.0.mlp.down_proj": 1.4859201908111572,
        "model.layers.0.mlp": 1.4859201908111572,
        "model.layers.0.post_feedforward_layernorm": 5.578832149505615,
        "model.layers.0": 8.875300407409668,
        "model.layers.1.input_layernorm": 8.352275848388672,
        "model.layers.1.post_attention_layernorm": 6.708233833312988,
        "model.layers.1.pre_feedforward_layernorm": 14.035127639770508,
        "model.layers.1.mlp.gate_proj": 14.033912658691406,
        "model.layers.1.mlp.up_proj": 7.8066487312316895,
        "model.layers.1.mlp.down_proj": 1.071508765220642,
        "model.layers.1.mlp": 1.071508765220642,
        "model.layers.1.post_feedforward_layernorm": 7.027341842651367,
        "model.layers.1": 10.343446731567383,
        "model.layers.2.input_layernorm": 10.113558769226074,
        "model.layers.2.post_attention_layernorm": 4.673015117645264,
        "model.layers.2.pre_feedforward_layernorm": 10.478487014770508
      }
    },
    {
      "sequence": "What What What is the structure of a sonnet?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 6.944495232208915,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.063426971435547,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.110426902770996,
        "model.layers.0.pre_feedforward_layernorm": 14.889213562011719,
        "model.layers.0.mlp.gate_proj": 16.723834991455078,
        "model.layers.0.mlp.up_proj": 7.172157287597656,
        "model.layers.0.mlp.down_proj": 1.657699465751648,
        "model.layers.0.mlp": 1.657699465751648,
        "model.layers.0.post_feedforward_layernorm": 6.545845985412598,
        "model.layers.0": 9.34255599975586,
        "model.layers.1.input_layernorm": 9.67885971069336,
        "model.layers.1.post_attention_layernorm": 4.841922760009766,
        "model.layers.1.pre_feedforward_layernorm": 12.70595645904541,
        "model.layers.1.mlp.gate_proj": 12.06069278717041,
        "model.layers.1.mlp.up_proj": 7.276465892791748,
        "model.layers.1.mlp.down_proj": 0.9467854499816895,
        "model.layers.1.mlp": 0.9467854499816895,
        "model.layers.1.post_feedforward_layernorm": 6.325398921966553,
        "model.layers.1": 9.445429801940918,
        "model.layers.2.input_layernorm": 9.190807342529297,
        "model.layers.2.post_attention_layernorm": 4.193380355834961,
        "model.layers.2.pre_feedforward_layernorm": 8.948044776916504
      }
    },
    {
      "sequence": "How How How does photosynthesis work?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 7.562442655148714,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.035942077636719,
        "model.layers.0.pre_feedforward_layernorm": 14.851065635681152,
        "model.layers.0.mlp.gate_proj": 15.075067520141602,
        "model.layers.0.mlp.up_proj": 7.43034029006958,
        "model.layers.0.mlp.down_proj": 1.878672480583191,
        "model.layers.0.mlp": 1.878672480583191,
        "model.layers.0.post_feedforward_layernorm": 7.017000675201416,
        "model.layers.0": 10.72310733795166,
        "model.layers.1.input_layernorm": 10.05494499206543,
        "model.layers.1.post_attention_layernorm": 5.781076908111572,
        "model.layers.1.pre_feedforward_layernorm": 13.813624382019043,
        "model.layers.1.mlp.gate_proj": 13.77285099029541,
        "model.layers.1.mlp.up_proj": 8.02526569366455,
        "model.layers.1.mlp.down_proj": 1.0221368074417114,
        "model.layers.1.mlp": 1.0221368074417114,
        "model.layers.1.post_feedforward_layernorm": 8.272165298461914,
        "model.layers.1": 10.791555404663086,
        "model.layers.2.input_layernorm": 10.86005687713623,
        "model.layers.2.post_attention_layernorm": 4.888970851898193,
        "model.layers.2.pre_feedforward_layernorm": 10.6633882522583
      }
    },
    {
      "sequence": "What What What was the Industrial Revolution?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 7.652367032092551,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.16183853149414,
        "model.layers.0.pre_feedforward_layernorm": 17.189725875854492,
        "model.layers.0.mlp.gate_proj": 17.793670654296875,
        "model.layers.0.mlp.up_proj": 8.09252643585205,
        "model.layers.0.mlp.down_proj": 1.914933681488037,
        "model.layers.0.mlp": 1.914933681488037,
        "model.layers.0.post_feedforward_layernorm": 8.223515510559082,
        "model.layers.0": 10.257827758789062,
        "model.layers.1.input_layernorm": 10.317039489746094,
        "model.layers.1.post_attention_layernorm": 5.561276912689209,
        "model.layers.1.pre_feedforward_layernorm": 13.380731582641602,
        "model.layers.1.mlp.gate_proj": 12.698601722717285,
        "model.layers.1.mlp.up_proj": 7.948685169219971,
        "model.layers.1.mlp.down_proj": 1.0424017906188965,
        "model.layers.1.mlp": 1.0424017906188965,
        "model.layers.1.post_feedforward_layernorm": 7.077263355255127,
        "model.layers.1": 10.286580085754395,
        "model.layers.2.input_layernorm": 10.129450798034668,
        "model.layers.2.post_attention_layernorm": 4.629990577697754,
        "model.layers.2.pre_feedforward_layernorm": 9.596394538879395
      }
    },
    {
      "sequence": "How How How does music theory work?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 7.777907174566518,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.19737434387207,
        "model.layers.0.pre_feedforward_layernorm": 16.942276000976562,
        "model.layers.0.mlp.gate_proj": 16.281808853149414,
        "model.layers.0.mlp.up_proj": 8.12303352355957,
        "model.layers.0.mlp.down_proj": 1.8853299617767334,
        "model.layers.0.mlp": 1.8853299617767334,
        "model.layers.0.post_feedforward_layernorm": 8.497782707214355,
        "model.layers.0": 10.103846549987793,
        "model.layers.1.input_layernorm": 9.805395126342773,
        "model.layers.1.post_attention_layernorm": 5.907748222351074,
        "model.layers.1.pre_feedforward_layernorm": 14.022551536560059,
        "model.layers.1.mlp.gate_proj": 14.185384750366211,
        "model.layers.1.mlp.up_proj": 8.023123741149902,
        "model.layers.1.mlp.down_proj": 1.0049270391464233,
        "model.layers.1.mlp": 1.0049270391464233,
        "model.layers.1.post_feedforward_layernorm": 7.204493999481201,
        "model.layers.1": 10.60287094116211,
        "model.layers.2.input_layernorm": 10.521381378173828,
        "model.layers.2.post_attention_layernorm": 4.537405490875244,
        "model.layers.2.pre_feedforward_layernorm": 10.410222053527832
      }
    },
    {
      "sequence": "How How How does exercise affect health?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 7.852288163226584,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.77387809753418,
        "model.layers.0.pre_feedforward_layernorm": 17.14099884033203,
        "model.layers.0.mlp.gate_proj": 16.34756088256836,
        "model.layers.0.mlp.up_proj": 8.214381217956543,
        "model.layers.0.mlp.down_proj": 1.840179681777954,
        "model.layers.0.mlp": 1.840179681777954,
        "model.layers.0.post_feedforward_layernorm": 8.345982551574707,
        "model.layers.0": 10.544718742370605,
        "model.layers.1.input_layernorm": 9.918120384216309,
        "model.layers.1.post_attention_layernorm": 5.699154853820801,
        "model.layers.1.pre_feedforward_layernorm": 14.203189849853516,
        "model.layers.1.mlp.gate_proj": 14.070140838623047,
        "model.layers.1.mlp.up_proj": 8.044624328613281,
        "model.layers.1.mlp.down_proj": 1.0609772205352783,
        "model.layers.1.mlp": 1.0609772205352783,
        "model.layers.1.post_feedforward_layernorm": 7.334469318389893,
        "model.layers.1": 10.664782524108887,
        "model.layers.2.input_layernorm": 10.600210189819336,
        "model.layers.2.post_attention_layernorm": 4.813162326812744,
        "model.layers.2.pre_feedforward_layernorm": 10.340287208557129
      }
    },
    {
      "sequence": "What What What is the meaning of life?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 7.864379105360611,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.189026832580566,
        "model.layers.0.pre_feedforward_layernorm": 17.47235107421875,
        "model.layers.0.mlp.gate_proj": 16.66474723815918,
        "model.layers.0.mlp.up_proj": 8.18966293334961,
        "model.layers.0.mlp.down_proj": 1.8576539754867554,
        "model.layers.0.mlp": 1.8576539754867554,
        "model.layers.0.post_feedforward_layernorm": 8.337295532226562,
        "model.layers.0": 10.21231460571289,
        "model.layers.1.input_layernorm": 10.226545333862305,
        "model.layers.1.post_attention_layernorm": 5.847258567810059,
        "model.layers.1.pre_feedforward_layernorm": 14.825443267822266,
        "model.layers.1.mlp.gate_proj": 14.105917930603027,
        "model.layers.1.mlp.up_proj": 8.412538528442383,
        "model.layers.1.mlp.down_proj": 1.1607253551483154,
        "model.layers.1.mlp": 1.1607253551483154,
        "model.layers.1.post_feedforward_layernorm": 6.518555641174316,
        "model.layers.1": 10.493627548217773,
        "model.layers.2.input_layernorm": 10.517831802368164,
        "model.layers.2.post_attention_layernorm": 4.196146011352539,
        "model.layers.2.pre_feedforward_layernorm": 10.414306640625
      }
    },
    {
      "sequence": "How How How do vaccines work?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 7.868914811507516,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.61966609954834,
        "model.layers.0.pre_feedforward_layernorm": 16.1261043548584,
        "model.layers.0.mlp.gate_proj": 15.96567153930664,
        "model.layers.0.mlp.up_proj": 7.768741130828857,
        "model.layers.0.mlp.down_proj": 1.8499091863632202,
        "model.layers.0.mlp": 1.8499091863632202,
        "model.layers.0.post_feedforward_layernorm": 7.565349102020264,
        "model.layers.0": 10.104246139526367,
        "model.layers.1.input_layernorm": 10.128006935119629,
        "model.layers.1.post_attention_layernorm": 6.295169353485107,
        "model.layers.1.pre_feedforward_layernorm": 14.842923164367676,
        "model.layers.1.mlp.gate_proj": 14.905529022216797,
        "model.layers.1.mlp.up_proj": 8.477296829223633,
        "model.layers.1.mlp.down_proj": 1.1589607000350952,
        "model.layers.1.mlp": 1.1589607000350952,
        "model.layers.1.post_feedforward_layernorm": 7.498125076293945,
        "model.layers.1": 11.255744934082031,
        "model.layers.2.input_layernorm": 11.21695327758789,
        "model.layers.2.post_attention_layernorm": 4.964116096496582,
        "model.layers.2.pre_feedforward_layernorm": 11.155518531799316
      }
    },
    {
      "sequence": "How How How do transformers process language?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 7.879359286764394,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.01516342163086,
        "model.layers.0.pre_feedforward_layernorm": 16.229434967041016,
        "model.layers.0.mlp.gate_proj": 16.530160903930664,
        "model.layers.0.mlp.up_proj": 7.934974670410156,
        "model.layers.0.mlp.down_proj": 1.950421690940857,
        "model.layers.0.mlp": 1.950421690940857,
        "model.layers.0.post_feedforward_layernorm": 7.8468017578125,
        "model.layers.0": 10.648990631103516,
        "model.layers.1.input_layernorm": 10.319488525390625,
        "model.layers.1.post_attention_layernorm": 6.176089286804199,
        "model.layers.1.pre_feedforward_layernorm": 14.999561309814453,
        "model.layers.1.mlp.gate_proj": 15.265457153320312,
        "model.layers.1.mlp.up_proj": 8.596829414367676,
        "model.layers.1.mlp.down_proj": 1.105073094367981,
        "model.layers.1.mlp": 1.105073094367981,
        "model.layers.1.post_feedforward_layernorm": 7.243101119995117,
        "model.layers.1": 11.064716339111328,
        "model.layers.2.input_layernorm": 10.713898658752441,
        "model.layers.2.post_attention_layernorm": 4.2570695877075195,
        "model.layers.2.pre_feedforward_layernorm": 10.527884483337402
      }
    },
    {
      "sequence": "How How How do we define truth?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 7.912890029990154,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.884532928466797,
        "model.layers.0.pre_feedforward_layernorm": 16.908557891845703,
        "model.layers.0.mlp.gate_proj": 15.918524742126465,
        "model.layers.0.mlp.up_proj": 8.1763277053833,
        "model.layers.0.mlp.down_proj": 1.799275279045105,
        "model.layers.0.mlp": 1.799275279045105,
        "model.layers.0.post_feedforward_layernorm": 8.284027099609375,
        "model.layers.0": 9.530301094055176,
        "model.layers.1.input_layernorm": 9.481103897094727,
        "model.layers.1.post_attention_layernorm": 6.413849353790283,
        "model.layers.1.pre_feedforward_layernorm": 15.250840187072754,
        "model.layers.1.mlp.gate_proj": 15.448406219482422,
        "model.layers.1.mlp.up_proj": 8.403645515441895,
        "model.layers.1.mlp.down_proj": 1.0494730472564697,
        "model.layers.1.mlp": 1.0494730472564697,
        "model.layers.1.post_feedforward_layernorm": 7.563085556030273,
        "model.layers.1": 10.518726348876953,
        "model.layers.2.input_layernorm": 10.537657737731934,
        "model.layers.2.post_attention_layernorm": 5.36133337020874,
        "model.layers.2.pre_feedforward_layernorm": 10.87340259552002
      }
    },
    {
      "sequence": "The following is a test: Describe the process of machine learning.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 7.922870563424152,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.317299842834473,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 6.476176738739014,
        "model.layers.0.pre_feedforward_layernorm": 7.834965705871582,
        "model.layers.0.mlp.gate_proj": 7.950967788696289,
        "model.layers.0.mlp.up_proj": 4.342823028564453,
        "model.layers.0.mlp.down_proj": 1.104061484336853,
        "model.layers.0.mlp": 1.104061484336853,
        "model.layers.0.post_feedforward_layernorm": 3.4814560413360596,
        "model.layers.0": 5.980367183685303,
        "model.layers.1.input_layernorm": 5.772156238555908,
        "model.layers.1.post_attention_layernorm": 11.25001049041748,
        "model.layers.1.pre_feedforward_layernorm": 20.609325408935547,
        "model.layers.1.mlp.gate_proj": 23.150041580200195,
        "model.layers.1.mlp.up_proj": 11.110799789428711,
        "model.layers.1.mlp.down_proj": 1.2919005155563354,
        "model.layers.1.mlp": 1.2919005155563354,
        "model.layers.1.post_feedforward_layernorm": 8.325380325317383,
        "model.layers.1": 12.223817825317383,
        "model.layers.2.input_layernorm": 12.660751342773438,
        "model.layers.2.post_attention_layernorm": 10.057456970214844,
        "model.layers.2.pre_feedforward_layernorm": 17.890302658081055
      }
    },
    {
      "sequence": "The following is a test: Describe free will vs determinism.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 7.9411194790964545,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.317299842834473,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 5.769226551055908,
        "model.layers.0.pre_feedforward_layernorm": 7.2901506423950195,
        "model.layers.0.mlp.gate_proj": 7.400426864624023,
        "model.layers.0.mlp.up_proj": 3.987077474594116,
        "model.layers.0.mlp.down_proj": 0.9919412732124329,
        "model.layers.0.mlp": 0.9919412732124329,
        "model.layers.0.post_feedforward_layernorm": 3.1131417751312256,
        "model.layers.0": 5.40360689163208,
        "model.layers.1.input_layernorm": 5.289940357208252,
        "model.layers.1.post_attention_layernorm": 11.946392059326172,
        "model.layers.1.pre_feedforward_layernorm": 20.505584716796875,
        "model.layers.1.mlp.gate_proj": 24.05147933959961,
        "model.layers.1.mlp.up_proj": 11.402358055114746,
        "model.layers.1.mlp.down_proj": 1.433735966682434,
        "model.layers.1.mlp": 1.433735966682434,
        "model.layers.1.post_feedforward_layernorm": 9.910029411315918,
        "model.layers.1": 13.166399955749512,
        "model.layers.2.input_layernorm": 13.338481903076172,
        "model.layers.2.post_attention_layernorm": 8.966747283935547,
        "model.layers.2.pre_feedforward_layernorm": 17.936050415039062
      }
    },
    {
      "sequence": "How How How does encryption work?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 7.966519138087397,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.851747512817383,
        "model.layers.0.pre_feedforward_layernorm": 15.744987487792969,
        "model.layers.0.mlp.gate_proj": 15.688069343566895,
        "model.layers.0.mlp.up_proj": 7.70474100112915,
        "model.layers.0.mlp.down_proj": 1.885493278503418,
        "model.layers.0.mlp": 1.885493278503418,
        "model.layers.0.post_feedforward_layernorm": 7.768582820892334,
        "model.layers.0": 10.314494132995605,
        "model.layers.1.input_layernorm": 10.178559303283691,
        "model.layers.1.post_attention_layernorm": 6.5352067947387695,
        "model.layers.1.pre_feedforward_layernorm": 15.165565490722656,
        "model.layers.1.mlp.gate_proj": 15.10660457611084,
        "model.layers.1.mlp.up_proj": 8.708993911743164,
        "model.layers.1.mlp.down_proj": 1.1260958909988403,
        "model.layers.1.mlp": 1.1260958909988403,
        "model.layers.1.post_feedforward_layernorm": 7.725882530212402,
        "model.layers.1": 11.278166770935059,
        "model.layers.2.input_layernorm": 11.445171356201172,
        "model.layers.2.post_attention_layernorm": 5.404381275177002,
        "model.layers.2.pre_feedforward_layernorm": 11.507468223571777
      }
    },
    {
      "sequence": "The following is a test: Please explain step by step.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.055643755456675,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 7.264656066894531,
        "model.layers.0.pre_feedforward_layernorm": 9.990792274475098,
        "model.layers.0.mlp.gate_proj": 10.657925605773926,
        "model.layers.0.mlp.up_proj": 5.489068031311035,
        "model.layers.0.mlp.down_proj": 1.5749247074127197,
        "model.layers.0.mlp": 1.5749247074127197,
        "model.layers.0.post_feedforward_layernorm": 5.457127571105957,
        "model.layers.0": 7.787093639373779,
        "model.layers.1.input_layernorm": 7.9320902824401855,
        "model.layers.1.post_attention_layernorm": 10.713852882385254,
        "model.layers.1.pre_feedforward_layernorm": 19.0129337310791,
        "model.layers.1.mlp.gate_proj": 20.643518447875977,
        "model.layers.1.mlp.up_proj": 10.504912376403809,
        "model.layers.1.mlp.down_proj": 1.3506356477737427,
        "model.layers.1.mlp": 1.3506356477737427,
        "model.layers.1.post_feedforward_layernorm": 8.126317024230957,
        "model.layers.1": 13.6513090133667,
        "model.layers.2.input_layernorm": 13.303791999816895,
        "model.layers.2.post_attention_layernorm": 6.497935771942139,
        "model.layers.2.pre_feedforward_layernorm": 14.067765235900879
      }
    },
    {
      "sequence": "The following is a test: What is the structure of a sonnet?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.062762778738271,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.294113159179688,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 7.809998512268066,
        "model.layers.0.pre_feedforward_layernorm": 10.161489486694336,
        "model.layers.0.mlp.gate_proj": 10.892485618591309,
        "model.layers.0.mlp.up_proj": 5.367869853973389,
        "model.layers.0.mlp.down_proj": 1.1442838907241821,
        "model.layers.0.mlp": 1.1442838907241821,
        "model.layers.0.post_feedforward_layernorm": 5.643692970275879,
        "model.layers.0": 6.69406270980835,
        "model.layers.1.input_layernorm": 7.597836494445801,
        "model.layers.1.post_attention_layernorm": 9.939837455749512,
        "model.layers.1.pre_feedforward_layernorm": 18.62863540649414,
        "model.layers.1.mlp.gate_proj": 21.269182205200195,
        "model.layers.1.mlp.up_proj": 9.798383712768555,
        "model.layers.1.mlp.down_proj": 1.339685082435608,
        "model.layers.1.mlp": 1.339685082435608,
        "model.layers.1.post_feedforward_layernorm": 9.687182426452637,
        "model.layers.1": 13.159417152404785,
        "model.layers.2.input_layernorm": 12.563084602355957,
        "model.layers.2.post_attention_layernorm": 8.173580169677734,
        "model.layers.2.pre_feedforward_layernorm": 14.794754028320312
      }
    },
    {
      "sequence": "What What What are investment strategies?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.06495912178703,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.459543228149414,
        "model.layers.0.pre_feedforward_layernorm": 16.18778419494629,
        "model.layers.0.mlp.gate_proj": 17.043180465698242,
        "model.layers.0.mlp.up_proj": 7.785510063171387,
        "model.layers.0.mlp.down_proj": 1.9020650386810303,
        "model.layers.0.mlp": 1.9020650386810303,
        "model.layers.0.post_feedforward_layernorm": 6.982924461364746,
        "model.layers.0": 11.355154037475586,
        "model.layers.1.input_layernorm": 10.285876274108887,
        "model.layers.1.post_attention_layernorm": 7.085728168487549,
        "model.layers.1.pre_feedforward_layernorm": 14.711323738098145,
        "model.layers.1.mlp.gate_proj": 14.728589057922363,
        "model.layers.1.mlp.up_proj": 8.562664985656738,
        "model.layers.1.mlp.down_proj": 1.2011579275131226,
        "model.layers.1.mlp": 1.2011579275131226,
        "model.layers.1.post_feedforward_layernorm": 7.75909948348999,
        "model.layers.1": 12.077330589294434,
        "model.layers.2.input_layernorm": 11.145959854125977,
        "model.layers.2.post_attention_layernorm": 5.061638832092285,
        "model.layers.2.pre_feedforward_layernorm": 10.977167129516602
      }
    },
    {
      "sequence": "How How How do I change a tire?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.137814957162607,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.60495376586914,
        "model.layers.0.pre_feedforward_layernorm": 17.460006713867188,
        "model.layers.0.mlp.gate_proj": 17.493677139282227,
        "model.layers.0.mlp.up_proj": 8.302924156188965,
        "model.layers.0.mlp.down_proj": 1.8562763929367065,
        "model.layers.0.mlp": 1.8562763929367065,
        "model.layers.0.post_feedforward_layernorm": 8.827009201049805,
        "model.layers.0": 10.460330963134766,
        "model.layers.1.input_layernorm": 10.180903434753418,
        "model.layers.1.post_attention_layernorm": 5.900669574737549,
        "model.layers.1.pre_feedforward_layernorm": 15.2640962600708,
        "model.layers.1.mlp.gate_proj": 15.337040901184082,
        "model.layers.1.mlp.up_proj": 8.545943260192871,
        "model.layers.1.mlp.down_proj": 1.08319890499115,
        "model.layers.1.mlp": 1.08319890499115,
        "model.layers.1.post_feedforward_layernorm": 6.6359028816223145,
        "model.layers.1": 11.198046684265137,
        "model.layers.2.input_layernorm": 11.126971244812012,
        "model.layers.2.post_attention_layernorm": 4.710389137268066,
        "model.layers.2.pre_feedforward_layernorm": 11.021536827087402
      }
    },
    {
      "sequence": "What What What causes climate change?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.154064199198848,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.29985523223877,
        "model.layers.0.pre_feedforward_layernorm": 17.509471893310547,
        "model.layers.0.mlp.gate_proj": 17.783960342407227,
        "model.layers.0.mlp.up_proj": 8.120573043823242,
        "model.layers.0.mlp.down_proj": 1.8316141366958618,
        "model.layers.0.mlp": 1.8316141366958618,
        "model.layers.0.post_feedforward_layernorm": 8.124128341674805,
        "model.layers.0": 10.704362869262695,
        "model.layers.1.input_layernorm": 10.305517196655273,
        "model.layers.1.post_attention_layernorm": 7.247642517089844,
        "model.layers.1.pre_feedforward_layernorm": 14.729825019836426,
        "model.layers.1.mlp.gate_proj": 14.912260055541992,
        "model.layers.1.mlp.up_proj": 8.291481971740723,
        "model.layers.1.mlp.down_proj": 1.1064800024032593,
        "model.layers.1.mlp": 1.1064800024032593,
        "model.layers.1.post_feedforward_layernorm": 8.197293281555176,
        "model.layers.1": 11.680768013000488,
        "model.layers.2.input_layernorm": 11.031506538391113,
        "model.layers.2.post_attention_layernorm": 4.268457889556885,
        "model.layers.2.pre_feedforward_layernorm": 10.382044792175293
      }
    },
    {
      "sequence": "The following is a test: Analyze Shakespeare's writing style.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.209446658258853,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.317299842834473,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 6.148153781890869,
        "model.layers.0.pre_feedforward_layernorm": 7.564055442810059,
        "model.layers.0.mlp.gate_proj": 7.741116523742676,
        "model.layers.0.mlp.up_proj": 4.181499004364014,
        "model.layers.0.mlp.down_proj": 1.149774432182312,
        "model.layers.0.mlp": 1.149774432182312,
        "model.layers.0.post_feedforward_layernorm": 3.708920478820801,
        "model.layers.0": 6.034501552581787,
        "model.layers.1.input_layernorm": 5.764327526092529,
        "model.layers.1.post_attention_layernorm": 12.400519371032715,
        "model.layers.1.pre_feedforward_layernorm": 21.014244079589844,
        "model.layers.1.mlp.gate_proj": 24.44959831237793,
        "model.layers.1.mlp.up_proj": 11.678833961486816,
        "model.layers.1.mlp.down_proj": 1.4579898118972778,
        "model.layers.1.mlp": 1.4579898118972778,
        "model.layers.1.post_feedforward_layernorm": 10.0296049118042,
        "model.layers.1": 13.12606430053711,
        "model.layers.2.input_layernorm": 13.828091621398926,
        "model.layers.2.post_attention_layernorm": 9.157794952392578,
        "model.layers.2.pre_feedforward_layernorm": 18.45711898803711
      }
    },
    {
      "sequence": "What What What are the applications of deep learning?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.243805792020714,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.063426971435547,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.700267791748047,
        "model.layers.0.pre_feedforward_layernorm": 18.775524139404297,
        "model.layers.0.mlp.gate_proj": 19.813034057617188,
        "model.layers.0.mlp.up_proj": 8.558445930480957,
        "model.layers.0.mlp.down_proj": 1.9134560823440552,
        "model.layers.0.mlp": 1.9134560823440552,
        "model.layers.0.post_feedforward_layernorm": 8.950104713439941,
        "model.layers.0": 10.6592378616333,
        "model.layers.1.input_layernorm": 10.445321083068848,
        "model.layers.1.post_attention_layernorm": 6.328225135803223,
        "model.layers.1.pre_feedforward_layernorm": 15.457722663879395,
        "model.layers.1.mlp.gate_proj": 14.895172119140625,
        "model.layers.1.mlp.up_proj": 8.491048812866211,
        "model.layers.1.mlp.down_proj": 1.1404485702514648,
        "model.layers.1.mlp": 1.1404485702514648,
        "model.layers.1.post_feedforward_layernorm": 6.093003273010254,
        "model.layers.1": 11.529720306396484,
        "model.layers.2.input_layernorm": 10.504268646240234,
        "model.layers.2.post_attention_layernorm": 4.095678806304932,
        "model.layers.2.pre_feedforward_layernorm": 10.139521598815918
      }
    },
    {
      "sequence": "The following is a test: Compare and contrast these concepts.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.281776158706002,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 7.914422035217285,
        "model.layers.0.pre_feedforward_layernorm": 8.768235206604004,
        "model.layers.0.mlp.gate_proj": 9.102572441101074,
        "model.layers.0.mlp.up_proj": 4.645480632781982,
        "model.layers.0.mlp.down_proj": 1.4191325902938843,
        "model.layers.0.mlp": 1.4191325902938843,
        "model.layers.0.post_feedforward_layernorm": 4.3616228103637695,
        "model.layers.0": 7.535784721374512,
        "model.layers.1.input_layernorm": 6.558802127838135,
        "model.layers.1.post_attention_layernorm": 11.133257865905762,
        "model.layers.1.pre_feedforward_layernorm": 20.496915817260742,
        "model.layers.1.mlp.gate_proj": 23.287887573242188,
        "model.layers.1.mlp.up_proj": 10.999881744384766,
        "model.layers.1.mlp.down_proj": 1.4637216329574585,
        "model.layers.1.mlp": 1.4637216329574585,
        "model.layers.1.post_feedforward_layernorm": 9.401490211486816,
        "model.layers.1": 12.633402824401855,
        "model.layers.2.input_layernorm": 13.40424633026123,
        "model.layers.2.post_attention_layernorm": 8.716136932373047,
        "model.layers.2.pre_feedforward_layernorm": 17.42740821838379
      }
    },
    {
      "sequence": "How How How did the internet develop?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.282057876172273,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 14.258221626281738,
        "model.layers.0.pre_feedforward_layernorm": 18.644683837890625,
        "model.layers.0.mlp.gate_proj": 17.799861907958984,
        "model.layers.0.mlp.up_proj": 8.738432884216309,
        "model.layers.0.mlp.down_proj": 1.920163631439209,
        "model.layers.0.mlp": 1.920163631439209,
        "model.layers.0.post_feedforward_layernorm": 9.880939483642578,
        "model.layers.0": 10.140787124633789,
        "model.layers.1.input_layernorm": 10.205300331115723,
        "model.layers.1.post_attention_layernorm": 6.021719455718994,
        "model.layers.1.pre_feedforward_layernorm": 15.112220764160156,
        "model.layers.1.mlp.gate_proj": 15.686262130737305,
        "model.layers.1.mlp.up_proj": 8.517338752746582,
        "model.layers.1.mlp.down_proj": 1.0564559698104858,
        "model.layers.1.mlp": 1.0564559698104858,
        "model.layers.1.post_feedforward_layernorm": 7.138071060180664,
        "model.layers.1": 10.874815940856934,
        "model.layers.2.input_layernorm": 11.001293182373047,
        "model.layers.2.post_attention_layernorm": 4.702365398406982,
        "model.layers.2.pre_feedforward_layernorm": 11.067126274108887
      }
    },
    {
      "sequence": "What What What is consciousness?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.295430898666382,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.349345207214355,
        "model.layers.0.pre_feedforward_layernorm": 16.819915771484375,
        "model.layers.0.mlp.gate_proj": 17.232892990112305,
        "model.layers.0.mlp.up_proj": 8.123960494995117,
        "model.layers.0.mlp.down_proj": 1.8868805170059204,
        "model.layers.0.mlp": 1.8868805170059204,
        "model.layers.0.post_feedforward_layernorm": 7.459321022033691,
        "model.layers.0": 10.882328987121582,
        "model.layers.1.input_layernorm": 10.672534942626953,
        "model.layers.1.post_attention_layernorm": 6.984935760498047,
        "model.layers.1.pre_feedforward_layernorm": 14.974527359008789,
        "model.layers.1.mlp.gate_proj": 14.450993537902832,
        "model.layers.1.mlp.up_proj": 8.671125411987305,
        "model.layers.1.mlp.down_proj": 1.092146396636963,
        "model.layers.1.mlp": 1.092146396636963,
        "model.layers.1.post_feedforward_layernorm": 7.749020099639893,
        "model.layers.1": 12.379215240478516,
        "model.layers.2.input_layernorm": 12.07181167602539,
        "model.layers.2.post_attention_layernorm": 6.225154876708984,
        "model.layers.2.pre_feedforward_layernorm": 12.409858703613281
      }
    },
    {
      "sequence": "The following is a test: Explain how neural networks work.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.299410467562469,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 8.489594459533691,
        "model.layers.0.pre_feedforward_layernorm": 9.781705856323242,
        "model.layers.0.mlp.gate_proj": 9.810498237609863,
        "model.layers.0.mlp.up_proj": 5.3297648429870605,
        "model.layers.0.mlp.down_proj": 1.503353238105774,
        "model.layers.0.mlp": 1.503353238105774,
        "model.layers.0.post_feedforward_layernorm": 4.188925266265869,
        "model.layers.0": 8.310332298278809,
        "model.layers.1.input_layernorm": 6.917108535766602,
        "model.layers.1.post_attention_layernorm": 11.241498947143555,
        "model.layers.1.pre_feedforward_layernorm": 18.994871139526367,
        "model.layers.1.mlp.gate_proj": 22.629348754882812,
        "model.layers.1.mlp.up_proj": 10.718246459960938,
        "model.layers.1.mlp.down_proj": 1.3895736932754517,
        "model.layers.1.mlp": 1.3895736932754517,
        "model.layers.1.post_feedforward_layernorm": 8.858497619628906,
        "model.layers.1": 13.138343811035156,
        "model.layers.2.input_layernorm": 12.984004974365234,
        "model.layers.2.post_attention_layernorm": 8.43869686126709,
        "model.layers.2.pre_feedforward_layernorm": 16.941553115844727
      }
    },
    {
      "sequence": "What What What are the pros and cons?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.332492600316586,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.004188537597656,
        "model.layers.0.pre_feedforward_layernorm": 19.18760108947754,
        "model.layers.0.mlp.gate_proj": 19.520240783691406,
        "model.layers.0.mlp.up_proj": 8.755352020263672,
        "model.layers.0.mlp.down_proj": 2.007122755050659,
        "model.layers.0.mlp": 2.007122755050659,
        "model.layers.0.post_feedforward_layernorm": 9.259851455688477,
        "model.layers.0": 11.634296417236328,
        "model.layers.1.input_layernorm": 11.081664085388184,
        "model.layers.1.post_attention_layernorm": 5.472148418426514,
        "model.layers.1.pre_feedforward_layernorm": 14.643682479858398,
        "model.layers.1.mlp.gate_proj": 14.171146392822266,
        "model.layers.1.mlp.up_proj": 8.629199028015137,
        "model.layers.1.mlp.down_proj": 1.301701545715332,
        "model.layers.1.mlp": 1.301701545715332,
        "model.layers.1.post_feedforward_layernorm": 6.17042350769043,
        "model.layers.1": 11.41041374206543,
        "model.layers.2.input_layernorm": 10.59326457977295,
        "model.layers.2.post_attention_layernorm": 4.163131237030029,
        "model.layers.2.pre_feedforward_layernorm": 10.112686157226562
      }
    },
    {
      "sequence": "The following is a test: How do you solve quadratic equations?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.424522804177325,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.317299842834473,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 8.477950096130371,
        "model.layers.0.pre_feedforward_layernorm": 10.40514850616455,
        "model.layers.0.mlp.gate_proj": 10.279006958007812,
        "model.layers.0.mlp.up_proj": 5.410224914550781,
        "model.layers.0.mlp.down_proj": 1.2507939338684082,
        "model.layers.0.mlp": 1.2507939338684082,
        "model.layers.0.post_feedforward_layernorm": 5.78120231628418,
        "model.layers.0": 7.396738529205322,
        "model.layers.1.input_layernorm": 7.455836772918701,
        "model.layers.1.post_attention_layernorm": 11.772994041442871,
        "model.layers.1.pre_feedforward_layernorm": 21.230562210083008,
        "model.layers.1.mlp.gate_proj": 22.366628646850586,
        "model.layers.1.mlp.up_proj": 10.549528121948242,
        "model.layers.1.mlp.down_proj": 1.4841164350509644,
        "model.layers.1.mlp": 1.4841164350509644,
        "model.layers.1.post_feedforward_layernorm": 9.758230209350586,
        "model.layers.1": 14.288918495178223,
        "model.layers.2.input_layernorm": 13.43367862701416,
        "model.layers.2.post_attention_layernorm": 7.078411102294922,
        "model.layers.2.pre_feedforward_layernorm": 14.291844367980957
      }
    },
    {
      "sequence": "What What What is artificial intelligence?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.43846224701923,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.513626098632812,
        "model.layers.0.pre_feedforward_layernorm": 17.406707763671875,
        "model.layers.0.mlp.gate_proj": 17.30133056640625,
        "model.layers.0.mlp.up_proj": 8.350784301757812,
        "model.layers.0.mlp.down_proj": 1.901919960975647,
        "model.layers.0.mlp": 1.901919960975647,
        "model.layers.0.post_feedforward_layernorm": 8.283127784729004,
        "model.layers.0": 11.245325088500977,
        "model.layers.1.input_layernorm": 10.905290603637695,
        "model.layers.1.post_attention_layernorm": 7.2523016929626465,
        "model.layers.1.pre_feedforward_layernorm": 15.335591316223145,
        "model.layers.1.mlp.gate_proj": 14.98245906829834,
        "model.layers.1.mlp.up_proj": 9.104204177856445,
        "model.layers.1.mlp.down_proj": 1.305659532546997,
        "model.layers.1.mlp": 1.305659532546997,
        "model.layers.1.post_feedforward_layernorm": 7.629088878631592,
        "model.layers.1": 12.325029373168945,
        "model.layers.2.input_layernorm": 11.859283447265625,
        "model.layers.2.post_attention_layernorm": 5.352320671081543,
        "model.layers.2.pre_feedforward_layernorm": 11.74486255645752
      }
    },
    {
      "sequence": "What What What is the Fibonacci sequence?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.469628790150518,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.964058876037598,
        "model.layers.0.pre_feedforward_layernorm": 16.86673927307129,
        "model.layers.0.mlp.gate_proj": 18.751205444335938,
        "model.layers.0.mlp.up_proj": 8.22130012512207,
        "model.layers.0.mlp.down_proj": 2.0610122680664062,
        "model.layers.0.mlp": 2.0610122680664062,
        "model.layers.0.post_feedforward_layernorm": 7.848843097686768,
        "model.layers.0": 11.555696487426758,
        "model.layers.1.input_layernorm": 11.336545944213867,
        "model.layers.1.post_attention_layernorm": 6.398406028747559,
        "model.layers.1.pre_feedforward_layernorm": 16.287710189819336,
        "model.layers.1.mlp.gate_proj": 16.09593963623047,
        "model.layers.1.mlp.up_proj": 9.537660598754883,
        "model.layers.1.mlp.down_proj": 1.275071144104004,
        "model.layers.1.mlp": 1.275071144104004,
        "model.layers.1.post_feedforward_layernorm": 8.016257286071777,
        "model.layers.1": 12.09489917755127,
        "model.layers.2.input_layernorm": 11.902946472167969,
        "model.layers.2.post_attention_layernorm": 4.74662446975708,
        "model.layers.2.pre_feedforward_layernorm": 11.759810447692871
      }
    },
    {
      "sequence": "What What What caused World War II?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.52989529526752,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.16950511932373,
        "model.layers.0.pre_feedforward_layernorm": 17.98690414428711,
        "model.layers.0.mlp.gate_proj": 18.156658172607422,
        "model.layers.0.mlp.up_proj": 8.316978454589844,
        "model.layers.0.mlp.down_proj": 1.9556260108947754,
        "model.layers.0.mlp": 1.9556260108947754,
        "model.layers.0.post_feedforward_layernorm": 8.673253059387207,
        "model.layers.0": 11.364448547363281,
        "model.layers.1.input_layernorm": 11.141487121582031,
        "model.layers.1.post_attention_layernorm": 7.466660976409912,
        "model.layers.1.pre_feedforward_layernorm": 15.642962455749512,
        "model.layers.1.mlp.gate_proj": 15.67912483215332,
        "model.layers.1.mlp.up_proj": 9.446446418762207,
        "model.layers.1.mlp.down_proj": 1.2672029733657837,
        "model.layers.1.mlp": 1.2672029733657837,
        "model.layers.1.post_feedforward_layernorm": 8.191873550415039,
        "model.layers.1": 12.49289321899414,
        "model.layers.2.input_layernorm": 12.079551696777344,
        "model.layers.2.post_attention_layernorm": 4.039804935455322,
        "model.layers.2.pre_feedforward_layernorm": 11.14872932434082
      }
    },
    {
      "sequence": "The following is a test: Can you help me understand this?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.575825349144313,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.317299842834473,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 9.253739356994629,
        "model.layers.0.pre_feedforward_layernorm": 11.331748962402344,
        "model.layers.0.mlp.gate_proj": 11.462943077087402,
        "model.layers.0.mlp.up_proj": 5.863270282745361,
        "model.layers.0.mlp.down_proj": 1.1487714052200317,
        "model.layers.0.mlp": 1.1487714052200317,
        "model.layers.0.post_feedforward_layernorm": 6.339500904083252,
        "model.layers.0": 8.648355484008789,
        "model.layers.1.input_layernorm": 8.755730628967285,
        "model.layers.1.post_attention_layernorm": 11.849087715148926,
        "model.layers.1.pre_feedforward_layernorm": 20.244503021240234,
        "model.layers.1.mlp.gate_proj": 22.360034942626953,
        "model.layers.1.mlp.up_proj": 10.690211296081543,
        "model.layers.1.mlp.down_proj": 1.4208016395568848,
        "model.layers.1.mlp": 1.4208016395568848,
        "model.layers.1.post_feedforward_layernorm": 8.488566398620605,
        "model.layers.1": 14.600167274475098,
        "model.layers.2.input_layernorm": 13.359785079956055,
        "model.layers.2.post_attention_layernorm": 6.8876166343688965,
        "model.layers.2.pre_feedforward_layernorm": 13.652276039123535
      }
    },
    {
      "sequence": "The following is a test: Prove the Pythagorean theorem.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 8.707324826199075,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 9.611210823059082,
        "model.layers.0.pre_feedforward_layernorm": 11.272148132324219,
        "model.layers.0.mlp.gate_proj": 11.49036979675293,
        "model.layers.0.mlp.up_proj": 5.879739284515381,
        "model.layers.0.mlp.down_proj": 1.9144563674926758,
        "model.layers.0.mlp": 1.9144563674926758,
        "model.layers.0.post_feedforward_layernorm": 5.36384916305542,
        "model.layers.0": 9.985432624816895,
        "model.layers.1.input_layernorm": 8.010884284973145,
        "model.layers.1.post_attention_layernorm": 11.378092765808105,
        "model.layers.1.pre_feedforward_layernorm": 19.724611282348633,
        "model.layers.1.mlp.gate_proj": 20.165319442749023,
        "model.layers.1.mlp.up_proj": 10.771797180175781,
        "model.layers.1.mlp.down_proj": 1.5268241167068481,
        "model.layers.1.mlp": 1.5268241167068481,
        "model.layers.1.post_feedforward_layernorm": 9.767557144165039,
        "model.layers.1": 13.233625411987305,
        "model.layers.2.input_layernorm": 12.433993339538574,
        "model.layers.2.post_attention_layernorm": 9.382636070251465,
        "model.layers.2.pre_feedforward_layernorm": 16.566837310791016
      }
    },
    {
      "sequence": "The following is a test: What is the Fibonacci sequence?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.288009311841881,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 9.489117622375488,
        "model.layers.0.pre_feedforward_layernorm": 12.266690254211426,
        "model.layers.0.mlp.gate_proj": 12.984170913696289,
        "model.layers.0.mlp.up_proj": 6.388476371765137,
        "model.layers.0.mlp.down_proj": 1.3866550922393799,
        "model.layers.0.mlp": 1.3866550922393799,
        "model.layers.0.post_feedforward_layernorm": 7.211198329925537,
        "model.layers.0": 8.360543251037598,
        "model.layers.1.input_layernorm": 8.963839530944824,
        "model.layers.1.post_attention_layernorm": 11.470410346984863,
        "model.layers.1.pre_feedforward_layernorm": 21.68162727355957,
        "model.layers.1.mlp.gate_proj": 24.329654693603516,
        "model.layers.1.mlp.up_proj": 11.63670539855957,
        "model.layers.1.mlp.down_proj": 1.6057708263397217,
        "model.layers.1.mlp": 1.6057708263397217,
        "model.layers.1.post_feedforward_layernorm": 10.387845993041992,
        "model.layers.1": 14.748456001281738,
        "model.layers.2.input_layernorm": 15.242194175720215,
        "model.layers.2.post_attention_layernorm": 7.60597562789917,
        "model.layers.2.pre_feedforward_layernorm": 16.54486083984375
      }
    },
    {
      "sequence": "The following is a test: Explain quantum computing principles.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.369705127633136,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.35292911529541,
        "model.layers.0.pre_feedforward_layernorm": 11.712420463562012,
        "model.layers.0.mlp.gate_proj": 12.06385326385498,
        "model.layers.0.mlp.up_proj": 6.2805070877075195,
        "model.layers.0.mlp.down_proj": 1.8985034227371216,
        "model.layers.0.mlp": 1.8985034227371216,
        "model.layers.0.post_feedforward_layernorm": 5.248044490814209,
        "model.layers.0": 10.448500633239746,
        "model.layers.1.input_layernorm": 8.095954895019531,
        "model.layers.1.post_attention_layernorm": 12.036497116088867,
        "model.layers.1.pre_feedforward_layernorm": 20.997303009033203,
        "model.layers.1.mlp.gate_proj": 24.711498260498047,
        "model.layers.1.mlp.up_proj": 11.744344711303711,
        "model.layers.1.mlp.down_proj": 1.5498559474945068,
        "model.layers.1.mlp": 1.5498559474945068,
        "model.layers.1.post_feedforward_layernorm": 10.161771774291992,
        "model.layers.1": 15.143485069274902,
        "model.layers.2.input_layernorm": 14.175368309020996,
        "model.layers.2.post_attention_layernorm": 9.321078300476074,
        "model.layers.2.pre_feedforward_layernorm": 17.76513671875
      }
    },
    {
      "sequence": "The following is a test: Describe ancient Egyptian civilization.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.406793480334075,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 9.36531925201416,
        "model.layers.0.pre_feedforward_layernorm": 10.96535587310791,
        "model.layers.0.mlp.gate_proj": 11.313375473022461,
        "model.layers.0.mlp.up_proj": 5.8564453125,
        "model.layers.0.mlp.down_proj": 1.4640028476715088,
        "model.layers.0.mlp": 1.4640028476715088,
        "model.layers.0.post_feedforward_layernorm": 4.549524307250977,
        "model.layers.0": 8.619345664978027,
        "model.layers.1.input_layernorm": 7.133872032165527,
        "model.layers.1.post_attention_layernorm": 12.921435356140137,
        "model.layers.1.pre_feedforward_layernorm": 22.95162010192871,
        "model.layers.1.mlp.gate_proj": 26.398344039916992,
        "model.layers.1.mlp.up_proj": 12.092422485351562,
        "model.layers.1.mlp.down_proj": 1.5685228109359741,
        "model.layers.1.mlp": 1.5685228109359741,
        "model.layers.1.post_feedforward_layernorm": 10.667845726013184,
        "model.layers.1": 14.933587074279785,
        "model.layers.2.input_layernorm": 14.761604309082031,
        "model.layers.2.post_attention_layernorm": 10.334432601928711,
        "model.layers.2.pre_feedforward_layernorm": 19.0788631439209
      }
    },
    {
      "sequence": "The following is a test: What are the applications of deep learning?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.407069672708927,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.294113159179688,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 9.146907806396484,
        "model.layers.0.pre_feedforward_layernorm": 10.807838439941406,
        "model.layers.0.mlp.gate_proj": 11.69545841217041,
        "model.layers.0.mlp.up_proj": 5.793920040130615,
        "model.layers.0.mlp.down_proj": 1.200846552848816,
        "model.layers.0.mlp": 1.200846552848816,
        "model.layers.0.post_feedforward_layernorm": 6.963813781738281,
        "model.layers.0": 7.463695526123047,
        "model.layers.1.input_layernorm": 8.030364990234375,
        "model.layers.1.post_attention_layernorm": 12.782614707946777,
        "model.layers.1.pre_feedforward_layernorm": 23.333698272705078,
        "model.layers.1.mlp.gate_proj": 26.935304641723633,
        "model.layers.1.mlp.up_proj": 12.152303695678711,
        "model.layers.1.mlp.down_proj": 1.7260656356811523,
        "model.layers.1.mlp": 1.7260656356811523,
        "model.layers.1.post_feedforward_layernorm": 10.46103572845459,
        "model.layers.1": 16.432575225830078,
        "model.layers.2.input_layernorm": 15.499530792236328,
        "model.layers.2.post_attention_layernorm": 7.765033721923828,
        "model.layers.2.pre_feedforward_layernorm": 16.95056915283203
      }
    },
    {
      "sequence": "The following is a test: Describe impressionist painting techniques.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.466208520142928,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 7.558022499084473,
        "model.layers.0.pre_feedforward_layernorm": 9.177328109741211,
        "model.layers.0.mlp.gate_proj": 9.623397827148438,
        "model.layers.0.mlp.up_proj": 5.016185760498047,
        "model.layers.0.mlp.down_proj": 1.3108898401260376,
        "model.layers.0.mlp": 1.3108898401260376,
        "model.layers.0.post_feedforward_layernorm": 4.069455623626709,
        "model.layers.0": 7.192878246307373,
        "model.layers.1.input_layernorm": 6.232170104980469,
        "model.layers.1.post_attention_layernorm": 14.452640533447266,
        "model.layers.1.pre_feedforward_layernorm": 24.64528465270996,
        "model.layers.1.mlp.gate_proj": 29.091445922851562,
        "model.layers.1.mlp.up_proj": 12.94391918182373,
        "model.layers.1.mlp.down_proj": 1.6702100038528442,
        "model.layers.1.mlp": 1.6702100038528442,
        "model.layers.1.post_feedforward_layernorm": 11.82283878326416,
        "model.layers.1": 15.369028091430664,
        "model.layers.2.input_layernorm": 15.608114242553711,
        "model.layers.2.post_attention_layernorm": 10.298032760620117,
        "model.layers.2.pre_feedforward_layernorm": 20.332258224487305
      }
    },
    {
      "sequence": "The following is a test: Describe the water cycle.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.469807987627776,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.31773567199707,
        "model.layers.0.pre_feedforward_layernorm": 11.481124877929688,
        "model.layers.0.mlp.gate_proj": 11.854985237121582,
        "model.layers.0.mlp.up_proj": 6.2100114822387695,
        "model.layers.0.mlp.down_proj": 1.6538105010986328,
        "model.layers.0.mlp": 1.6538105010986328,
        "model.layers.0.post_feedforward_layernorm": 5.0288405418396,
        "model.layers.0": 9.024507522583008,
        "model.layers.1.input_layernorm": 7.952263832092285,
        "model.layers.1.post_attention_layernorm": 11.983999252319336,
        "model.layers.1.pre_feedforward_layernorm": 22.231582641601562,
        "model.layers.1.mlp.gate_proj": 24.766450881958008,
        "model.layers.1.mlp.up_proj": 12.117873191833496,
        "model.layers.1.mlp.down_proj": 1.4446889162063599,
        "model.layers.1.mlp": 1.4446889162063599,
        "model.layers.1.post_feedforward_layernorm": 9.851974487304688,
        "model.layers.1": 14.444814682006836,
        "model.layers.2.input_layernorm": 15.334747314453125,
        "model.layers.2.post_attention_layernorm": 10.55077838897705,
        "model.layers.2.pre_feedforward_layernorm": 20.109088897705078
      }
    },
    {
      "sequence": "The following is a test: Explain the Renaissance period.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.630212524662847,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.100747108459473,
        "model.layers.0.pre_feedforward_layernorm": 11.66333293914795,
        "model.layers.0.mlp.gate_proj": 12.050154685974121,
        "model.layers.0.mlp.up_proj": 6.113630294799805,
        "model.layers.0.mlp.down_proj": 1.6763051748275757,
        "model.layers.0.mlp": 1.6763051748275757,
        "model.layers.0.post_feedforward_layernorm": 4.759147644042969,
        "model.layers.0": 9.410391807556152,
        "model.layers.1.input_layernorm": 7.743733882904053,
        "model.layers.1.post_attention_layernorm": 13.161513328552246,
        "model.layers.1.pre_feedforward_layernorm": 22.29094886779785,
        "model.layers.1.mlp.gate_proj": 27.551219940185547,
        "model.layers.1.mlp.up_proj": 12.185256004333496,
        "model.layers.1.mlp.down_proj": 1.7036635875701904,
        "model.layers.1.mlp": 1.7036635875701904,
        "model.layers.1.post_feedforward_layernorm": 12.321725845336914,
        "model.layers.1": 14.344402313232422,
        "model.layers.2.input_layernorm": 15.097066879272461,
        "model.layers.2.post_attention_layernorm": 8.64568042755127,
        "model.layers.2.pre_feedforward_layernorm": 18.948192596435547
      }
    },
    {
      "sequence": "The following is a test: How does photosynthesis work?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.684561750163203,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.688804626464844,
        "model.layers.0.pre_feedforward_layernorm": 12.753804206848145,
        "model.layers.0.mlp.gate_proj": 12.449516296386719,
        "model.layers.0.mlp.up_proj": 6.811288833618164,
        "model.layers.0.mlp.down_proj": 1.5101629495620728,
        "model.layers.0.mlp": 1.5101629495620728,
        "model.layers.0.post_feedforward_layernorm": 7.590105056762695,
        "model.layers.0": 8.970588684082031,
        "model.layers.1.input_layernorm": 9.226713180541992,
        "model.layers.1.post_attention_layernorm": 13.261794090270996,
        "model.layers.1.pre_feedforward_layernorm": 22.351953506469727,
        "model.layers.1.mlp.gate_proj": 24.334733963012695,
        "model.layers.1.mlp.up_proj": 11.89078426361084,
        "model.layers.1.mlp.down_proj": 1.5038620233535767,
        "model.layers.1.mlp": 1.5038620233535767,
        "model.layers.1.post_feedforward_layernorm": 10.839641571044922,
        "model.layers.1": 16.84417152404785,
        "model.layers.2.input_layernorm": 15.792390823364258,
        "model.layers.2.post_attention_layernorm": 7.866715431213379,
        "model.layers.2.pre_feedforward_layernorm": 16.69605827331543
      }
    },
    {
      "sequence": "The following is a test: Explain film editing principles.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.718445249225782,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.715520858764648,
        "model.layers.0.pre_feedforward_layernorm": 12.192014694213867,
        "model.layers.0.mlp.gate_proj": 12.148385047912598,
        "model.layers.0.mlp.up_proj": 6.505824565887451,
        "model.layers.0.mlp.down_proj": 1.8755576610565186,
        "model.layers.0.mlp": 1.8755576610565186,
        "model.layers.0.post_feedforward_layernorm": 5.424490928649902,
        "model.layers.0": 10.283257484436035,
        "model.layers.1.input_layernorm": 8.25020980834961,
        "model.layers.1.post_attention_layernorm": 13.775300025939941,
        "model.layers.1.pre_feedforward_layernorm": 21.72924041748047,
        "model.layers.1.mlp.gate_proj": 25.604923248291016,
        "model.layers.1.mlp.up_proj": 12.483642578125,
        "model.layers.1.mlp.down_proj": 1.8774467706680298,
        "model.layers.1.mlp": 1.8774467706680298,
        "model.layers.1.post_feedforward_layernorm": 12.57750415802002,
        "model.layers.1": 14.279508590698242,
        "model.layers.2.input_layernorm": 14.8407621383667,
        "model.layers.2.post_attention_layernorm": 8.680872917175293,
        "model.layers.2.pre_feedforward_layernorm": 18.17896842956543
      }
    },
    {
      "sequence": "The following is a test: How does exercise affect health?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.795808149420697,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.461503028869629,
        "model.layers.0.pre_feedforward_layernorm": 13.3629732131958,
        "model.layers.0.mlp.gate_proj": 13.460673332214355,
        "model.layers.0.mlp.up_proj": 7.0217180252075195,
        "model.layers.0.mlp.down_proj": 1.5100841522216797,
        "model.layers.0.mlp": 1.5100841522216797,
        "model.layers.0.post_feedforward_layernorm": 8.250365257263184,
        "model.layers.0": 9.214548110961914,
        "model.layers.1.input_layernorm": 9.438247680664062,
        "model.layers.1.post_attention_layernorm": 13.494589805603027,
        "model.layers.1.pre_feedforward_layernorm": 22.095462799072266,
        "model.layers.1.mlp.gate_proj": 25.788759231567383,
        "model.layers.1.mlp.up_proj": 11.538553237915039,
        "model.layers.1.mlp.down_proj": 1.4173579216003418,
        "model.layers.1.mlp": 1.4173579216003418,
        "model.layers.1.post_feedforward_layernorm": 10.088560104370117,
        "model.layers.1": 16.49462890625,
        "model.layers.2.input_layernorm": 15.298941612243652,
        "model.layers.2.post_attention_layernorm": 7.832026958465576,
        "model.layers.2.pre_feedforward_layernorm": 16.279556274414062
      }
    },
    {
      "sequence": "The following is a test: Describe API design principles.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.796774003816688,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.477986335754395,
        "model.layers.0.pre_feedforward_layernorm": 11.294230461120605,
        "model.layers.0.mlp.gate_proj": 11.712300300598145,
        "model.layers.0.mlp.up_proj": 6.186037063598633,
        "model.layers.0.mlp.down_proj": 1.6902774572372437,
        "model.layers.0.mlp": 1.6902774572372437,
        "model.layers.0.post_feedforward_layernorm": 5.101076126098633,
        "model.layers.0": 9.821083068847656,
        "model.layers.1.input_layernorm": 7.8903913497924805,
        "model.layers.1.post_attention_layernorm": 13.58260440826416,
        "model.layers.1.pre_feedforward_layernorm": 23.737913131713867,
        "model.layers.1.mlp.gate_proj": 26.184364318847656,
        "model.layers.1.mlp.up_proj": 12.93750286102295,
        "model.layers.1.mlp.down_proj": 1.6750233173370361,
        "model.layers.1.mlp": 1.6750233173370361,
        "model.layers.1.post_feedforward_layernorm": 10.543983459472656,
        "model.layers.1": 16.723283767700195,
        "model.layers.2.input_layernorm": 15.507726669311523,
        "model.layers.2.post_attention_layernorm": 9.719684600830078,
        "model.layers.2.pre_feedforward_layernorm": 18.827226638793945
      }
    },
    {
      "sequence": "The following is a test: How do we define truth?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.80961268881093,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.126089096069336,
        "model.layers.0.pre_feedforward_layernorm": 12.577129364013672,
        "model.layers.0.mlp.gate_proj": 12.443607330322266,
        "model.layers.0.mlp.up_proj": 6.559972763061523,
        "model.layers.0.mlp.down_proj": 1.398667335510254,
        "model.layers.0.mlp": 1.398667335510254,
        "model.layers.0.post_feedforward_layernorm": 7.939406871795654,
        "model.layers.0": 8.499785423278809,
        "model.layers.1.input_layernorm": 9.11159896850586,
        "model.layers.1.post_attention_layernorm": 12.350578308105469,
        "model.layers.1.pre_feedforward_layernorm": 22.466951370239258,
        "model.layers.1.mlp.gate_proj": 27.459199905395508,
        "model.layers.1.mlp.up_proj": 11.572813987731934,
        "model.layers.1.mlp.down_proj": 1.6137402057647705,
        "model.layers.1.mlp": 1.6137402057647705,
        "model.layers.1.post_feedforward_layernorm": 11.06828784942627,
        "model.layers.1": 16.50617790222168,
        "model.layers.2.input_layernorm": 15.54112720489502,
        "model.layers.2.post_attention_layernorm": 8.74521541595459,
        "model.layers.2.pre_feedforward_layernorm": 17.300739288330078
      }
    },
    {
      "sequence": "What What What is version control?",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.875374172044838,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 16.08699607849121,
        "model.layers.0.pre_feedforward_layernorm": 20.598373413085938,
        "model.layers.0.mlp.gate_proj": 21.865270614624023,
        "model.layers.0.mlp.up_proj": 9.34611988067627,
        "model.layers.0.mlp.down_proj": 2.2834312915802,
        "model.layers.0.mlp": 2.2834312915802,
        "model.layers.0.post_feedforward_layernorm": 10.873833656311035,
        "model.layers.0": 12.541585922241211,
        "model.layers.1.input_layernorm": 12.012828826904297,
        "model.layers.1.post_attention_layernorm": 9.365660667419434,
        "model.layers.1.pre_feedforward_layernorm": 17.670034408569336,
        "model.layers.1.mlp.gate_proj": 17.450761795043945,
        "model.layers.1.mlp.up_proj": 10.725817680358887,
        "model.layers.1.mlp.down_proj": 1.3640453815460205,
        "model.layers.1.mlp": 1.3640453815460205,
        "model.layers.1.post_feedforward_layernorm": 8.101049423217773,
        "model.layers.1": 15.153409957885742,
        "model.layers.2.input_layernorm": 13.608234405517578,
        "model.layers.2.post_attention_layernorm": 5.737951278686523,
        "model.layers.2.pre_feedforward_layernorm": 13.62258529663086
      }
    },
    {
      "sequence": "The following is a test: What is the meaning of life?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.940948050955068,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.317299842834473,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 9.935776710510254,
        "model.layers.0.pre_feedforward_layernorm": 13.319755554199219,
        "model.layers.0.mlp.gate_proj": 15.083657264709473,
        "model.layers.0.mlp.up_proj": 6.8617095947265625,
        "model.layers.0.mlp.down_proj": 1.4061070680618286,
        "model.layers.0.mlp": 1.4061070680618286,
        "model.layers.0.post_feedforward_layernorm": 7.545894145965576,
        "model.layers.0": 8.583000183105469,
        "model.layers.1.input_layernorm": 9.445867538452148,
        "model.layers.1.post_attention_layernorm": 12.356301307678223,
        "model.layers.1.pre_feedforward_layernorm": 24.207120895385742,
        "model.layers.1.mlp.gate_proj": 28.204172134399414,
        "model.layers.1.mlp.up_proj": 12.325187683105469,
        "model.layers.1.mlp.down_proj": 1.704363226890564,
        "model.layers.1.mlp": 1.704363226890564,
        "model.layers.1.post_feedforward_layernorm": 10.318229675292969,
        "model.layers.1": 15.088065147399902,
        "model.layers.2.input_layernorm": 15.636770248413086,
        "model.layers.2.post_attention_layernorm": 7.736236572265625,
        "model.layers.2.pre_feedforward_layernorm": 17.455820083618164
      }
    },
    {
      "sequence": "Explain  database  normalization.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 9.990457493325938,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.889543533325195,
        "model.layers.0.pre_feedforward_layernorm": 17.80035972595215,
        "model.layers.0.mlp.gate_proj": 20.877124786376953,
        "model.layers.0.mlp.up_proj": 9.951554298400879,
        "model.layers.0.mlp.down_proj": 1.9281225204467773,
        "model.layers.0.mlp": 1.9281225204467773,
        "model.layers.0.post_feedforward_layernorm": 8.491816520690918,
        "model.layers.0": 11.312690734863281,
        "model.layers.1.input_layernorm": 11.878929138183594,
        "model.layers.1.post_attention_layernorm": 9.843119621276855,
        "model.layers.1.pre_feedforward_layernorm": 20.86214828491211,
        "model.layers.1.mlp.gate_proj": 21.01047706604004,
        "model.layers.1.mlp.up_proj": 12.026033401489258,
        "model.layers.1.mlp.down_proj": 1.328301191329956,
        "model.layers.1.mlp": 1.328301191329956,
        "model.layers.1.post_feedforward_layernorm": 10.050152778625488,
        "model.layers.1": 14.605819702148438,
        "model.layers.2.input_layernorm": 15.296298027038574,
        "model.layers.2.post_attention_layernorm": 6.690505027770996,
        "model.layers.2.pre_feedforward_layernorm": 16.30118751525879
      }
    },
    {
      "sequence": "The following is a test: How does music theory work?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.003648291463437,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.479999542236328,
        "model.layers.0.pre_feedforward_layernorm": 13.242818832397461,
        "model.layers.0.mlp.gate_proj": 12.973062515258789,
        "model.layers.0.mlp.up_proj": 7.064133644104004,
        "model.layers.0.mlp.down_proj": 1.552597999572754,
        "model.layers.0.mlp": 1.552597999572754,
        "model.layers.0.post_feedforward_layernorm": 8.71440601348877,
        "model.layers.0": 9.127602577209473,
        "model.layers.1.input_layernorm": 9.655678749084473,
        "model.layers.1.post_attention_layernorm": 13.902202606201172,
        "model.layers.1.pre_feedforward_layernorm": 23.06264877319336,
        "model.layers.1.mlp.gate_proj": 25.497760772705078,
        "model.layers.1.mlp.up_proj": 12.15317153930664,
        "model.layers.1.mlp.down_proj": 1.4697030782699585,
        "model.layers.1.mlp": 1.4697030782699585,
        "model.layers.1.post_feedforward_layernorm": 10.553768157958984,
        "model.layers.1": 17.373470306396484,
        "model.layers.2.input_layernorm": 15.975055694580078,
        "model.layers.2.post_attention_layernorm": 8.015416145324707,
        "model.layers.2.pre_feedforward_layernorm": 16.920516967773438
      }
    },
    {
      "sequence": "The following is a test: How do I change a tire?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.010588873987613,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.317299842834473,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.999029159545898,
        "model.layers.0.pre_feedforward_layernorm": 14.523421287536621,
        "model.layers.0.mlp.gate_proj": 15.068471908569336,
        "model.layers.0.mlp.up_proj": 7.168881893157959,
        "model.layers.0.mlp.down_proj": 1.6132005453109741,
        "model.layers.0.mlp": 1.6132005453109741,
        "model.layers.0.post_feedforward_layernorm": 8.20257568359375,
        "model.layers.0": 9.604068756103516,
        "model.layers.1.input_layernorm": 10.26267147064209,
        "model.layers.1.post_attention_layernorm": 11.934527397155762,
        "model.layers.1.pre_feedforward_layernorm": 24.452852249145508,
        "model.layers.1.mlp.gate_proj": 27.547121047973633,
        "model.layers.1.mlp.up_proj": 11.950621604919434,
        "model.layers.1.mlp.down_proj": 1.5164185762405396,
        "model.layers.1.mlp": 1.5164185762405396,
        "model.layers.1.post_feedforward_layernorm": 9.88541030883789,
        "model.layers.1": 15.464128494262695,
        "model.layers.2.input_layernorm": 15.074002265930176,
        "model.layers.2.post_attention_layernorm": 7.520917892456055,
        "model.layers.2.pre_feedforward_layernorm": 16.008304595947266
      }
    },
    {
      "sequence": "The following is a test: What was the Industrial Revolution?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.056962168735007,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 9.857114791870117,
        "model.layers.0.pre_feedforward_layernorm": 12.161561012268066,
        "model.layers.0.mlp.gate_proj": 13.121564865112305,
        "model.layers.0.mlp.up_proj": 6.404843330383301,
        "model.layers.0.mlp.down_proj": 1.3829824924468994,
        "model.layers.0.mlp": 1.3829824924468994,
        "model.layers.0.post_feedforward_layernorm": 7.259300231933594,
        "model.layers.0": 8.291840553283691,
        "model.layers.1.input_layernorm": 8.782747268676758,
        "model.layers.1.post_attention_layernorm": 13.9049654006958,
        "model.layers.1.pre_feedforward_layernorm": 24.322887420654297,
        "model.layers.1.mlp.gate_proj": 29.946044921875,
        "model.layers.1.mlp.up_proj": 12.590880393981934,
        "model.layers.1.mlp.down_proj": 1.7035568952560425,
        "model.layers.1.mlp": 1.7035568952560425,
        "model.layers.1.post_feedforward_layernorm": 11.813525199890137,
        "model.layers.1": 16.911197662353516,
        "model.layers.2.input_layernorm": 16.244291305541992,
        "model.layers.2.post_attention_layernorm": 7.93787145614624,
        "model.layers.2.pre_feedforward_layernorm": 17.258819580078125
      }
    },
    {
      "sequence": "The following is a test: Describe time management methods.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.086097354474274,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.248085021972656,
        "model.layers.0.pre_feedforward_layernorm": 12.128581047058105,
        "model.layers.0.mlp.gate_proj": 12.56747055053711,
        "model.layers.0.mlp.up_proj": 6.6117448806762695,
        "model.layers.0.mlp.down_proj": 1.8572444915771484,
        "model.layers.0.mlp": 1.8572444915771484,
        "model.layers.0.post_feedforward_layernorm": 5.648033618927002,
        "model.layers.0": 10.466400146484375,
        "model.layers.1.input_layernorm": 8.353304862976074,
        "model.layers.1.post_attention_layernorm": 13.704940795898438,
        "model.layers.1.pre_feedforward_layernorm": 23.115041732788086,
        "model.layers.1.mlp.gate_proj": 26.83033561706543,
        "model.layers.1.mlp.up_proj": 12.928674697875977,
        "model.layers.1.mlp.down_proj": 1.6246107816696167,
        "model.layers.1.mlp": 1.6246107816696167,
        "model.layers.1.post_feedforward_layernorm": 10.546427726745605,
        "model.layers.1": 16.70128631591797,
        "model.layers.2.input_layernorm": 16.348207473754883,
        "model.layers.2.post_attention_layernorm": 9.785650253295898,
        "model.layers.2.pre_feedforward_layernorm": 19.684537887573242
      }
    },
    {
      "sequence": "Explain  probability  theory.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.106759641481483,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.213557243347168,
        "model.layers.0.pre_feedforward_layernorm": 19.281373977661133,
        "model.layers.0.mlp.gate_proj": 19.798221588134766,
        "model.layers.0.mlp.up_proj": 10.194611549377441,
        "model.layers.0.mlp.down_proj": 2.003655195236206,
        "model.layers.0.mlp": 2.003655195236206,
        "model.layers.0.post_feedforward_layernorm": 8.418693542480469,
        "model.layers.0": 11.565942764282227,
        "model.layers.1.input_layernorm": 11.71532917022705,
        "model.layers.1.post_attention_layernorm": 10.31726360321045,
        "model.layers.1.pre_feedforward_layernorm": 21.083295822143555,
        "model.layers.1.mlp.gate_proj": 22.495071411132812,
        "model.layers.1.mlp.up_proj": 12.102215766906738,
        "model.layers.1.mlp.down_proj": 1.3426445722579956,
        "model.layers.1.mlp": 1.3426445722579956,
        "model.layers.1.post_feedforward_layernorm": 10.14047908782959,
        "model.layers.1": 13.876274108886719,
        "model.layers.2.input_layernorm": 14.544897079467773,
        "model.layers.2.post_attention_layernorm": 6.592763900756836,
        "model.layers.2.pre_feedforward_layernorm": 16.042966842651367
      }
    },
    {
      "sequence": "The following is a test: Explain probability theory.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.122561641361402,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.423189163208008,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.186406135559082,
        "model.layers.0.pre_feedforward_layernorm": 14.444160461425781,
        "model.layers.0.mlp.gate_proj": 14.79468059539795,
        "model.layers.0.mlp.up_proj": 7.637026309967041,
        "model.layers.0.mlp.down_proj": 2.347400188446045,
        "model.layers.0.mlp": 2.347400188446045,
        "model.layers.0.post_feedforward_layernorm": 6.483376979827881,
        "model.layers.0": 13.051334381103516,
        "model.layers.1.input_layernorm": 9.652347564697266,
        "model.layers.1.post_attention_layernorm": 11.446300506591797,
        "model.layers.1.pre_feedforward_layernorm": 20.589740753173828,
        "model.layers.1.mlp.gate_proj": 24.110118865966797,
        "model.layers.1.mlp.up_proj": 11.995086669921875,
        "model.layers.1.mlp.down_proj": 1.721306562423706,
        "model.layers.1.mlp": 1.721306562423706,
        "model.layers.1.post_feedforward_layernorm": 10.837552070617676,
        "model.layers.1": 15.13516616821289,
        "model.layers.2.input_layernorm": 14.717816352844238,
        "model.layers.2.post_attention_layernorm": 9.55212688446045,
        "model.layers.2.pre_feedforward_layernorm": 18.62507438659668
      }
    },
    {
      "sequence": "The following is a test: How does encryption work?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.132197037987087,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.157217025756836,
        "model.layers.0.pre_feedforward_layernorm": 13.826515197753906,
        "model.layers.0.mlp.gate_proj": 13.577197074890137,
        "model.layers.0.mlp.up_proj": 7.284333229064941,
        "model.layers.0.mlp.down_proj": 1.6196486949920654,
        "model.layers.0.mlp": 1.6196486949920654,
        "model.layers.0.post_feedforward_layernorm": 8.739808082580566,
        "model.layers.0": 9.328503608703613,
        "model.layers.1.input_layernorm": 9.984410285949707,
        "model.layers.1.post_attention_layernorm": 12.821144104003906,
        "model.layers.1.pre_feedforward_layernorm": 22.850378036499023,
        "model.layers.1.mlp.gate_proj": 25.943511962890625,
        "model.layers.1.mlp.up_proj": 12.382505416870117,
        "model.layers.1.mlp.down_proj": 1.5495659112930298,
        "model.layers.1.mlp": 1.5495659112930298,
        "model.layers.1.post_feedforward_layernorm": 10.428537368774414,
        "model.layers.1": 17.112958908081055,
        "model.layers.2.input_layernorm": 16.436023712158203,
        "model.layers.2.post_attention_layernorm": 8.564767837524414,
        "model.layers.2.pre_feedforward_layernorm": 17.916484832763672
      }
    },
    {
      "sequence": "The following is a test: How do transformers process language?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.17181276238483,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.471138954162598,
        "model.layers.0.pre_feedforward_layernorm": 13.007118225097656,
        "model.layers.0.mlp.gate_proj": 12.880149841308594,
        "model.layers.0.mlp.up_proj": 6.831348419189453,
        "model.layers.0.mlp.down_proj": 1.5262194871902466,
        "model.layers.0.mlp": 1.5262194871902466,
        "model.layers.0.post_feedforward_layernorm": 7.685654640197754,
        "model.layers.0": 8.886659622192383,
        "model.layers.1.input_layernorm": 9.289542198181152,
        "model.layers.1.post_attention_layernorm": 13.965575218200684,
        "model.layers.1.pre_feedforward_layernorm": 24.447294235229492,
        "model.layers.1.mlp.gate_proj": 28.312204360961914,
        "model.layers.1.mlp.up_proj": 12.983362197875977,
        "model.layers.1.mlp.down_proj": 1.613770842552185,
        "model.layers.1.mlp": 1.613770842552185,
        "model.layers.1.post_feedforward_layernorm": 11.223384857177734,
        "model.layers.1": 17.78774070739746,
        "model.layers.2.input_layernorm": 16.329954147338867,
        "model.layers.2.post_attention_layernorm": 8.076895713806152,
        "model.layers.2.pre_feedforward_layernorm": 17.166093826293945
      }
    },
    {
      "sequence": "The following is a test: Explain calculus concepts.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.175313617872154,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.423189163208008,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.343389511108398,
        "model.layers.0.pre_feedforward_layernorm": 14.979776382446289,
        "model.layers.0.mlp.gate_proj": 15.17419719696045,
        "model.layers.0.mlp.up_proj": 7.933239936828613,
        "model.layers.0.mlp.down_proj": 2.2631499767303467,
        "model.layers.0.mlp": 2.2631499767303467,
        "model.layers.0.post_feedforward_layernorm": 6.465019226074219,
        "model.layers.0": 12.366693496704102,
        "model.layers.1.input_layernorm": 9.863241195678711,
        "model.layers.1.post_attention_layernorm": 11.676858901977539,
        "model.layers.1.pre_feedforward_layernorm": 21.15196990966797,
        "model.layers.1.mlp.gate_proj": 23.714277267456055,
        "model.layers.1.mlp.up_proj": 12.073166847229004,
        "model.layers.1.mlp.down_proj": 1.7318756580352783,
        "model.layers.1.mlp": 1.7318756580352783,
        "model.layers.1.post_feedforward_layernorm": 10.828802108764648,
        "model.layers.1": 15.34869384765625,
        "model.layers.2.input_layernorm": 15.387138366699219,
        "model.layers.2.post_attention_layernorm": 9.32479190826416,
        "model.layers.2.pre_feedforward_layernorm": 17.987716674804688
      }
    },
    {
      "sequence": "What  is  artificial  intelligence?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.179179699524589,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.102811813354492,
        "model.layers.0.pre_feedforward_layernorm": 19.90103530883789,
        "model.layers.0.mlp.gate_proj": 21.122140884399414,
        "model.layers.0.mlp.up_proj": 9.702414512634277,
        "model.layers.0.mlp.down_proj": 1.737757921218872,
        "model.layers.0.mlp": 1.737757921218872,
        "model.layers.0.post_feedforward_layernorm": 7.839199542999268,
        "model.layers.0": 11.595325469970703,
        "model.layers.1.input_layernorm": 12.779385566711426,
        "model.layers.1.post_attention_layernorm": 10.767704010009766,
        "model.layers.1.pre_feedforward_layernorm": 20.403947830200195,
        "model.layers.1.mlp.gate_proj": 17.699085235595703,
        "model.layers.1.mlp.up_proj": 11.876128196716309,
        "model.layers.1.mlp.down_proj": 1.4454878568649292,
        "model.layers.1.mlp": 1.4454878568649292,
        "model.layers.1.post_feedforward_layernorm": 9.12333869934082,
        "model.layers.1": 14.085759162902832,
        "model.layers.2.input_layernorm": 14.432583808898926,
        "model.layers.2.post_attention_layernorm": 10.896953582763672,
        "model.layers.2.pre_feedforward_layernorm": 16.88251495361328
      }
    },
    {
      "sequence": "Explain  calculus  concepts.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.183525583018428,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.374725341796875,
        "model.layers.0.pre_feedforward_layernorm": 18.760787963867188,
        "model.layers.0.mlp.gate_proj": 20.761764526367188,
        "model.layers.0.mlp.up_proj": 10.219712257385254,
        "model.layers.0.mlp.down_proj": 1.9790048599243164,
        "model.layers.0.mlp": 1.9790048599243164,
        "model.layers.0.post_feedforward_layernorm": 8.567493438720703,
        "model.layers.0": 11.455465316772461,
        "model.layers.1.input_layernorm": 12.22555923461914,
        "model.layers.1.post_attention_layernorm": 10.432912826538086,
        "model.layers.1.pre_feedforward_layernorm": 21.1948299407959,
        "model.layers.1.mlp.gate_proj": 21.93470001220703,
        "model.layers.1.mlp.up_proj": 12.113408088684082,
        "model.layers.1.mlp.down_proj": 1.6001920700073242,
        "model.layers.1.mlp": 1.6001920700073242,
        "model.layers.1.post_feedforward_layernorm": 10.109360694885254,
        "model.layers.1": 14.394563674926758,
        "model.layers.2.input_layernorm": 15.296442031860352,
        "model.layers.2.post_attention_layernorm": 6.713983058929443,
        "model.layers.2.pre_feedforward_layernorm": 16.127071380615234
      }
    },
    {
      "sequence": "The following is a test: Explain database normalization.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.18370354693869,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.423189163208008,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.99372386932373,
        "model.layers.0.pre_feedforward_layernorm": 14.464439392089844,
        "model.layers.0.mlp.gate_proj": 14.662753105163574,
        "model.layers.0.mlp.up_proj": 7.567325592041016,
        "model.layers.0.mlp.down_proj": 2.182359218597412,
        "model.layers.0.mlp": 2.182359218597412,
        "model.layers.0.post_feedforward_layernorm": 6.389129638671875,
        "model.layers.0": 12.392228126525879,
        "model.layers.1.input_layernorm": 9.609314918518066,
        "model.layers.1.post_attention_layernorm": 11.593782424926758,
        "model.layers.1.pre_feedforward_layernorm": 21.727859497070312,
        "model.layers.1.mlp.gate_proj": 25.051271438598633,
        "model.layers.1.mlp.up_proj": 12.405915260314941,
        "model.layers.1.mlp.down_proj": 1.8986215591430664,
        "model.layers.1.mlp": 1.8986215591430664,
        "model.layers.1.post_feedforward_layernorm": 10.926097869873047,
        "model.layers.1": 15.338554382324219,
        "model.layers.2.input_layernorm": 15.195137023925781,
        "model.layers.2.post_attention_layernorm": 8.887958526611328,
        "model.layers.2.pre_feedforward_layernorm": 18.434539794921875
      }
    },
    {
      "sequence": "Translate:  Bonjour  means  hello  in  French.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.224680133487867,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.19483757019043,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.074588775634766,
        "model.layers.0.pre_feedforward_layernorm": 18.154544830322266,
        "model.layers.0.mlp.gate_proj": 19.467124938964844,
        "model.layers.0.mlp.up_proj": 8.79720401763916,
        "model.layers.0.mlp.down_proj": 1.6743489503860474,
        "model.layers.0.mlp": 1.6743489503860474,
        "model.layers.0.post_feedforward_layernorm": 7.638887405395508,
        "model.layers.0": 10.415501594543457,
        "model.layers.1.input_layernorm": 12.24315357208252,
        "model.layers.1.post_attention_layernorm": 10.23928451538086,
        "model.layers.1.pre_feedforward_layernorm": 21.617979049682617,
        "model.layers.1.mlp.gate_proj": 24.836801528930664,
        "model.layers.1.mlp.up_proj": 12.562148094177246,
        "model.layers.1.mlp.down_proj": 1.535601258277893,
        "model.layers.1.mlp": 1.535601258277893,
        "model.layers.1.post_feedforward_layernorm": 11.097420692443848,
        "model.layers.1": 12.832404136657715,
        "model.layers.2.input_layernorm": 14.072602272033691,
        "model.layers.2.post_attention_layernorm": 9.358774185180664,
        "model.layers.2.pre_feedforward_layernorm": 17.144485473632812
      }
    },
    {
      "sequence": "What  is  consciousness?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.249869989312213,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.474601745605469,
        "model.layers.0.pre_feedforward_layernorm": 22.216638565063477,
        "model.layers.0.mlp.gate_proj": 24.769678115844727,
        "model.layers.0.mlp.up_proj": 9.94245433807373,
        "model.layers.0.mlp.down_proj": 1.698386549949646,
        "model.layers.0.mlp": 1.698386549949646,
        "model.layers.0.post_feedforward_layernorm": 8.383257865905762,
        "model.layers.0": 11.029317855834961,
        "model.layers.1.input_layernorm": 12.309982299804688,
        "model.layers.1.post_attention_layernorm": 11.15976333618164,
        "model.layers.1.pre_feedforward_layernorm": 19.63223648071289,
        "model.layers.1.mlp.gate_proj": 19.34320831298828,
        "model.layers.1.mlp.up_proj": 11.411619186401367,
        "model.layers.1.mlp.down_proj": 1.1947036981582642,
        "model.layers.1.mlp": 1.1947036981582642,
        "model.layers.1.post_feedforward_layernorm": 8.33733081817627,
        "model.layers.1": 13.44776725769043,
        "model.layers.2.input_layernorm": 13.502942085266113,
        "model.layers.2.post_attention_layernorm": 10.329039573669434,
        "model.layers.2.pre_feedforward_layernorm": 16.29107666015625
      }
    },
    {
      "sequence": "Explain  cooking  techniques.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.25976272251295,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.888723373413086,
        "model.layers.0.pre_feedforward_layernorm": 17.905912399291992,
        "model.layers.0.mlp.gate_proj": 19.844087600708008,
        "model.layers.0.mlp.up_proj": 9.596338272094727,
        "model.layers.0.mlp.down_proj": 1.7462165355682373,
        "model.layers.0.mlp": 1.7462165355682373,
        "model.layers.0.post_feedforward_layernorm": 7.665403366088867,
        "model.layers.0": 10.89281940460205,
        "model.layers.1.input_layernorm": 11.283590316772461,
        "model.layers.1.post_attention_layernorm": 12.47672176361084,
        "model.layers.1.pre_feedforward_layernorm": 22.619691848754883,
        "model.layers.1.mlp.gate_proj": 24.32285499572754,
        "model.layers.1.mlp.up_proj": 12.728178977966309,
        "model.layers.1.mlp.down_proj": 1.5231547355651855,
        "model.layers.1.mlp": 1.5231547355651855,
        "model.layers.1.post_feedforward_layernorm": 10.298161506652832,
        "model.layers.1": 14.592325210571289,
        "model.layers.2.input_layernorm": 15.62074089050293,
        "model.layers.2.post_attention_layernorm": 6.650337219238281,
        "model.layers.2.pre_feedforward_layernorm": 16.669998168945312
      }
    },
    {
      "sequence": "The following is a test: What causes climate change?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.259912967681885,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.575504302978516,
        "model.layers.0.pre_feedforward_layernorm": 13.5829496383667,
        "model.layers.0.mlp.gate_proj": 14.578485488891602,
        "model.layers.0.mlp.up_proj": 7.467726230621338,
        "model.layers.0.mlp.down_proj": 1.5913199186325073,
        "model.layers.0.mlp": 1.5913199186325073,
        "model.layers.0.post_feedforward_layernorm": 8.28646183013916,
        "model.layers.0": 9.426156044006348,
        "model.layers.1.input_layernorm": 10.030848503112793,
        "model.layers.1.post_attention_layernorm": 14.725133895874023,
        "model.layers.1.pre_feedforward_layernorm": 23.370975494384766,
        "model.layers.1.mlp.gate_proj": 27.31880760192871,
        "model.layers.1.mlp.up_proj": 12.418079376220703,
        "model.layers.1.mlp.down_proj": 1.5225528478622437,
        "model.layers.1.mlp": 1.5225528478622437,
        "model.layers.1.post_feedforward_layernorm": 11.05184555053711,
        "model.layers.1": 17.274385452270508,
        "model.layers.2.input_layernorm": 15.818565368652344,
        "model.layers.2.post_attention_layernorm": 7.564023494720459,
        "model.layers.2.pre_feedforward_layernorm": 16.912498474121094
      }
    },
    {
      "sequence": "The following is a test: Explain ethical dilemmas.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.269684086675229,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.423189163208008,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.256665229797363,
        "model.layers.0.pre_feedforward_layernorm": 14.552695274353027,
        "model.layers.0.mlp.gate_proj": 14.553028106689453,
        "model.layers.0.mlp.up_proj": 7.757040977478027,
        "model.layers.0.mlp.down_proj": 2.1712112426757812,
        "model.layers.0.mlp": 2.1712112426757812,
        "model.layers.0.post_feedforward_layernorm": 6.358731269836426,
        "model.layers.0": 12.140345573425293,
        "model.layers.1.input_layernorm": 9.39296817779541,
        "model.layers.1.post_attention_layernorm": 12.813921928405762,
        "model.layers.1.pre_feedforward_layernorm": 21.0410213470459,
        "model.layers.1.mlp.gate_proj": 24.863779067993164,
        "model.layers.1.mlp.up_proj": 12.234111785888672,
        "model.layers.1.mlp.down_proj": 1.9646430015563965,
        "model.layers.1.mlp": 1.9646430015563965,
        "model.layers.1.post_feedforward_layernorm": 11.824641227722168,
        "model.layers.1": 15.047581672668457,
        "model.layers.2.input_layernorm": 15.107901573181152,
        "model.layers.2.post_attention_layernorm": 9.463789939880371,
        "model.layers.2.pre_feedforward_layernorm": 19.099613189697266
      }
    },
    {
      "sequence": "Describe  free  will  vs  determinism.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.293534392895905,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.852492332458496,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 9.78408145904541,
        "model.layers.0.pre_feedforward_layernorm": 17.079450607299805,
        "model.layers.0.mlp.gate_proj": 18.128517150878906,
        "model.layers.0.mlp.up_proj": 8.647355079650879,
        "model.layers.0.mlp.down_proj": 1.4811041355133057,
        "model.layers.0.mlp": 1.4811041355133057,
        "model.layers.0.post_feedforward_layernorm": 6.593494892120361,
        "model.layers.0": 8.917501449584961,
        "model.layers.1.input_layernorm": 9.962271690368652,
        "model.layers.1.post_attention_layernorm": 13.352606773376465,
        "model.layers.1.pre_feedforward_layernorm": 22.469411849975586,
        "model.layers.1.mlp.gate_proj": 24.2524471282959,
        "model.layers.1.mlp.up_proj": 13.480951309204102,
        "model.layers.1.mlp.down_proj": 1.815539002418518,
        "model.layers.1.mlp": 1.815539002418518,
        "model.layers.1.post_feedforward_layernorm": 13.595654487609863,
        "model.layers.1": 15.06749439239502,
        "model.layers.2.input_layernorm": 15.82485580444336,
        "model.layers.2.post_attention_layernorm": 7.191551685333252,
        "model.layers.2.pre_feedforward_layernorm": 17.957866668701172
      }
    },
    {
      "sequence": "The following is a test: What are the pros and cons?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.311412365540214,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.317299842834473,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.06165599822998,
        "model.layers.0.pre_feedforward_layernorm": 14.06127643585205,
        "model.layers.0.mlp.gate_proj": 15.04113483428955,
        "model.layers.0.mlp.up_proj": 7.048051834106445,
        "model.layers.0.mlp.down_proj": 1.43649423122406,
        "model.layers.0.mlp": 1.43649423122406,
        "model.layers.0.post_feedforward_layernorm": 8.691692352294922,
        "model.layers.0": 8.909590721130371,
        "model.layers.1.input_layernorm": 9.991455078125,
        "model.layers.1.post_attention_layernorm": 12.85510540008545,
        "model.layers.1.pre_feedforward_layernorm": 25.14795684814453,
        "model.layers.1.mlp.gate_proj": 29.79466438293457,
        "model.layers.1.mlp.up_proj": 12.526139259338379,
        "model.layers.1.mlp.down_proj": 1.7083156108856201,
        "model.layers.1.mlp": 1.7083156108856201,
        "model.layers.1.post_feedforward_layernorm": 11.008164405822754,
        "model.layers.1": 16.41912269592285,
        "model.layers.2.input_layernorm": 15.864897727966309,
        "model.layers.2.post_attention_layernorm": 7.691446304321289,
        "model.layers.2.pre_feedforward_layernorm": 16.44321060180664
      }
    },
    {
      "sequence": "What  causes  climate  change?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.38048907984858,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.347290992736816,
        "model.layers.0.pre_feedforward_layernorm": 19.746809005737305,
        "model.layers.0.mlp.gate_proj": 21.931598663330078,
        "model.layers.0.mlp.up_proj": 10.348045349121094,
        "model.layers.0.mlp.down_proj": 1.9573649168014526,
        "model.layers.0.mlp": 1.9573649168014526,
        "model.layers.0.post_feedforward_layernorm": 8.585892677307129,
        "model.layers.0": 12.613045692443848,
        "model.layers.1.input_layernorm": 13.974067687988281,
        "model.layers.1.post_attention_layernorm": 10.799948692321777,
        "model.layers.1.pre_feedforward_layernorm": 21.343591690063477,
        "model.layers.1.mlp.gate_proj": 20.815000534057617,
        "model.layers.1.mlp.up_proj": 12.140717506408691,
        "model.layers.1.mlp.down_proj": 1.337889313697815,
        "model.layers.1.mlp": 1.337889313697815,
        "model.layers.1.post_feedforward_layernorm": 9.85315990447998,
        "model.layers.1": 14.870359420776367,
        "model.layers.2.input_layernorm": 14.680201530456543,
        "model.layers.2.post_attention_layernorm": 6.886275291442871,
        "model.layers.2.pre_feedforward_layernorm": 14.68042278289795
      }
    },
    {
      "sequence": "Describe  ancient  Egyptian  civilization.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.405704083650008,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.641499519348145,
        "model.layers.0.pre_feedforward_layernorm": 21.326091766357422,
        "model.layers.0.mlp.gate_proj": 23.26774024963379,
        "model.layers.0.mlp.up_proj": 10.579350471496582,
        "model.layers.0.mlp.down_proj": 1.7503973245620728,
        "model.layers.0.mlp": 1.7503973245620728,
        "model.layers.0.post_feedforward_layernorm": 7.507179260253906,
        "model.layers.0": 11.284920692443848,
        "model.layers.1.input_layernorm": 12.264379501342773,
        "model.layers.1.post_attention_layernorm": 9.547597885131836,
        "model.layers.1.pre_feedforward_layernorm": 22.695249557495117,
        "model.layers.1.mlp.gate_proj": 21.171781539916992,
        "model.layers.1.mlp.up_proj": 11.802467346191406,
        "model.layers.1.mlp.down_proj": 1.378266453742981,
        "model.layers.1.mlp": 1.378266453742981,
        "model.layers.1.post_feedforward_layernorm": 10.177908897399902,
        "model.layers.1": 13.644261360168457,
        "model.layers.2.input_layernorm": 14.677420616149902,
        "model.layers.2.post_attention_layernorm": 7.1319580078125,
        "model.layers.2.pre_feedforward_layernorm": 15.723407745361328
      }
    },
    {
      "sequence": "The following is a test: How did the internet develop?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.425374466439951,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.193499565124512,
        "model.layers.0.pre_feedforward_layernorm": 14.321037292480469,
        "model.layers.0.mlp.gate_proj": 14.568069458007812,
        "model.layers.0.mlp.up_proj": 7.1570963859558105,
        "model.layers.0.mlp.down_proj": 1.4334772825241089,
        "model.layers.0.mlp": 1.4334772825241089,
        "model.layers.0.post_feedforward_layernorm": 9.026564598083496,
        "model.layers.0": 9.410502433776855,
        "model.layers.1.input_layernorm": 10.09296989440918,
        "model.layers.1.post_attention_layernorm": 13.566292762756348,
        "model.layers.1.pre_feedforward_layernorm": 24.246103286743164,
        "model.layers.1.mlp.gate_proj": 29.227737426757812,
        "model.layers.1.mlp.up_proj": 12.53386402130127,
        "model.layers.1.mlp.down_proj": 1.6509844064712524,
        "model.layers.1.mlp": 1.6509844064712524,
        "model.layers.1.post_feedforward_layernorm": 11.10292911529541,
        "model.layers.1": 16.517942428588867,
        "model.layers.2.input_layernorm": 15.90678882598877,
        "model.layers.2.post_attention_layernorm": 8.40598201751709,
        "model.layers.2.pre_feedforward_layernorm": 17.009714126586914
      }
    },
    {
      "sequence": "The following is a test: How do vaccines work?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.483982832535453,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.040974617004395,
        "model.layers.0.pre_feedforward_layernorm": 13.974739074707031,
        "model.layers.0.mlp.gate_proj": 14.28635311126709,
        "model.layers.0.mlp.up_proj": 7.460843563079834,
        "model.layers.0.mlp.down_proj": 1.6316972970962524,
        "model.layers.0.mlp": 1.6316972970962524,
        "model.layers.0.post_feedforward_layernorm": 8.401552200317383,
        "model.layers.0": 9.270684242248535,
        "model.layers.1.input_layernorm": 9.978597640991211,
        "model.layers.1.post_attention_layernorm": 13.789806365966797,
        "model.layers.1.pre_feedforward_layernorm": 25.095409393310547,
        "model.layers.1.mlp.gate_proj": 28.544832229614258,
        "model.layers.1.mlp.up_proj": 13.224495887756348,
        "model.layers.1.mlp.down_proj": 1.6561328172683716,
        "model.layers.1.mlp": 1.6561328172683716,
        "model.layers.1.post_feedforward_layernorm": 11.341598510742188,
        "model.layers.1": 17.685768127441406,
        "model.layers.2.input_layernorm": 16.55840492248535,
        "model.layers.2.post_attention_layernorm": 8.130453109741211,
        "model.layers.2.pre_feedforward_layernorm": 17.423625946044922
      }
    },
    {
      "sequence": "Describe  API  design  principles.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.502217106197191,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.712940216064453,
        "model.layers.0.pre_feedforward_layernorm": 19.719118118286133,
        "model.layers.0.mlp.gate_proj": 21.416906356811523,
        "model.layers.0.mlp.up_proj": 10.581093788146973,
        "model.layers.0.mlp.down_proj": 1.8711588382720947,
        "model.layers.0.mlp": 1.8711588382720947,
        "model.layers.0.post_feedforward_layernorm": 8.622941017150879,
        "model.layers.0": 11.86427116394043,
        "model.layers.1.input_layernorm": 12.771470069885254,
        "model.layers.1.post_attention_layernorm": 10.571044921875,
        "model.layers.1.pre_feedforward_layernorm": 20.723743438720703,
        "model.layers.1.mlp.gate_proj": 21.69182586669922,
        "model.layers.1.mlp.up_proj": 12.79881763458252,
        "model.layers.1.mlp.down_proj": 1.4241771697998047,
        "model.layers.1.mlp": 1.4241771697998047,
        "model.layers.1.post_feedforward_layernorm": 10.347251892089844,
        "model.layers.1": 15.350245475769043,
        "model.layers.2.input_layernorm": 15.504284858703613,
        "model.layers.2.post_attention_layernorm": 6.9839043617248535,
        "model.layers.2.pre_feedforward_layernorm": 16.756149291992188
      }
    },
    {
      "sequence": "How  does  encryption  work?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.543726910715518,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.706024169921875,
        "model.layers.0.pre_feedforward_layernorm": 18.297975540161133,
        "model.layers.0.mlp.gate_proj": 19.740488052368164,
        "model.layers.0.mlp.up_proj": 10.376991271972656,
        "model.layers.0.mlp.down_proj": 1.6890610456466675,
        "model.layers.0.mlp": 1.6890610456466675,
        "model.layers.0.post_feedforward_layernorm": 9.379199028015137,
        "model.layers.0": 11.711406707763672,
        "model.layers.1.input_layernorm": 12.833831787109375,
        "model.layers.1.post_attention_layernorm": 12.961490631103516,
        "model.layers.1.pre_feedforward_layernorm": 22.727672576904297,
        "model.layers.1.mlp.gate_proj": 23.28876495361328,
        "model.layers.1.mlp.up_proj": 12.632278442382812,
        "model.layers.1.mlp.down_proj": 1.4208433628082275,
        "model.layers.1.mlp": 1.4208433628082275,
        "model.layers.1.post_feedforward_layernorm": 9.873440742492676,
        "model.layers.1": 14.434202194213867,
        "model.layers.2.input_layernorm": 14.416214942932129,
        "model.layers.2.post_attention_layernorm": 8.737351417541504,
        "model.layers.2.pre_feedforward_layernorm": 15.62426471710205
      }
    },
    {
      "sequence": "The following is a test: What is consciousness?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.616907700248387,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.423189163208008,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.241415023803711,
        "model.layers.0.pre_feedforward_layernorm": 14.919915199279785,
        "model.layers.0.mlp.gate_proj": 15.680791854858398,
        "model.layers.0.mlp.up_proj": 8.034968376159668,
        "model.layers.0.mlp.down_proj": 1.7834362983703613,
        "model.layers.0.mlp": 1.7834362983703613,
        "model.layers.0.post_feedforward_layernorm": 8.609235763549805,
        "model.layers.0": 9.874998092651367,
        "model.layers.1.input_layernorm": 10.684831619262695,
        "model.layers.1.post_attention_layernorm": 13.717784881591797,
        "model.layers.1.pre_feedforward_layernorm": 24.453479766845703,
        "model.layers.1.mlp.gate_proj": 28.05565071105957,
        "model.layers.1.mlp.up_proj": 13.060532569885254,
        "model.layers.1.mlp.down_proj": 1.5401501655578613,
        "model.layers.1.mlp": 1.5401501655578613,
        "model.layers.1.post_feedforward_layernorm": 10.943537712097168,
        "model.layers.1": 17.083261489868164,
        "model.layers.2.input_layernorm": 16.68390464782715,
        "model.layers.2.post_attention_layernorm": 8.165521621704102,
        "model.layers.2.pre_feedforward_layernorm": 17.9086856842041
      }
    },
    {
      "sequence": "What  was  the  Industrial  Revolution?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.623938208040984,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.252198219299316,
        "model.layers.0.pre_feedforward_layernorm": 21.309412002563477,
        "model.layers.0.mlp.gate_proj": 21.140979766845703,
        "model.layers.0.mlp.up_proj": 10.603907585144043,
        "model.layers.0.mlp.down_proj": 1.856565237045288,
        "model.layers.0.mlp": 1.856565237045288,
        "model.layers.0.post_feedforward_layernorm": 8.881855964660645,
        "model.layers.0": 11.191100120544434,
        "model.layers.1.input_layernorm": 12.676908493041992,
        "model.layers.1.post_attention_layernorm": 11.611431121826172,
        "model.layers.1.pre_feedforward_layernorm": 22.341957092285156,
        "model.layers.1.mlp.gate_proj": 21.902624130249023,
        "model.layers.1.mlp.up_proj": 12.606317520141602,
        "model.layers.1.mlp.down_proj": 1.5194411277770996,
        "model.layers.1.mlp": 1.5194411277770996,
        "model.layers.1.post_feedforward_layernorm": 10.997932434082031,
        "model.layers.1": 13.916183471679688,
        "model.layers.2.input_layernorm": 14.16739559173584,
        "model.layers.2.post_attention_layernorm": 8.99472713470459,
        "model.layers.2.pre_feedforward_layernorm": 15.07983684539795
      }
    },
    {
      "sequence": "What  are  investment  strategies?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.647626835366953,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.405592918395996,
        "model.layers.0.pre_feedforward_layernorm": 20.94288444519043,
        "model.layers.0.mlp.gate_proj": 22.978918075561523,
        "model.layers.0.mlp.up_proj": 10.677815437316895,
        "model.layers.0.mlp.down_proj": 1.8241955041885376,
        "model.layers.0.mlp": 1.8241955041885376,
        "model.layers.0.post_feedforward_layernorm": 8.468205451965332,
        "model.layers.0": 12.151456832885742,
        "model.layers.1.input_layernorm": 12.965723037719727,
        "model.layers.1.post_attention_layernorm": 11.767781257629395,
        "model.layers.1.pre_feedforward_layernorm": 21.352703094482422,
        "model.layers.1.mlp.gate_proj": 21.37642478942871,
        "model.layers.1.mlp.up_proj": 12.460009574890137,
        "model.layers.1.mlp.down_proj": 1.4097310304641724,
        "model.layers.1.mlp": 1.4097310304641724,
        "model.layers.1.post_feedforward_layernorm": 9.819618225097656,
        "model.layers.1": 15.004487991333008,
        "model.layers.2.input_layernorm": 14.789666175842285,
        "model.layers.2.post_attention_layernorm": 9.003945350646973,
        "model.layers.2.pre_feedforward_layernorm": 15.718018531799316
      }
    },
    {
      "sequence": "Explain  the  Renaissance  period.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.687537856723951,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.498729705810547,
        "model.layers.0.pre_feedforward_layernorm": 18.793081283569336,
        "model.layers.0.mlp.gate_proj": 20.561141967773438,
        "model.layers.0.mlp.up_proj": 9.934426307678223,
        "model.layers.0.mlp.down_proj": 2.05055570602417,
        "model.layers.0.mlp": 2.05055570602417,
        "model.layers.0.post_feedforward_layernorm": 9.33232593536377,
        "model.layers.0": 11.931041717529297,
        "model.layers.1.input_layernorm": 13.12666130065918,
        "model.layers.1.post_attention_layernorm": 11.583593368530273,
        "model.layers.1.pre_feedforward_layernorm": 22.743345260620117,
        "model.layers.1.mlp.gate_proj": 23.303895950317383,
        "model.layers.1.mlp.up_proj": 13.393011093139648,
        "model.layers.1.mlp.down_proj": 1.5307457447052002,
        "model.layers.1.mlp": 1.5307457447052002,
        "model.layers.1.post_feedforward_layernorm": 10.984679222106934,
        "model.layers.1": 14.769082069396973,
        "model.layers.2.input_layernorm": 16.42064094543457,
        "model.layers.2.post_attention_layernorm": 6.321758270263672,
        "model.layers.2.pre_feedforward_layernorm": 17.409040451049805
      }
    },
    {
      "sequence": "Explain  film  editing  principles.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.784498784853065,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.095429420471191,
        "model.layers.0.pre_feedforward_layernorm": 19.92391014099121,
        "model.layers.0.mlp.gate_proj": 21.3297119140625,
        "model.layers.0.mlp.up_proj": 10.424799919128418,
        "model.layers.0.mlp.down_proj": 2.020514726638794,
        "model.layers.0.mlp": 2.020514726638794,
        "model.layers.0.post_feedforward_layernorm": 9.145827293395996,
        "model.layers.0": 11.798057556152344,
        "model.layers.1.input_layernorm": 12.711395263671875,
        "model.layers.1.post_attention_layernorm": 11.382416725158691,
        "model.layers.1.pre_feedforward_layernorm": 22.903663635253906,
        "model.layers.1.mlp.gate_proj": 23.264341354370117,
        "model.layers.1.mlp.up_proj": 13.291051864624023,
        "model.layers.1.mlp.down_proj": 1.4252997636795044,
        "model.layers.1.mlp": 1.4252997636795044,
        "model.layers.1.post_feedforward_layernorm": 10.914844512939453,
        "model.layers.1": 14.778307914733887,
        "model.layers.2.input_layernorm": 16.20706558227539,
        "model.layers.2.post_attention_layernorm": 6.767073154449463,
        "model.layers.2.pre_feedforward_layernorm": 17.669633865356445
      }
    },
    {
      "sequence": "The following is a test: What is artificial intelligence?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.847742215446804,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.626677513122559,
        "model.layers.0.pre_feedforward_layernorm": 14.476029396057129,
        "model.layers.0.mlp.gate_proj": 15.491294860839844,
        "model.layers.0.mlp.up_proj": 7.740814208984375,
        "model.layers.0.mlp.down_proj": 1.606086254119873,
        "model.layers.0.mlp": 1.606086254119873,
        "model.layers.0.post_feedforward_layernorm": 8.648127555847168,
        "model.layers.0": 9.894916534423828,
        "model.layers.1.input_layernorm": 10.481553077697754,
        "model.layers.1.post_attention_layernorm": 13.915666580200195,
        "model.layers.1.pre_feedforward_layernorm": 25.63052749633789,
        "model.layers.1.mlp.gate_proj": 30.73281478881836,
        "model.layers.1.mlp.up_proj": 13.363377571105957,
        "model.layers.1.mlp.down_proj": 1.6875382661819458,
        "model.layers.1.mlp": 1.6875382661819458,
        "model.layers.1.post_feedforward_layernorm": 10.999452590942383,
        "model.layers.1": 17.535991668701172,
        "model.layers.2.input_layernorm": 17.309690475463867,
        "model.layers.2.post_attention_layernorm": 8.145561218261719,
        "model.layers.2.pre_feedforward_layernorm": 18.570520401000977
      }
    },
    {
      "sequence": "The following is a test: What are investment strategies?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.866338377413543,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.980796813964844,
        "model.layers.0.pre_feedforward_layernorm": 13.436742782592773,
        "model.layers.0.mlp.gate_proj": 14.78656005859375,
        "model.layers.0.mlp.up_proj": 7.233654499053955,
        "model.layers.0.mlp.down_proj": 1.5793300867080688,
        "model.layers.0.mlp": 1.5793300867080688,
        "model.layers.0.post_feedforward_layernorm": 7.77816915512085,
        "model.layers.0": 9.426968574523926,
        "model.layers.1.input_layernorm": 9.567998886108398,
        "model.layers.1.post_attention_layernorm": 14.931370735168457,
        "model.layers.1.pre_feedforward_layernorm": 25.508241653442383,
        "model.layers.1.mlp.gate_proj": 30.333051681518555,
        "model.layers.1.mlp.up_proj": 13.095486640930176,
        "model.layers.1.mlp.down_proj": 1.8209787607192993,
        "model.layers.1.mlp": 1.8209787607192993,
        "model.layers.1.post_feedforward_layernorm": 12.400166511535645,
        "model.layers.1": 18.98046112060547,
        "model.layers.2.input_layernorm": 18.481388092041016,
        "model.layers.2.post_attention_layernorm": 8.389169692993164,
        "model.layers.2.pre_feedforward_layernorm": 19.447132110595703
      }
    },
    {
      "sequence": "What  is  the  Fibonacci  sequence?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.915545152581256,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.39484977722168,
        "model.layers.0.pre_feedforward_layernorm": 21.002805709838867,
        "model.layers.0.mlp.gate_proj": 20.328840255737305,
        "model.layers.0.mlp.up_proj": 10.274116516113281,
        "model.layers.0.mlp.down_proj": 2.049006700515747,
        "model.layers.0.mlp": 2.049006700515747,
        "model.layers.0.post_feedforward_layernorm": 9.430123329162598,
        "model.layers.0": 12.87598991394043,
        "model.layers.1.input_layernorm": 14.183655738830566,
        "model.layers.1.post_attention_layernorm": 11.594057083129883,
        "model.layers.1.pre_feedforward_layernorm": 22.898435592651367,
        "model.layers.1.mlp.gate_proj": 21.343780517578125,
        "model.layers.1.mlp.up_proj": 13.314818382263184,
        "model.layers.1.mlp.down_proj": 1.5402069091796875,
        "model.layers.1.mlp": 1.5402069091796875,
        "model.layers.1.post_feedforward_layernorm": 10.03813648223877,
        "model.layers.1": 14.365960121154785,
        "model.layers.2.input_layernorm": 14.884638786315918,
        "model.layers.2.post_attention_layernorm": 9.988313674926758,
        "model.layers.2.pre_feedforward_layernorm": 17.03679084777832
      }
    },
    {
      "sequence": "How  do  you  solve  quadratic  equations?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.922282737234365,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.206338882446289,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.223787307739258,
        "model.layers.0.pre_feedforward_layernorm": 17.681095123291016,
        "model.layers.0.mlp.gate_proj": 18.1945858001709,
        "model.layers.0.mlp.up_proj": 9.473072052001953,
        "model.layers.0.mlp.down_proj": 1.9609785079956055,
        "model.layers.0.mlp": 1.9609785079956055,
        "model.layers.0.post_feedforward_layernorm": 8.808059692382812,
        "model.layers.0": 12.370039939880371,
        "model.layers.1.input_layernorm": 13.165436744689941,
        "model.layers.1.post_attention_layernorm": 14.925361633300781,
        "model.layers.1.pre_feedforward_layernorm": 23.47994041442871,
        "model.layers.1.mlp.gate_proj": 25.076587677001953,
        "model.layers.1.mlp.up_proj": 13.456266403198242,
        "model.layers.1.mlp.down_proj": 1.8199489116668701,
        "model.layers.1.mlp": 1.8199489116668701,
        "model.layers.1.post_feedforward_layernorm": 13.430534362792969,
        "model.layers.1": 15.444357872009277,
        "model.layers.2.input_layernorm": 15.003392219543457,
        "model.layers.2.post_attention_layernorm": 8.133599281311035,
        "model.layers.2.pre_feedforward_layernorm": 15.578192710876465
      }
    },
    {
      "sequence": "Explain  quantum  computing  principles.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 10.932007001793902,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.1896390914917,
        "model.layers.0.pre_feedforward_layernorm": 20.46501350402832,
        "model.layers.0.mlp.gate_proj": 23.03542709350586,
        "model.layers.0.mlp.up_proj": 10.800782203674316,
        "model.layers.0.mlp.down_proj": 2.228121757507324,
        "model.layers.0.mlp": 2.228121757507324,
        "model.layers.0.post_feedforward_layernorm": 9.434773445129395,
        "model.layers.0": 12.759472846984863,
        "model.layers.1.input_layernorm": 13.213395118713379,
        "model.layers.1.post_attention_layernorm": 10.399922370910645,
        "model.layers.1.pre_feedforward_layernorm": 22.03894805908203,
        "model.layers.1.mlp.gate_proj": 23.361852645874023,
        "model.layers.1.mlp.up_proj": 13.190823554992676,
        "model.layers.1.mlp.down_proj": 1.5969963073730469,
        "model.layers.1.mlp": 1.5969963073730469,
        "model.layers.1.post_feedforward_layernorm": 12.091187477111816,
        "model.layers.1": 15.258877754211426,
        "model.layers.2.input_layernorm": 15.653707504272461,
        "model.layers.2.post_attention_layernorm": 6.6917643547058105,
        "model.layers.2.pre_feedforward_layernorm": 16.656024932861328
      }
    },
    {
      "sequence": "The following is a test: Explain cooking techniques.",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.088411973870318,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.423189163208008,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.736703872680664,
        "model.layers.0.pre_feedforward_layernorm": 15.146294593811035,
        "model.layers.0.mlp.gate_proj": 15.406159400939941,
        "model.layers.0.mlp.up_proj": 8.054125785827637,
        "model.layers.0.mlp.down_proj": 2.2670915126800537,
        "model.layers.0.mlp": 2.2670915126800537,
        "model.layers.0.post_feedforward_layernorm": 6.631972312927246,
        "model.layers.0": 13.127209663391113,
        "model.layers.1.input_layernorm": 10.013223648071289,
        "model.layers.1.post_attention_layernorm": 13.44797134399414,
        "model.layers.1.pre_feedforward_layernorm": 24.615583419799805,
        "model.layers.1.mlp.gate_proj": 29.959941864013672,
        "model.layers.1.mlp.up_proj": 13.541827201843262,
        "model.layers.1.mlp.down_proj": 2.0798048973083496,
        "model.layers.1.mlp": 2.0798048973083496,
        "model.layers.1.post_feedforward_layernorm": 12.38935375213623,
        "model.layers.1": 16.41297721862793,
        "model.layers.2.input_layernorm": 16.195240020751953,
        "model.layers.2.post_attention_layernorm": 9.572471618652344,
        "model.layers.2.pre_feedforward_layernorm": 19.665437698364258
      }
    },
    {
      "sequence": "Prove  the  Pythagorean  theorem.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.093944280043893,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.226203918457031,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.551355361938477,
        "model.layers.0.pre_feedforward_layernorm": 19.847755432128906,
        "model.layers.0.mlp.gate_proj": 21.788463592529297,
        "model.layers.0.mlp.up_proj": 10.599729537963867,
        "model.layers.0.mlp.down_proj": 2.341179370880127,
        "model.layers.0.mlp": 2.341179370880127,
        "model.layers.0.post_feedforward_layernorm": 9.485457420349121,
        "model.layers.0": 12.095610618591309,
        "model.layers.1.input_layernorm": 13.041069030761719,
        "model.layers.1.post_attention_layernorm": 10.757306098937988,
        "model.layers.1.pre_feedforward_layernorm": 21.52937126159668,
        "model.layers.1.mlp.gate_proj": 23.805150985717773,
        "model.layers.1.mlp.up_proj": 12.586845397949219,
        "model.layers.1.mlp.down_proj": 1.6214826107025146,
        "model.layers.1.mlp": 1.6214826107025146,
        "model.layers.1.post_feedforward_layernorm": 12.0586576461792,
        "model.layers.1": 15.12918758392334,
        "model.layers.2.input_layernorm": 15.587138175964355,
        "model.layers.2.post_attention_layernorm": 10.19133472442627,
        "model.layers.2.pre_feedforward_layernorm": 18.954757690429688
      }
    },
    {
      "sequence": "How  did  the  internet  develop?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.261223544245182,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.844846725463867,
        "model.layers.0.pre_feedforward_layernorm": 23.808095932006836,
        "model.layers.0.mlp.gate_proj": 22.934778213500977,
        "model.layers.0.mlp.up_proj": 11.573503494262695,
        "model.layers.0.mlp.down_proj": 1.9384900331497192,
        "model.layers.0.mlp": 1.9384900331497192,
        "model.layers.0.post_feedforward_layernorm": 11.691417694091797,
        "model.layers.0": 13.055893898010254,
        "model.layers.1.input_layernorm": 14.710311889648438,
        "model.layers.1.post_attention_layernorm": 11.041531562805176,
        "model.layers.1.pre_feedforward_layernorm": 22.664209365844727,
        "model.layers.1.mlp.gate_proj": 21.32076072692871,
        "model.layers.1.mlp.up_proj": 12.759875297546387,
        "model.layers.1.mlp.down_proj": 1.485033392906189,
        "model.layers.1.mlp": 1.485033392906189,
        "model.layers.1.post_feedforward_layernorm": 9.584399223327637,
        "model.layers.1": 15.090100288391113,
        "model.layers.2.input_layernorm": 15.42724323272705,
        "model.layers.2.post_attention_layernorm": 7.399738311767578,
        "model.layers.2.pre_feedforward_layernorm": 15.33059024810791
      }
    },
    {
      "sequence": "The following is a test: What caused World War II?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.27837053589199,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.327595710754395,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.91394329071045,
        "model.layers.0.pre_feedforward_layernorm": 13.721267700195312,
        "model.layers.0.mlp.gate_proj": 15.123139381408691,
        "model.layers.0.mlp.up_proj": 7.383810997009277,
        "model.layers.0.mlp.down_proj": 1.4784188270568848,
        "model.layers.0.mlp": 1.4784188270568848,
        "model.layers.0.post_feedforward_layernorm": 7.8549885749816895,
        "model.layers.0": 9.530567169189453,
        "model.layers.1.input_layernorm": 10.136463165283203,
        "model.layers.1.post_attention_layernorm": 15.524335861206055,
        "model.layers.1.pre_feedforward_layernorm": 28.131370544433594,
        "model.layers.1.mlp.gate_proj": 32.61411666870117,
        "model.layers.1.mlp.up_proj": 14.763208389282227,
        "model.layers.1.mlp.down_proj": 1.9912265539169312,
        "model.layers.1.mlp": 1.9912265539169312,
        "model.layers.1.post_feedforward_layernorm": 12.851598739624023,
        "model.layers.1": 18.952693939208984,
        "model.layers.2.input_layernorm": 18.786134719848633,
        "model.layers.2.post_attention_layernorm": 8.661482810974121,
        "model.layers.2.pre_feedforward_layernorm": 19.186513900756836
      }
    },
    {
      "sequence": "How  does  photosynthesis  work?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.341444492340088,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.156083106994629,
        "model.layers.0.pre_feedforward_layernorm": 19.59900665283203,
        "model.layers.0.mlp.gate_proj": 20.73655128479004,
        "model.layers.0.mlp.up_proj": 10.498056411743164,
        "model.layers.0.mlp.down_proj": 1.9183778762817383,
        "model.layers.0.mlp": 1.9183778762817383,
        "model.layers.0.post_feedforward_layernorm": 9.770051002502441,
        "model.layers.0": 12.798395156860352,
        "model.layers.1.input_layernorm": 14.112829208374023,
        "model.layers.1.post_attention_layernorm": 13.480000495910645,
        "model.layers.1.pre_feedforward_layernorm": 24.388044357299805,
        "model.layers.1.mlp.gate_proj": 24.70316505432129,
        "model.layers.1.mlp.up_proj": 13.584235191345215,
        "model.layers.1.mlp.down_proj": 1.4609112739562988,
        "model.layers.1.mlp": 1.4609112739562988,
        "model.layers.1.post_feedforward_layernorm": 10.896538734436035,
        "model.layers.1": 15.818663597106934,
        "model.layers.2.input_layernorm": 15.899018287658691,
        "model.layers.2.post_attention_layernorm": 9.909638404846191,
        "model.layers.2.pre_feedforward_layernorm": 17.11371612548828
      }
    },
    {
      "sequence": "Describe  impressionist  painting  techniques.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.565459956293521,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.706790447235107,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.189374923706055,
        "model.layers.0.pre_feedforward_layernorm": 22.06804656982422,
        "model.layers.0.mlp.gate_proj": 24.31443214416504,
        "model.layers.0.mlp.up_proj": 10.867748260498047,
        "model.layers.0.mlp.down_proj": 2.2775886058807373,
        "model.layers.0.mlp": 2.2775886058807373,
        "model.layers.0.post_feedforward_layernorm": 9.675098419189453,
        "model.layers.0": 13.306473731994629,
        "model.layers.1.input_layernorm": 13.58679485321045,
        "model.layers.1.post_attention_layernorm": 11.890413284301758,
        "model.layers.1.pre_feedforward_layernorm": 24.18492317199707,
        "model.layers.1.mlp.gate_proj": 24.72826385498047,
        "model.layers.1.mlp.up_proj": 13.907361030578613,
        "model.layers.1.mlp.down_proj": 1.6476755142211914,
        "model.layers.1.mlp": 1.6476755142211914,
        "model.layers.1.post_feedforward_layernorm": 12.251686096191406,
        "model.layers.1": 15.934623718261719,
        "model.layers.2.input_layernorm": 16.881826400756836,
        "model.layers.2.post_attention_layernorm": 6.973590850830078,
        "model.layers.2.pre_feedforward_layernorm": 18.687602996826172
      }
    },
    {
      "sequence": "Analyze  Shakespeare's  writing  style.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.5673159516376,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.9965434074401855,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.43975830078125,
        "model.layers.0.pre_feedforward_layernorm": 19.062204360961914,
        "model.layers.0.mlp.gate_proj": 21.50545310974121,
        "model.layers.0.mlp.up_proj": 10.351264953613281,
        "model.layers.0.mlp.down_proj": 2.3276143074035645,
        "model.layers.0.mlp": 2.3276143074035645,
        "model.layers.0.post_feedforward_layernorm": 10.769787788391113,
        "model.layers.0": 13.071395874023438,
        "model.layers.1.input_layernorm": 14.352968215942383,
        "model.layers.1.post_attention_layernorm": 12.34351921081543,
        "model.layers.1.pre_feedforward_layernorm": 24.972469329833984,
        "model.layers.1.mlp.gate_proj": 25.22541046142578,
        "model.layers.1.mlp.up_proj": 14.995965003967285,
        "model.layers.1.mlp.down_proj": 1.6956486701965332,
        "model.layers.1.mlp": 1.6956486701965332,
        "model.layers.1.post_feedforward_layernorm": 11.38642692565918,
        "model.layers.1": 16.372018814086914,
        "model.layers.2.input_layernorm": 18.25728416442871,
        "model.layers.2.post_attention_layernorm": 7.347311019897461,
        "model.layers.2.pre_feedforward_layernorm": 19.551959991455078
      }
    },
    {
      "sequence": "Explain  ethical  dilemmas.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.587455044622006,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.152703285217285,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.785188674926758,
        "model.layers.0.pre_feedforward_layernorm": 18.743940353393555,
        "model.layers.0.mlp.gate_proj": 22.579458236694336,
        "model.layers.0.mlp.up_proj": 10.90850830078125,
        "model.layers.0.mlp.down_proj": 1.8942327499389648,
        "model.layers.0.mlp": 1.8942327499389648,
        "model.layers.0.post_feedforward_layernorm": 8.186805725097656,
        "model.layers.0": 12.204877853393555,
        "model.layers.1.input_layernorm": 13.019927978515625,
        "model.layers.1.post_attention_layernorm": 12.713549613952637,
        "model.layers.1.pre_feedforward_layernorm": 24.31048583984375,
        "model.layers.1.mlp.gate_proj": 26.933231353759766,
        "model.layers.1.mlp.up_proj": 13.973162651062012,
        "model.layers.1.mlp.down_proj": 1.8586170673370361,
        "model.layers.1.mlp": 1.8586170673370361,
        "model.layers.1.post_feedforward_layernorm": 12.594287872314453,
        "model.layers.1": 17.437911987304688,
        "model.layers.2.input_layernorm": 18.529420852661133,
        "model.layers.2.post_attention_layernorm": 7.256448268890381,
        "model.layers.2.pre_feedforward_layernorm": 19.675857543945312
      }
    },
    {
      "sequence": "DESCRIBE API DESIGN PRINCIPLES.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.592731911203135,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 10.001755714416504,
        "model.layers.0.pre_feedforward_layernorm": 16.040372848510742,
        "model.layers.0.mlp.gate_proj": 21.253393173217773,
        "model.layers.0.mlp.up_proj": 9.774092674255371,
        "model.layers.0.mlp.down_proj": 1.939142107963562,
        "model.layers.0.mlp": 1.939142107963562,
        "model.layers.0.post_feedforward_layernorm": 9.522065162658691,
        "model.layers.0": 11.593415260314941,
        "model.layers.1.input_layernorm": 12.496109962463379,
        "model.layers.1.post_attention_layernorm": 15.915460586547852,
        "model.layers.1.pre_feedforward_layernorm": 24.26873779296875,
        "model.layers.1.mlp.gate_proj": 25.437856674194336,
        "model.layers.1.mlp.up_proj": 13.851031303405762,
        "model.layers.1.mlp.down_proj": 2.18599009513855,
        "model.layers.1.mlp": 2.18599009513855,
        "model.layers.1.post_feedforward_layernorm": 16.126996994018555,
        "model.layers.1": 15.749510765075684,
        "model.layers.2.input_layernorm": 16.070323944091797,
        "model.layers.2.post_attention_layernorm": 14.333243370056152,
        "model.layers.2.pre_feedforward_layernorm": 23.054487228393555
      }
    },
    {
      "sequence": "TRANSLATE: BONJOUR MEANS HELLO IN FRENCH.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.64749340389086,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.8216638565063477,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 9.19089412689209,
        "model.layers.0.pre_feedforward_layernorm": 16.585491180419922,
        "model.layers.0.mlp.gate_proj": 21.98479461669922,
        "model.layers.0.mlp.up_proj": 9.988931655883789,
        "model.layers.0.mlp.down_proj": 1.8856441974639893,
        "model.layers.0.mlp": 1.8856441974639893,
        "model.layers.0.post_feedforward_layernorm": 8.175996780395508,
        "model.layers.0": 9.711416244506836,
        "model.layers.1.input_layernorm": 11.6354398727417,
        "model.layers.1.post_attention_layernorm": 15.11897087097168,
        "model.layers.1.pre_feedforward_layernorm": 27.566715240478516,
        "model.layers.1.mlp.gate_proj": 31.252378463745117,
        "model.layers.1.mlp.up_proj": 16.184326171875,
        "model.layers.1.mlp.down_proj": 2.136836290359497,
        "model.layers.1.mlp": 2.136836290359497,
        "model.layers.1.post_feedforward_layernorm": 13.818793296813965,
        "model.layers.1": 15.326339721679688,
        "model.layers.2.input_layernorm": 16.71228790283203,
        "model.layers.2.post_attention_layernorm": 11.630657196044922,
        "model.layers.2.pre_feedforward_layernorm": 22.142290115356445
      }
    },
    {
      "sequence": "The following is a test: What is version control?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.669478644495426,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.347805976867676,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.83229923248291,
        "model.layers.0.pre_feedforward_layernorm": 16.364328384399414,
        "model.layers.0.mlp.gate_proj": 16.94053077697754,
        "model.layers.0.mlp.up_proj": 8.24939250946045,
        "model.layers.0.mlp.down_proj": 1.7809354066848755,
        "model.layers.0.mlp": 1.7809354066848755,
        "model.layers.0.post_feedforward_layernorm": 10.523821830749512,
        "model.layers.0": 11.096693992614746,
        "model.layers.1.input_layernorm": 11.557531356811523,
        "model.layers.1.post_attention_layernorm": 14.015384674072266,
        "model.layers.1.pre_feedforward_layernorm": 27.16221809387207,
        "model.layers.1.mlp.gate_proj": 33.10064697265625,
        "model.layers.1.mlp.up_proj": 14.19139289855957,
        "model.layers.1.mlp.down_proj": 1.8494569063186646,
        "model.layers.1.mlp": 1.8494569063186646,
        "model.layers.1.post_feedforward_layernorm": 11.216448783874512,
        "model.layers.1": 18.237403869628906,
        "model.layers.2.input_layernorm": 18.269672393798828,
        "model.layers.2.post_attention_layernorm": 8.50993537902832,
        "model.layers.2.pre_feedforward_layernorm": 19.521717071533203
      }
    },
    {
      "sequence": "Explain  how  neural  networks  work.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.729756293089494,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.053613662719727,
        "model.layers.0.pre_feedforward_layernorm": 21.742891311645508,
        "model.layers.0.mlp.gate_proj": 25.621801376342773,
        "model.layers.0.mlp.up_proj": 11.640641212463379,
        "model.layers.0.mlp.down_proj": 2.262981414794922,
        "model.layers.0.mlp": 2.262981414794922,
        "model.layers.0.post_feedforward_layernorm": 9.97785472869873,
        "model.layers.0": 13.37949275970459,
        "model.layers.1.input_layernorm": 14.449857711791992,
        "model.layers.1.post_attention_layernorm": 11.38348388671875,
        "model.layers.1.pre_feedforward_layernorm": 23.34992027282715,
        "model.layers.1.mlp.gate_proj": 21.546855926513672,
        "model.layers.1.mlp.up_proj": 13.329085350036621,
        "model.layers.1.mlp.down_proj": 1.5944526195526123,
        "model.layers.1.mlp": 1.5944526195526123,
        "model.layers.1.post_feedforward_layernorm": 12.174711227416992,
        "model.layers.1": 16.527034759521484,
        "model.layers.2.input_layernorm": 17.494352340698242,
        "model.layers.2.post_attention_layernorm": 9.078133583068848,
        "model.layers.2.pre_feedforward_layernorm": 19.395998001098633
      }
    },
    {
      "sequence": "Please  explain  step  by  step.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.844681573950727,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.833176612854004,
        "model.layers.0.pre_feedforward_layernorm": 22.746444702148438,
        "model.layers.0.mlp.gate_proj": 24.977895736694336,
        "model.layers.0.mlp.up_proj": 11.83203411102295,
        "model.layers.0.mlp.down_proj": 2.557129144668579,
        "model.layers.0.mlp": 2.557129144668579,
        "model.layers.0.post_feedforward_layernorm": 11.880704879760742,
        "model.layers.0": 14.115238189697266,
        "model.layers.1.input_layernorm": 15.654885292053223,
        "model.layers.1.post_attention_layernorm": 10.35208797454834,
        "model.layers.1.pre_feedforward_layernorm": 23.17426109313965,
        "model.layers.1.mlp.gate_proj": 23.118425369262695,
        "model.layers.1.mlp.up_proj": 13.912103652954102,
        "model.layers.1.mlp.down_proj": 1.8535244464874268,
        "model.layers.1.mlp": 1.8535244464874268,
        "model.layers.1.post_feedforward_layernorm": 12.001224517822266,
        "model.layers.1": 16.29336929321289,
        "model.layers.2.input_layernorm": 16.934467315673828,
        "model.layers.2.post_attention_layernorm": 8.322422981262207,
        "model.layers.2.pre_feedforward_layernorm": 17.533828735351562
      }
    },
    {
      "sequence": "What  are  the  applications  of  deep  learning?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 11.858060857524043,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.294113159179688,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.460956573486328,
        "model.layers.0.pre_feedforward_layernorm": 21.370014190673828,
        "model.layers.0.mlp.gate_proj": 24.169363021850586,
        "model.layers.0.mlp.up_proj": 11.31317138671875,
        "model.layers.0.mlp.down_proj": 2.121082305908203,
        "model.layers.0.mlp": 2.121082305908203,
        "model.layers.0.post_feedforward_layernorm": 10.537967681884766,
        "model.layers.0": 14.139259338378906,
        "model.layers.1.input_layernorm": 15.52552604675293,
        "model.layers.1.post_attention_layernorm": 12.455984115600586,
        "model.layers.1.pre_feedforward_layernorm": 24.81305503845215,
        "model.layers.1.mlp.gate_proj": 22.6086368560791,
        "model.layers.1.mlp.up_proj": 14.083494186401367,
        "model.layers.1.mlp.down_proj": 1.842071294784546,
        "model.layers.1.mlp": 1.842071294784546,
        "model.layers.1.post_feedforward_layernorm": 11.236543655395508,
        "model.layers.1": 17.496828079223633,
        "model.layers.2.input_layernorm": 17.3087100982666,
        "model.layers.2.post_attention_layernorm": 8.726473808288574,
        "model.layers.2.pre_feedforward_layernorm": 17.26899528503418
      }
    },
    {
      "sequence": "Describe  the  water  cycle.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.10656099734099,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.885756492614746,
        "model.layers.0.pre_feedforward_layernorm": 22.82724380493164,
        "model.layers.0.mlp.gate_proj": 24.417421340942383,
        "model.layers.0.mlp.up_proj": 11.591028213500977,
        "model.layers.0.mlp.down_proj": 2.181739568710327,
        "model.layers.0.mlp": 2.181739568710327,
        "model.layers.0.post_feedforward_layernorm": 10.663494110107422,
        "model.layers.0": 12.75180721282959,
        "model.layers.1.input_layernorm": 14.236438751220703,
        "model.layers.1.post_attention_layernorm": 11.288849830627441,
        "model.layers.1.pre_feedforward_layernorm": 25.25050926208496,
        "model.layers.1.mlp.gate_proj": 26.537946701049805,
        "model.layers.1.mlp.up_proj": 14.466096878051758,
        "model.layers.1.mlp.down_proj": 1.8899788856506348,
        "model.layers.1.mlp": 1.8899788856506348,
        "model.layers.1.post_feedforward_layernorm": 13.207609176635742,
        "model.layers.1": 17.01510238647461,
        "model.layers.2.input_layernorm": 18.441890716552734,
        "model.layers.2.post_attention_layernorm": 7.758147239685059,
        "model.layers.2.pre_feedforward_layernorm": 19.423810958862305
      }
    },
    {
      "sequence": "How  does  exercise  affect  health?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.159324811852496,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 16.352115631103516,
        "model.layers.0.pre_feedforward_layernorm": 24.53653907775879,
        "model.layers.0.mlp.gate_proj": 27.274572372436523,
        "model.layers.0.mlp.up_proj": 12.625864028930664,
        "model.layers.0.mlp.down_proj": 2.3882968425750732,
        "model.layers.0.mlp": 2.3882968425750732,
        "model.layers.0.post_feedforward_layernorm": 12.441606521606445,
        "model.layers.0": 15.053984642028809,
        "model.layers.1.input_layernorm": 16.27127456665039,
        "model.layers.1.post_attention_layernorm": 11.727585792541504,
        "model.layers.1.pre_feedforward_layernorm": 24.294261932373047,
        "model.layers.1.mlp.gate_proj": 22.310348510742188,
        "model.layers.1.mlp.up_proj": 13.248262405395508,
        "model.layers.1.mlp.down_proj": 1.4480750560760498,
        "model.layers.1.mlp": 1.4480750560760498,
        "model.layers.1.post_feedforward_layernorm": 9.86482048034668,
        "model.layers.1": 16.931501388549805,
        "model.layers.2.input_layernorm": 16.581937789916992,
        "model.layers.2.post_attention_layernorm": 8.070457458496094,
        "model.layers.2.pre_feedforward_layernorm": 16.48279571533203
      }
    },
    {
      "sequence": "How  does  music  theory  work?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.311963599661123,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.91352653503418,
        "model.layers.0.pre_feedforward_layernorm": 24.85807228088379,
        "model.layers.0.mlp.gate_proj": 25.770320892333984,
        "model.layers.0.mlp.up_proj": 12.798437118530273,
        "model.layers.0.mlp.down_proj": 2.215043783187866,
        "model.layers.0.mlp": 2.215043783187866,
        "model.layers.0.post_feedforward_layernorm": 12.92927360534668,
        "model.layers.0": 14.184635162353516,
        "model.layers.1.input_layernorm": 15.45848274230957,
        "model.layers.1.post_attention_layernorm": 12.914603233337402,
        "model.layers.1.pre_feedforward_layernorm": 24.99956703186035,
        "model.layers.1.mlp.gate_proj": 22.91144561767578,
        "model.layers.1.mlp.up_proj": 14.004382133483887,
        "model.layers.1.mlp.down_proj": 1.463017463684082,
        "model.layers.1.mlp": 1.463017463684082,
        "model.layers.1.post_feedforward_layernorm": 10.782949447631836,
        "model.layers.1": 16.322956085205078,
        "model.layers.2.input_layernorm": 16.439002990722656,
        "model.layers.2.post_attention_layernorm": 10.031472206115723,
        "model.layers.2.pre_feedforward_layernorm": 17.576114654541016
      }
    },
    {
      "sequence": "HOW DOES MUSIC THEORY WORK?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.318999539250912,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 0.0,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 16.04489517211914,
        "model.layers.0.pre_feedforward_layernorm": 26.0339412689209,
        "model.layers.0.mlp.gate_proj": 29.74600601196289,
        "model.layers.0.mlp.up_proj": 15.426274299621582,
        "model.layers.0.mlp.down_proj": 2.2007131576538086,
        "model.layers.0.mlp": 2.2007131576538086,
        "model.layers.0.post_feedforward_layernorm": 11.096019744873047,
        "model.layers.0": 15.61788558959961,
        "model.layers.1.input_layernorm": 17.659317016601562,
        "model.layers.1.post_attention_layernorm": 12.23244571685791,
        "model.layers.1.pre_feedforward_layernorm": 23.928064346313477,
        "model.layers.1.mlp.gate_proj": 22.115768432617188,
        "model.layers.1.mlp.up_proj": 13.842273712158203,
        "model.layers.1.mlp.down_proj": 1.6078786849975586,
        "model.layers.1.mlp": 1.6078786849975586,
        "model.layers.1.post_feedforward_layernorm": 11.66630744934082,
        "model.layers.1": 16.27429962158203,
        "model.layers.2.input_layernorm": 15.855037689208984,
        "model.layers.2.post_attention_layernorm": 10.370455741882324,
        "model.layers.2.pre_feedforward_layernorm": 17.810813903808594
      }
    },
    {
      "sequence": "WHAT CAUSED WORLD WAR II?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.336316087971563,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.291982650756836,
        "model.layers.0.pre_feedforward_layernorm": 22.05905532836914,
        "model.layers.0.mlp.gate_proj": 24.875057220458984,
        "model.layers.0.mlp.up_proj": 11.730284690856934,
        "model.layers.0.mlp.down_proj": 1.8913426399230957,
        "model.layers.0.mlp": 1.8913426399230957,
        "model.layers.0.post_feedforward_layernorm": 9.177810668945312,
        "model.layers.0": 11.7551851272583,
        "model.layers.1.input_layernorm": 13.600517272949219,
        "model.layers.1.post_attention_layernorm": 15.766256332397461,
        "model.layers.1.pre_feedforward_layernorm": 29.097423553466797,
        "model.layers.1.mlp.gate_proj": 30.629459381103516,
        "model.layers.1.mlp.up_proj": 15.218436241149902,
        "model.layers.1.mlp.down_proj": 1.941782832145691,
        "model.layers.1.mlp": 1.941782832145691,
        "model.layers.1.post_feedforward_layernorm": 13.691530227661133,
        "model.layers.1": 15.781133651733398,
        "model.layers.2.input_layernorm": 16.097671508789062,
        "model.layers.2.post_attention_layernorm": 11.678906440734863,
        "model.layers.2.pre_feedforward_layernorm": 20.10007095336914
      }
    },
    {
      "sequence": "WHAT WAS THE INDUSTRIAL REVOLUTION?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.428006172180176,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 0.0,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.338715553283691,
        "model.layers.0.pre_feedforward_layernorm": 24.946529388427734,
        "model.layers.0.mlp.gate_proj": 28.779842376708984,
        "model.layers.0.mlp.up_proj": 15.195490837097168,
        "model.layers.0.mlp.down_proj": 2.112043857574463,
        "model.layers.0.mlp": 2.112043857574463,
        "model.layers.0.post_feedforward_layernorm": 10.547661781311035,
        "model.layers.0": 14.279679298400879,
        "model.layers.1.input_layernorm": 16.09515380859375,
        "model.layers.1.post_attention_layernorm": 14.058470726013184,
        "model.layers.1.pre_feedforward_layernorm": 24.247173309326172,
        "model.layers.1.mlp.gate_proj": 25.633670806884766,
        "model.layers.1.mlp.up_proj": 13.814714431762695,
        "model.layers.1.mlp.down_proj": 1.781947135925293,
        "model.layers.1.mlp": 1.781947135925293,
        "model.layers.1.post_feedforward_layernorm": 14.037297248840332,
        "model.layers.1": 16.348419189453125,
        "model.layers.2.input_layernorm": 16.01865005493164,
        "model.layers.2.post_attention_layernorm": 10.49808120727539,
        "model.layers.2.pre_feedforward_layernorm": 18.216609954833984
      }
    },
    {
      "sequence": "What  is  version  control?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.458788674810659,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.006135940551758,
        "model.layers.0.pre_feedforward_layernorm": 22.627277374267578,
        "model.layers.0.mlp.gate_proj": 23.13543128967285,
        "model.layers.0.mlp.up_proj": 11.413384437561035,
        "model.layers.0.mlp.down_proj": 2.2053558826446533,
        "model.layers.0.mlp": 2.2053558826446533,
        "model.layers.0.post_feedforward_layernorm": 11.654842376708984,
        "model.layers.0": 14.563462257385254,
        "model.layers.1.input_layernorm": 15.924907684326172,
        "model.layers.1.post_attention_layernorm": 13.59289264678955,
        "model.layers.1.pre_feedforward_layernorm": 25.290241241455078,
        "model.layers.1.mlp.gate_proj": 26.4252986907959,
        "model.layers.1.mlp.up_proj": 14.895941734313965,
        "model.layers.1.mlp.down_proj": 1.9932276010513306,
        "model.layers.1.mlp": 1.9932276010513306,
        "model.layers.1.post_feedforward_layernorm": 10.580182075500488,
        "model.layers.1": 17.170621871948242,
        "model.layers.2.input_layernorm": 17.678722381591797,
        "model.layers.2.post_attention_layernorm": 11.975093841552734,
        "model.layers.2.pre_feedforward_layernorm": 19.676223754882812
      }
    },
    {
      "sequence": "Compare  and  contrast  these  concepts.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.487994401351266,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.61788272857666,
        "model.layers.0.pre_feedforward_layernorm": 21.358219146728516,
        "model.layers.0.mlp.gate_proj": 23.812944412231445,
        "model.layers.0.mlp.up_proj": 10.629947662353516,
        "model.layers.0.mlp.down_proj": 2.2567033767700195,
        "model.layers.0.mlp": 2.2567033767700195,
        "model.layers.0.post_feedforward_layernorm": 10.465827941894531,
        "model.layers.0": 12.80768871307373,
        "model.layers.1.input_layernorm": 14.038640022277832,
        "model.layers.1.post_attention_layernorm": 14.207549095153809,
        "model.layers.1.pre_feedforward_layernorm": 25.97753143310547,
        "model.layers.1.mlp.gate_proj": 27.190114974975586,
        "model.layers.1.mlp.up_proj": 15.314743041992188,
        "model.layers.1.mlp.down_proj": 2.3933892250061035,
        "model.layers.1.mlp": 2.3933892250061035,
        "model.layers.1.post_feedforward_layernorm": 15.535316467285156,
        "model.layers.1": 17.847583770751953,
        "model.layers.2.input_layernorm": 18.73199462890625,
        "model.layers.2.post_attention_layernorm": 8.628273010253906,
        "model.layers.2.pre_feedforward_layernorm": 20.835630416870117
      }
    },
    {
      "sequence": "How  do  vaccines  work?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.505086639653081,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 14.497891426086426,
        "model.layers.0.pre_feedforward_layernorm": 24.21630096435547,
        "model.layers.0.mlp.gate_proj": 26.198883056640625,
        "model.layers.0.mlp.up_proj": 12.250871658325195,
        "model.layers.0.mlp.down_proj": 2.1041369438171387,
        "model.layers.0.mlp": 2.1041369438171387,
        "model.layers.0.post_feedforward_layernorm": 11.046306610107422,
        "model.layers.0": 13.950257301330566,
        "model.layers.1.input_layernorm": 15.1076021194458,
        "model.layers.1.post_attention_layernorm": 14.582098960876465,
        "model.layers.1.pre_feedforward_layernorm": 25.99094581604004,
        "model.layers.1.mlp.gate_proj": 27.667348861694336,
        "model.layers.1.mlp.up_proj": 14.425861358642578,
        "model.layers.1.mlp.down_proj": 1.773962140083313,
        "model.layers.1.mlp": 1.773962140083313,
        "model.layers.1.post_feedforward_layernorm": 12.614325523376465,
        "model.layers.1": 16.580059051513672,
        "model.layers.2.input_layernorm": 16.728424072265625,
        "model.layers.2.post_attention_layernorm": 8.847773551940918,
        "model.layers.2.pre_feedforward_layernorm": 17.525192260742188
      }
    },
    {
      "sequence": "Describe  time  management  methods.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.5132068136464,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.29603385925293,
        "model.layers.0.pre_feedforward_layernorm": 23.638296127319336,
        "model.layers.0.mlp.gate_proj": 25.545381546020508,
        "model.layers.0.mlp.up_proj": 11.818859100341797,
        "model.layers.0.mlp.down_proj": 2.4067466259002686,
        "model.layers.0.mlp": 2.4067466259002686,
        "model.layers.0.post_feedforward_layernorm": 11.162693977355957,
        "model.layers.0": 14.24319839477539,
        "model.layers.1.input_layernorm": 15.11124324798584,
        "model.layers.1.post_attention_layernorm": 13.675113677978516,
        "model.layers.1.pre_feedforward_layernorm": 26.96100425720215,
        "model.layers.1.mlp.gate_proj": 26.53118896484375,
        "model.layers.1.mlp.up_proj": 15.092573165893555,
        "model.layers.1.mlp.down_proj": 1.4464330673217773,
        "model.layers.1.mlp": 1.4464330673217773,
        "model.layers.1.post_feedforward_layernorm": 10.234031677246094,
        "model.layers.1": 18.242834091186523,
        "model.layers.2.input_layernorm": 19.19118881225586,
        "model.layers.2.post_attention_layernorm": 7.304593086242676,
        "model.layers.2.pre_feedforward_layernorm": 19.504850387573242
      }
    },
    {
      "sequence": "How  do  transformers  process  language?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.656079582546068,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 14.66584300994873,
        "model.layers.0.pre_feedforward_layernorm": 25.377540588378906,
        "model.layers.0.mlp.gate_proj": 27.089990615844727,
        "model.layers.0.mlp.up_proj": 12.543505668640137,
        "model.layers.0.mlp.down_proj": 2.491072416305542,
        "model.layers.0.mlp": 2.491072416305542,
        "model.layers.0.post_feedforward_layernorm": 12.563142776489258,
        "model.layers.0": 15.335432052612305,
        "model.layers.1.input_layernorm": 17.32564353942871,
        "model.layers.1.post_attention_layernorm": 12.697378158569336,
        "model.layers.1.pre_feedforward_layernorm": 24.787126541137695,
        "model.layers.1.mlp.gate_proj": 26.122333526611328,
        "model.layers.1.mlp.up_proj": 14.577316284179688,
        "model.layers.1.mlp.down_proj": 1.5529711246490479,
        "model.layers.1.mlp": 1.5529711246490479,
        "model.layers.1.post_feedforward_layernorm": 10.454645156860352,
        "model.layers.1": 17.3074951171875,
        "model.layers.2.input_layernorm": 16.91839027404785,
        "model.layers.2.post_attention_layernorm": 9.591994285583496,
        "model.layers.2.pre_feedforward_layernorm": 17.72016716003418
      }
    },
    {
      "sequence": "PLEASE EXPLAIN STEP BY STEP.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.671821480212005,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.192035675048828,
        "model.layers.0.pre_feedforward_layernorm": 23.261720657348633,
        "model.layers.0.mlp.gate_proj": 27.097267150878906,
        "model.layers.0.mlp.up_proj": 13.800691604614258,
        "model.layers.0.mlp.down_proj": 2.897592067718506,
        "model.layers.0.mlp": 2.897592067718506,
        "model.layers.0.post_feedforward_layernorm": 11.798565864562988,
        "model.layers.0": 15.369016647338867,
        "model.layers.1.input_layernorm": 17.962818145751953,
        "model.layers.1.post_attention_layernorm": 12.07915210723877,
        "model.layers.1.pre_feedforward_layernorm": 25.07645606994629,
        "model.layers.1.mlp.gate_proj": 25.83319091796875,
        "model.layers.1.mlp.up_proj": 15.291403770446777,
        "model.layers.1.mlp.down_proj": 2.035198926925659,
        "model.layers.1.mlp": 2.035198926925659,
        "model.layers.1.post_feedforward_layernorm": 11.768677711486816,
        "model.layers.1": 16.711811065673828,
        "model.layers.2.input_layernorm": 17.555194854736328,
        "model.layers.2.post_attention_layernorm": 11.144219398498535,
        "model.layers.2.pre_feedforward_layernorm": 21.125852584838867
      }
    },
    {
      "sequence": "Describe  the  process  of  machine  learning.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.731522746708082,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.206338882446289,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.518523216247559,
        "model.layers.0.pre_feedforward_layernorm": 22.169282913208008,
        "model.layers.0.mlp.gate_proj": 23.974754333496094,
        "model.layers.0.mlp.up_proj": 11.344491958618164,
        "model.layers.0.mlp.down_proj": 2.453472375869751,
        "model.layers.0.mlp": 2.453472375869751,
        "model.layers.0.post_feedforward_layernorm": 11.270197868347168,
        "model.layers.0": 14.03415298461914,
        "model.layers.1.input_layernorm": 15.655251502990723,
        "model.layers.1.post_attention_layernorm": 12.900261878967285,
        "model.layers.1.pre_feedforward_layernorm": 26.544654846191406,
        "model.layers.1.mlp.gate_proj": 27.53776741027832,
        "model.layers.1.mlp.up_proj": 15.706280708312988,
        "model.layers.1.mlp.down_proj": 2.0725746154785156,
        "model.layers.1.mlp": 2.0725746154785156,
        "model.layers.1.post_feedforward_layernorm": 14.249924659729004,
        "model.layers.1": 18.01225471496582,
        "model.layers.2.input_layernorm": 19.33375358581543,
        "model.layers.2.post_attention_layernorm": 8.836180686950684,
        "model.layers.2.pre_feedforward_layernorm": 20.478857040405273
      }
    },
    {
      "sequence": "What  caused  World  War  II?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.904352706411611,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.387335777282715,
        "model.layers.0.pre_feedforward_layernorm": 23.17824935913086,
        "model.layers.0.mlp.gate_proj": 24.841880798339844,
        "model.layers.0.mlp.up_proj": 11.245716094970703,
        "model.layers.0.mlp.down_proj": 1.8300042152404785,
        "model.layers.0.mlp": 1.8300042152404785,
        "model.layers.0.post_feedforward_layernorm": 9.387970924377441,
        "model.layers.0": 13.412928581237793,
        "model.layers.1.input_layernorm": 14.957100868225098,
        "model.layers.1.post_attention_layernorm": 15.281997680664062,
        "model.layers.1.pre_feedforward_layernorm": 28.786880493164062,
        "model.layers.1.mlp.gate_proj": 31.12281036376953,
        "model.layers.1.mlp.up_proj": 16.456295013427734,
        "model.layers.1.mlp.down_proj": 2.0728185176849365,
        "model.layers.1.mlp": 2.0728185176849365,
        "model.layers.1.post_feedforward_layernorm": 12.866543769836426,
        "model.layers.1": 18.212186813354492,
        "model.layers.2.input_layernorm": 19.21591567993164,
        "model.layers.2.post_attention_layernorm": 9.230533599853516,
        "model.layers.2.pre_feedforward_layernorm": 19.4863224029541
      }
    },
    {
      "sequence": "What  are  the  pros  and  cons?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 12.966523439987846,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.206338882446289,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 14.130868911743164,
        "model.layers.0.pre_feedforward_layernorm": 22.163114547729492,
        "model.layers.0.mlp.gate_proj": 25.20608139038086,
        "model.layers.0.mlp.up_proj": 11.451542854309082,
        "model.layers.0.mlp.down_proj": 2.0402204990386963,
        "model.layers.0.mlp": 2.0402204990386963,
        "model.layers.0.post_feedforward_layernorm": 10.391033172607422,
        "model.layers.0": 13.722671508789062,
        "model.layers.1.input_layernorm": 15.515616416931152,
        "model.layers.1.post_attention_layernorm": 15.146411895751953,
        "model.layers.1.pre_feedforward_layernorm": 28.007543563842773,
        "model.layers.1.mlp.gate_proj": 28.38483238220215,
        "model.layers.1.mlp.up_proj": 15.883174896240234,
        "model.layers.1.mlp.down_proj": 2.242288589477539,
        "model.layers.1.mlp": 2.242288589477539,
        "model.layers.1.post_feedforward_layernorm": 13.990338325500488,
        "model.layers.1": 18.98309326171875,
        "model.layers.2.input_layernorm": 19.04825782775879,
        "model.layers.2.post_attention_layernorm": 9.84962272644043,
        "model.layers.2.pre_feedforward_layernorm": 19.5844783782959
      }
    },
    {
      "sequence": "I NEED DETAILED INFORMATION ABOUT THIS TOPIC.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.166910337365191,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.8216638565063477,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 14.408287048339844,
        "model.layers.0.pre_feedforward_layernorm": 23.04916000366211,
        "model.layers.0.mlp.gate_proj": 26.94832992553711,
        "model.layers.0.mlp.up_proj": 13.89602279663086,
        "model.layers.0.mlp.down_proj": 3.0607800483703613,
        "model.layers.0.mlp": 3.0607800483703613,
        "model.layers.0.post_feedforward_layernorm": 14.114882469177246,
        "model.layers.0": 16.631946563720703,
        "model.layers.1.input_layernorm": 18.974586486816406,
        "model.layers.1.post_attention_layernorm": 13.010984420776367,
        "model.layers.1.pre_feedforward_layernorm": 25.44455337524414,
        "model.layers.1.mlp.gate_proj": 26.0590763092041,
        "model.layers.1.mlp.up_proj": 16.21122932434082,
        "model.layers.1.mlp.down_proj": 2.2023043632507324,
        "model.layers.1.mlp": 2.2023043632507324,
        "model.layers.1.post_feedforward_layernorm": 15.449872970581055,
        "model.layers.1": 17.23479652404785,
        "model.layers.2.input_layernorm": 18.01059341430664,
        "model.layers.2.post_attention_layernorm": 10.30630111694336,
        "model.layers.2.pre_feedforward_layernorm": 19.740482330322266
      }
    },
    {
      "sequence": "WHAT CAUSES CLIMATE CHANGE?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.175289755282195,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.521875381469727,
        "model.layers.0.pre_feedforward_layernorm": 27.077884674072266,
        "model.layers.0.mlp.gate_proj": 31.870765686035156,
        "model.layers.0.mlp.up_proj": 15.651270866394043,
        "model.layers.0.mlp.down_proj": 2.550598382949829,
        "model.layers.0.mlp": 2.550598382949829,
        "model.layers.0.post_feedforward_layernorm": 11.828977584838867,
        "model.layers.0": 16.166860580444336,
        "model.layers.1.input_layernorm": 18.525365829467773,
        "model.layers.1.post_attention_layernorm": 12.241889953613281,
        "model.layers.1.pre_feedforward_layernorm": 24.978872299194336,
        "model.layers.1.mlp.gate_proj": 25.50918197631836,
        "model.layers.1.mlp.up_proj": 14.731180191040039,
        "model.layers.1.mlp.down_proj": 1.5984286069869995,
        "model.layers.1.mlp": 1.5984286069869995,
        "model.layers.1.post_feedforward_layernorm": 12.643328666687012,
        "model.layers.1": 17.62586784362793,
        "model.layers.2.input_layernorm": 17.154033660888672,
        "model.layers.2.post_attention_layernorm": 11.1885347366333,
        "model.layers.2.pre_feedforward_layernorm": 19.124004364013672
      }
    },
    {
      "sequence": "I need detailed information about this topic. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.1800624391307,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.646179676055908,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 18.333961486816406,
        "model.layers.0.pre_feedforward_layernorm": 23.404197692871094,
        "model.layers.0.mlp.gate_proj": 27.539329528808594,
        "model.layers.0.mlp.up_proj": 12.754403114318848,
        "model.layers.0.mlp.down_proj": 2.4962146282196045,
        "model.layers.0.mlp": 2.4962146282196045,
        "model.layers.0.post_feedforward_layernorm": 10.04491901397705,
        "model.layers.0": 15.922703742980957,
        "model.layers.1.input_layernorm": 15.766904830932617,
        "model.layers.1.post_attention_layernorm": 10.801872253417969,
        "model.layers.1.pre_feedforward_layernorm": 25.568418502807617,
        "model.layers.1.mlp.gate_proj": 24.34132957458496,
        "model.layers.1.mlp.up_proj": 15.422633171081543,
        "model.layers.1.mlp.down_proj": 2.4088690280914307,
        "model.layers.1.mlp": 2.4088690280914307,
        "model.layers.1.post_feedforward_layernorm": 9.988973617553711,
        "model.layers.1": 18.633174896240234,
        "model.layers.2.input_layernorm": 18.303361892700195,
        "model.layers.2.post_attention_layernorm": 13.404579162597656,
        "model.layers.2.pre_feedforward_layernorm": 25.454326629638672
      }
    },
    {
      "sequence": "What  is  the  meaning  of  life?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.243553659190303,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.206338882446289,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.324055671691895,
        "model.layers.0.pre_feedforward_layernorm": 26.59128189086914,
        "model.layers.0.mlp.gate_proj": 28.852794647216797,
        "model.layers.0.mlp.up_proj": 12.92190170288086,
        "model.layers.0.mlp.down_proj": 2.294710874557495,
        "model.layers.0.mlp": 2.294710874557495,
        "model.layers.0.post_feedforward_layernorm": 10.541919708251953,
        "model.layers.0": 14.636974334716797,
        "model.layers.1.input_layernorm": 16.44267463684082,
        "model.layers.1.post_attention_layernorm": 14.559678077697754,
        "model.layers.1.pre_feedforward_layernorm": 26.9781494140625,
        "model.layers.1.mlp.gate_proj": 26.7325496673584,
        "model.layers.1.mlp.up_proj": 15.582948684692383,
        "model.layers.1.mlp.down_proj": 2.0764482021331787,
        "model.layers.1.mlp": 2.0764482021331787,
        "model.layers.1.post_feedforward_layernorm": 12.244026184082031,
        "model.layers.1": 17.69481658935547,
        "model.layers.2.input_layernorm": 18.683835983276367,
        "model.layers.2.post_attention_layernorm": 10.298739433288574,
        "model.layers.2.pre_feedforward_layernorm": 19.566730499267578
      }
    },
    {
      "sequence": "EXPLAIN THE ENTIRE PROCESS OF HOW A COMPUTER PROCESSES A PROGRAM FROM SOURCE CODE TO EXECUTION, INCLUDING COMPILATION, MEMORY MANAGEMENT, AND CPU OPERATIONS.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.288207012674082,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.807018756866455,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 11.280714988708496,
        "model.layers.0.pre_feedforward_layernorm": 19.70088768005371,
        "model.layers.0.mlp.gate_proj": 23.749347686767578,
        "model.layers.0.mlp.up_proj": 11.445178031921387,
        "model.layers.0.mlp.down_proj": 2.3175742626190186,
        "model.layers.0.mlp": 2.3175742626190186,
        "model.layers.0.post_feedforward_layernorm": 9.335407257080078,
        "model.layers.0": 11.316473007202148,
        "model.layers.1.input_layernorm": 13.85650634765625,
        "model.layers.1.post_attention_layernorm": 15.548442840576172,
        "model.layers.1.pre_feedforward_layernorm": 28.465190887451172,
        "model.layers.1.mlp.gate_proj": 31.489017486572266,
        "model.layers.1.mlp.up_proj": 17.285545349121094,
        "model.layers.1.mlp.down_proj": 2.4840335845947266,
        "model.layers.1.mlp": 2.4840335845947266,
        "model.layers.1.post_feedforward_layernorm": 19.46916961669922,
        "model.layers.1": 17.76700210571289,
        "model.layers.2.input_layernorm": 17.84971046447754,
        "model.layers.2.post_attention_layernorm": 13.393404006958008,
        "model.layers.2.pre_feedforward_layernorm": 26.266529083251953
      }
    },
    {
      "sequence": "DESCRIBE TIME MANAGEMENT METHODS.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.299052435418833,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.970218658447266,
        "model.layers.0.pre_feedforward_layernorm": 21.721569061279297,
        "model.layers.0.mlp.gate_proj": 27.637136459350586,
        "model.layers.0.mlp.up_proj": 12.516881942749023,
        "model.layers.0.mlp.down_proj": 2.5041918754577637,
        "model.layers.0.mlp": 2.5041918754577637,
        "model.layers.0.post_feedforward_layernorm": 12.71689224243164,
        "model.layers.0": 14.243513107299805,
        "model.layers.1.input_layernorm": 15.377083778381348,
        "model.layers.1.post_attention_layernorm": 15.442232131958008,
        "model.layers.1.pre_feedforward_layernorm": 27.50428009033203,
        "model.layers.1.mlp.gate_proj": 28.510257720947266,
        "model.layers.1.mlp.up_proj": 15.495206832885742,
        "model.layers.1.mlp.down_proj": 2.0330488681793213,
        "model.layers.1.mlp": 2.0330488681793213,
        "model.layers.1.post_feedforward_layernorm": 15.898561477661133,
        "model.layers.1": 17.455373764038086,
        "model.layers.2.input_layernorm": 18.134475708007812,
        "model.layers.2.post_attention_layernorm": 14.128337860107422,
        "model.layers.2.pre_feedforward_layernorm": 24.157987594604492
      }
    },
    {
      "sequence": "HOW DOES EXERCISE AFFECT HEALTH?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.308894540952599,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 0.0,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 17.69123077392578,
        "model.layers.0.pre_feedforward_layernorm": 27.715299606323242,
        "model.layers.0.mlp.gate_proj": 35.31113052368164,
        "model.layers.0.mlp.up_proj": 16.513456344604492,
        "model.layers.0.mlp.down_proj": 2.485809326171875,
        "model.layers.0.mlp": 2.485809326171875,
        "model.layers.0.post_feedforward_layernorm": 12.585750579833984,
        "model.layers.0": 17.01125144958496,
        "model.layers.1.input_layernorm": 18.758663177490234,
        "model.layers.1.post_attention_layernorm": 12.393104553222656,
        "model.layers.1.pre_feedforward_layernorm": 24.489547729492188,
        "model.layers.1.mlp.gate_proj": 22.97126007080078,
        "model.layers.1.mlp.up_proj": 14.424322128295898,
        "model.layers.1.mlp.down_proj": 1.757245659828186,
        "model.layers.1.mlp": 1.757245659828186,
        "model.layers.1.post_feedforward_layernorm": 11.996183395385742,
        "model.layers.1": 17.990299224853516,
        "model.layers.2.input_layernorm": 17.306964874267578,
        "model.layers.2.post_attention_layernorm": 11.691386222839355,
        "model.layers.2.pre_feedforward_layernorm": 18.768613815307617
      }
    },
    {
      "sequence": "WRITE PYTHON CODE TO SORT A LIST.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.321914579557335,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 0.0,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.380098342895508,
        "model.layers.0.pre_feedforward_layernorm": 22.323955535888672,
        "model.layers.0.mlp.gate_proj": 27.099966049194336,
        "model.layers.0.mlp.up_proj": 13.982329368591309,
        "model.layers.0.mlp.down_proj": 3.0332753658294678,
        "model.layers.0.mlp": 3.0332753658294678,
        "model.layers.0.post_feedforward_layernorm": 13.020464897155762,
        "model.layers.0": 15.72866439819336,
        "model.layers.1.input_layernorm": 18.97443962097168,
        "model.layers.1.post_attention_layernorm": 13.020454406738281,
        "model.layers.1.pre_feedforward_layernorm": 26.626157760620117,
        "model.layers.1.mlp.gate_proj": 29.145383834838867,
        "model.layers.1.mlp.up_proj": 16.999635696411133,
        "model.layers.1.mlp.down_proj": 1.9772640466690063,
        "model.layers.1.mlp": 1.9772640466690063,
        "model.layers.1.post_feedforward_layernorm": 13.542839050292969,
        "model.layers.1": 19.101301193237305,
        "model.layers.2.input_layernorm": 19.565793991088867,
        "model.layers.2.post_attention_layernorm": 11.470370292663574,
        "model.layers.2.pre_feedforward_layernorm": 22.40110206604004
      }
    },
    {
      "sequence": "Please explain step by step. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.331691949263863,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 18.504287719726562,
        "model.layers.0.pre_feedforward_layernorm": 23.457395553588867,
        "model.layers.0.mlp.gate_proj": 28.398527145385742,
        "model.layers.0.mlp.up_proj": 11.766732215881348,
        "model.layers.0.mlp.down_proj": 2.383500814437866,
        "model.layers.0.mlp": 2.383500814437866,
        "model.layers.0.post_feedforward_layernorm": 8.731316566467285,
        "model.layers.0": 16.728721618652344,
        "model.layers.1.input_layernorm": 15.001811027526855,
        "model.layers.1.post_attention_layernorm": 11.820533752441406,
        "model.layers.1.pre_feedforward_layernorm": 25.674097061157227,
        "model.layers.1.mlp.gate_proj": 27.973485946655273,
        "model.layers.1.mlp.up_proj": 15.065160751342773,
        "model.layers.1.mlp.down_proj": 2.7381975650787354,
        "model.layers.1.mlp": 2.7381975650787354,
        "model.layers.1.post_feedforward_layernorm": 9.930475234985352,
        "model.layers.1": 19.755395889282227,
        "model.layers.2.input_layernorm": 18.40483856201172,
        "model.layers.2.post_attention_layernorm": 12.678275108337402,
        "model.layers.2.pre_feedforward_layernorm": 24.57066535949707
      }
    },
    {
      "sequence": "Translate: Bonjour means hello in French. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.360290485879649,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.646179676055908,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 17.354543685913086,
        "model.layers.0.pre_feedforward_layernorm": 25.26679801940918,
        "model.layers.0.mlp.gate_proj": 29.23342514038086,
        "model.layers.0.mlp.up_proj": 13.616429328918457,
        "model.layers.0.mlp.down_proj": 2.266960620880127,
        "model.layers.0.mlp": 2.266960620880127,
        "model.layers.0.post_feedforward_layernorm": 8.1666898727417,
        "model.layers.0": 15.189725875854492,
        "model.layers.1.input_layernorm": 15.557584762573242,
        "model.layers.1.post_attention_layernorm": 11.212067604064941,
        "model.layers.1.pre_feedforward_layernorm": 26.45342254638672,
        "model.layers.1.mlp.gate_proj": 28.029823303222656,
        "model.layers.1.mlp.up_proj": 15.209558486938477,
        "model.layers.1.mlp.down_proj": 2.8933823108673096,
        "model.layers.1.mlp": 2.8933823108673096,
        "model.layers.1.post_feedforward_layernorm": 10.678999900817871,
        "model.layers.1": 17.42767906188965,
        "model.layers.2.input_layernorm": 18.106834411621094,
        "model.layers.2.post_attention_layernorm": 12.729422569274902,
        "model.layers.2.pre_feedforward_layernorm": 25.086811065673828
      }
    },
    {
      "sequence": "HOW DID THE INTERNET DEVELOP?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.38754069286844,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 0.0,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 18.37798500061035,
        "model.layers.0.pre_feedforward_layernorm": 28.62975311279297,
        "model.layers.0.mlp.gate_proj": 34.89093780517578,
        "model.layers.0.mlp.up_proj": 17.382965087890625,
        "model.layers.0.mlp.down_proj": 2.370011329650879,
        "model.layers.0.mlp": 2.370011329650879,
        "model.layers.0.post_feedforward_layernorm": 12.775236129760742,
        "model.layers.0": 16.976022720336914,
        "model.layers.1.input_layernorm": 19.109046936035156,
        "model.layers.1.post_attention_layernorm": 12.440025329589844,
        "model.layers.1.pre_feedforward_layernorm": 24.106952667236328,
        "model.layers.1.mlp.gate_proj": 21.07327651977539,
        "model.layers.1.mlp.up_proj": 14.927339553833008,
        "model.layers.1.mlp.down_proj": 1.8970742225646973,
        "model.layers.1.mlp": 1.8970742225646973,
        "model.layers.1.post_feedforward_layernorm": 12.901983261108398,
        "model.layers.1": 17.917509078979492,
        "model.layers.2.input_layernorm": 17.63100814819336,
        "model.layers.2.post_attention_layernorm": 11.586114883422852,
        "model.layers.2.pre_feedforward_layernorm": 18.653108596801758
      }
    },
    {
      "sequence": "EXPLAIN DATABASE NORMALIZATION.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.533745340679003,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.878293991088867,
        "model.layers.0.pre_feedforward_layernorm": 21.462100982666016,
        "model.layers.0.mlp.gate_proj": 28.157678604125977,
        "model.layers.0.mlp.up_proj": 13.215987205505371,
        "model.layers.0.mlp.down_proj": 2.6244008541107178,
        "model.layers.0.mlp": 2.6244008541107178,
        "model.layers.0.post_feedforward_layernorm": 10.727319717407227,
        "model.layers.0": 14.598977088928223,
        "model.layers.1.input_layernorm": 16.370695114135742,
        "model.layers.1.post_attention_layernorm": 14.371113777160645,
        "model.layers.1.pre_feedforward_layernorm": 27.333080291748047,
        "model.layers.1.mlp.gate_proj": 28.312795639038086,
        "model.layers.1.mlp.up_proj": 15.976893424987793,
        "model.layers.1.mlp.down_proj": 1.830295443534851,
        "model.layers.1.mlp": 1.830295443534851,
        "model.layers.1.post_feedforward_layernorm": 13.038546562194824,
        "model.layers.1": 18.096820831298828,
        "model.layers.2.input_layernorm": 19.42129898071289,
        "model.layers.2.post_attention_layernorm": 15.928313255310059,
        "model.layers.2.pre_feedforward_layernorm": 26.096920013427734
      }
    },
    {
      "sequence": "How  do  we  define  truth?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.570213483727496,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 16.376209259033203,
        "model.layers.0.pre_feedforward_layernorm": 25.89912986755371,
        "model.layers.0.mlp.gate_proj": 27.838783264160156,
        "model.layers.0.mlp.up_proj": 12.972660064697266,
        "model.layers.0.mlp.down_proj": 2.47627592086792,
        "model.layers.0.mlp": 2.47627592086792,
        "model.layers.0.post_feedforward_layernorm": 12.42669677734375,
        "model.layers.0": 15.550193786621094,
        "model.layers.1.input_layernorm": 17.501441955566406,
        "model.layers.1.post_attention_layernorm": 14.990676879882812,
        "model.layers.1.pre_feedforward_layernorm": 27.584802627563477,
        "model.layers.1.mlp.gate_proj": 30.157615661621094,
        "model.layers.1.mlp.up_proj": 15.374933242797852,
        "model.layers.1.mlp.down_proj": 2.0334367752075195,
        "model.layers.1.mlp": 2.0334367752075195,
        "model.layers.1.post_feedforward_layernorm": 13.714468002319336,
        "model.layers.1": 18.994314193725586,
        "model.layers.2.input_layernorm": 18.750686645507812,
        "model.layers.2.post_attention_layernorm": 8.422542572021484,
        "model.layers.2.pre_feedforward_layernorm": 18.616531372070312
      }
    },
    {
      "sequence": "WHAT ARE INVESTMENT STRATEGIES?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.630366854045702,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 18.093305587768555,
        "model.layers.0.pre_feedforward_layernorm": 29.80004119873047,
        "model.layers.0.mlp.gate_proj": 34.8587760925293,
        "model.layers.0.mlp.up_proj": 17.436336517333984,
        "model.layers.0.mlp.down_proj": 2.5912067890167236,
        "model.layers.0.mlp": 2.5912067890167236,
        "model.layers.0.post_feedforward_layernorm": 12.202413558959961,
        "model.layers.0": 18.195602416992188,
        "model.layers.1.input_layernorm": 19.81089973449707,
        "model.layers.1.post_attention_layernorm": 10.40101146697998,
        "model.layers.1.pre_feedforward_layernorm": 24.328147888183594,
        "model.layers.1.mlp.gate_proj": 23.308992385864258,
        "model.layers.1.mlp.up_proj": 14.657394409179688,
        "model.layers.1.mlp.down_proj": 1.6621017456054688,
        "model.layers.1.mlp": 1.6621017456054688,
        "model.layers.1.post_feedforward_layernorm": 10.912424087524414,
        "model.layers.1": 18.614669799804688,
        "model.layers.2.input_layernorm": 18.05202865600586,
        "model.layers.2.post_attention_layernorm": 11.98801326751709,
        "model.layers.2.pre_feedforward_layernorm": 19.438047409057617
      }
    },
    {
      "sequence": "How  do  I  change  a  tire?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.661634341530178,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.206338882446289,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 14.613890647888184,
        "model.layers.0.pre_feedforward_layernorm": 23.63337516784668,
        "model.layers.0.mlp.gate_proj": 26.37037467956543,
        "model.layers.0.mlp.up_proj": 11.790132522583008,
        "model.layers.0.mlp.down_proj": 2.3680613040924072,
        "model.layers.0.mlp": 2.3680613040924072,
        "model.layers.0.post_feedforward_layernorm": 11.03715705871582,
        "model.layers.0": 14.58792781829834,
        "model.layers.1.input_layernorm": 15.942232131958008,
        "model.layers.1.post_attention_layernorm": 16.64278221130371,
        "model.layers.1.pre_feedforward_layernorm": 30.61962127685547,
        "model.layers.1.mlp.gate_proj": 33.11601257324219,
        "model.layers.1.mlp.up_proj": 16.24897003173828,
        "model.layers.1.mlp.down_proj": 2.0827999114990234,
        "model.layers.1.mlp": 2.0827999114990234,
        "model.layers.1.post_feedforward_layernorm": 14.085755348205566,
        "model.layers.1": 19.069847106933594,
        "model.layers.2.input_layernorm": 19.424272537231445,
        "model.layers.2.post_attention_layernorm": 9.679010391235352,
        "model.layers.2.pre_feedforward_layernorm": 20.248167037963867
      }
    },
    {
      "sequence": "Write  Python  code  to  sort  a  list.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.682219774826713,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.294113159179688,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.997361183166504,
        "model.layers.0.pre_feedforward_layernorm": 21.685243606567383,
        "model.layers.0.mlp.gate_proj": 23.653648376464844,
        "model.layers.0.mlp.up_proj": 11.705976486206055,
        "model.layers.0.mlp.down_proj": 2.6579456329345703,
        "model.layers.0.mlp": 2.6579456329345703,
        "model.layers.0.post_feedforward_layernorm": 11.401252746582031,
        "model.layers.0": 14.789429664611816,
        "model.layers.1.input_layernorm": 17.104463577270508,
        "model.layers.1.post_attention_layernorm": 13.499457359313965,
        "model.layers.1.pre_feedforward_layernorm": 25.063310623168945,
        "model.layers.1.mlp.gate_proj": 25.347564697265625,
        "model.layers.1.mlp.up_proj": 15.773531913757324,
        "model.layers.1.mlp.down_proj": 2.445314645767212,
        "model.layers.1.mlp": 2.445314645767212,
        "model.layers.1.post_feedforward_layernorm": 15.870316505432129,
        "model.layers.1": 23.183637619018555,
        "model.layers.2.input_layernorm": 23.816951751708984,
        "model.layers.2.post_attention_layernorm": 14.717432022094727,
        "model.layers.2.pre_feedforward_layernorm": 25.580842971801758
      }
    },
    {
      "sequence": "Can  you  help  me  understand  this?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.711770596711531,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.206338882446289,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 16.2949275970459,
        "model.layers.0.pre_feedforward_layernorm": 26.2255859375,
        "model.layers.0.mlp.gate_proj": 28.22612762451172,
        "model.layers.0.mlp.up_proj": 12.574053764343262,
        "model.layers.0.mlp.down_proj": 2.327183961868286,
        "model.layers.0.mlp": 2.327183961868286,
        "model.layers.0.post_feedforward_layernorm": 12.202523231506348,
        "model.layers.0": 16.15291404724121,
        "model.layers.1.input_layernorm": 17.697946548461914,
        "model.layers.1.post_attention_layernorm": 14.542805671691895,
        "model.layers.1.pre_feedforward_layernorm": 28.334823608398438,
        "model.layers.1.mlp.gate_proj": 27.01754379272461,
        "model.layers.1.mlp.up_proj": 15.98088550567627,
        "model.layers.1.mlp.down_proj": 2.1544320583343506,
        "model.layers.1.mlp": 2.1544320583343506,
        "model.layers.1.post_feedforward_layernorm": 13.183385848999023,
        "model.layers.1": 20.352008819580078,
        "model.layers.2.input_layernorm": 19.722965240478516,
        "model.layers.2.post_attention_layernorm": 10.026309967041016,
        "model.layers.2.pre_feedforward_layernorm": 19.666345596313477
      }
    },
    {
      "sequence": "EXPLAIN FILM EDITING PRINCIPLES.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.75656973797342,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.892584800720215,
        "model.layers.0.pre_feedforward_layernorm": 21.430448532104492,
        "model.layers.0.mlp.gate_proj": 28.348196029663086,
        "model.layers.0.mlp.up_proj": 12.82951545715332,
        "model.layers.0.mlp.down_proj": 2.5491724014282227,
        "model.layers.0.mlp": 2.5491724014282227,
        "model.layers.0.post_feedforward_layernorm": 11.054184913635254,
        "model.layers.0": 14.450725555419922,
        "model.layers.1.input_layernorm": 15.807392120361328,
        "model.layers.1.post_attention_layernorm": 15.495471000671387,
        "model.layers.1.pre_feedforward_layernorm": 28.332435607910156,
        "model.layers.1.mlp.gate_proj": 31.5813045501709,
        "model.layers.1.mlp.up_proj": 16.191566467285156,
        "model.layers.1.mlp.down_proj": 2.0717339515686035,
        "model.layers.1.mlp": 2.0717339515686035,
        "model.layers.1.post_feedforward_layernorm": 15.342004776000977,
        "model.layers.1": 17.455013275146484,
        "model.layers.2.input_layernorm": 18.832046508789062,
        "model.layers.2.post_attention_layernorm": 14.909741401672363,
        "model.layers.2.pre_feedforward_layernorm": 27.128520965576172
      }
    },
    {
      "sequence": "WHAT ARE THE APPLICATIONS OF DEEP LEARNING?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 13.929906388987666,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 0.0,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 17.60520362854004,
        "model.layers.0.pre_feedforward_layernorm": 29.34159278869629,
        "model.layers.0.mlp.gate_proj": 35.874141693115234,
        "model.layers.0.mlp.up_proj": 17.537521362304688,
        "model.layers.0.mlp.down_proj": 2.6481072902679443,
        "model.layers.0.mlp": 2.6481072902679443,
        "model.layers.0.post_feedforward_layernorm": 13.392664909362793,
        "model.layers.0": 17.89614486694336,
        "model.layers.1.input_layernorm": 20.05879020690918,
        "model.layers.1.post_attention_layernorm": 11.792383193969727,
        "model.layers.1.pre_feedforward_layernorm": 25.710702896118164,
        "model.layers.1.mlp.gate_proj": 23.0909423828125,
        "model.layers.1.mlp.up_proj": 15.899850845336914,
        "model.layers.1.mlp.down_proj": 2.2325313091278076,
        "model.layers.1.mlp": 2.2325313091278076,
        "model.layers.1.post_feedforward_layernorm": 11.845054626464844,
        "model.layers.1": 19.671152114868164,
        "model.layers.2.input_layernorm": 18.70728874206543,
        "model.layers.2.post_attention_layernorm": 12.192116737365723,
        "model.layers.2.pre_feedforward_layernorm": 20.011018753051758
      }
    },
    {
      "sequence": "Write Python code to sort a list. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.056020031804623,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.646179676055908,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 16.64502716064453,
        "model.layers.0.pre_feedforward_layernorm": 23.456295013427734,
        "model.layers.0.mlp.gate_proj": 27.899703979492188,
        "model.layers.0.mlp.up_proj": 12.873941421508789,
        "model.layers.0.mlp.down_proj": 2.197131395339966,
        "model.layers.0.mlp": 2.197131395339966,
        "model.layers.0.post_feedforward_layernorm": 8.467370986938477,
        "model.layers.0": 14.843832969665527,
        "model.layers.1.input_layernorm": 15.178021430969238,
        "model.layers.1.post_attention_layernorm": 13.432024955749512,
        "model.layers.1.pre_feedforward_layernorm": 27.808116912841797,
        "model.layers.1.mlp.gate_proj": 32.113441467285156,
        "model.layers.1.mlp.up_proj": 16.793405532836914,
        "model.layers.1.mlp.down_proj": 2.4386825561523438,
        "model.layers.1.mlp": 2.4386825561523438,
        "model.layers.1.post_feedforward_layernorm": 13.130340576171875,
        "model.layers.1": 21.814598083496094,
        "model.layers.2.input_layernorm": 20.279375076293945,
        "model.layers.2.post_attention_layernorm": 13.310257911682129,
        "model.layers.2.pre_feedforward_layernorm": 28.324899673461914
      }
    },
    {
      "sequence": "EXPLAIN COOKING TECHNIQUES.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.291967993197234,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 14.077628135681152,
        "model.layers.0.pre_feedforward_layernorm": 24.28607177734375,
        "model.layers.0.mlp.gate_proj": 32.71770477294922,
        "model.layers.0.mlp.up_proj": 14.612739562988281,
        "model.layers.0.mlp.down_proj": 2.875401258468628,
        "model.layers.0.mlp": 2.875401258468628,
        "model.layers.0.post_feedforward_layernorm": 12.734262466430664,
        "model.layers.0": 15.992571830749512,
        "model.layers.1.input_layernorm": 17.233015060424805,
        "model.layers.1.post_attention_layernorm": 14.725239753723145,
        "model.layers.1.pre_feedforward_layernorm": 28.688661575317383,
        "model.layers.1.mlp.gate_proj": 30.202362060546875,
        "model.layers.1.mlp.up_proj": 16.032760620117188,
        "model.layers.1.mlp.down_proj": 2.0566413402557373,
        "model.layers.1.mlp": 2.0566413402557373,
        "model.layers.1.post_feedforward_layernorm": 13.705578804016113,
        "model.layers.1": 18.005022048950195,
        "model.layers.2.input_layernorm": 18.78230857849121,
        "model.layers.2.post_attention_layernorm": 15.525440216064453,
        "model.layers.2.pre_feedforward_layernorm": 26.1498966217041
      }
    },
    {
      "sequence": "DESCRIBE THE WATER CYCLE.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.428104058555935,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.408975601196289,
        "model.layers.0.pre_feedforward_layernorm": 23.34754180908203,
        "model.layers.0.mlp.gate_proj": 30.4371337890625,
        "model.layers.0.mlp.up_proj": 13.373501777648926,
        "model.layers.0.mlp.down_proj": 2.6803784370422363,
        "model.layers.0.mlp": 2.6803784370422363,
        "model.layers.0.post_feedforward_layernorm": 12.73026180267334,
        "model.layers.0": 14.377435684204102,
        "model.layers.1.input_layernorm": 16.3620662689209,
        "model.layers.1.post_attention_layernorm": 18.88772964477539,
        "model.layers.1.pre_feedforward_layernorm": 27.766157150268555,
        "model.layers.1.mlp.gate_proj": 31.10696029663086,
        "model.layers.1.mlp.up_proj": 16.009363174438477,
        "model.layers.1.mlp.down_proj": 2.7111551761627197,
        "model.layers.1.mlp": 2.7111551761627197,
        "model.layers.1.post_feedforward_layernorm": 20.39293098449707,
        "model.layers.1": 17.860382080078125,
        "model.layers.2.input_layernorm": 19.15114402770996,
        "model.layers.2.post_attention_layernorm": 15.795022010803223,
        "model.layers.2.pre_feedforward_layernorm": 27.16300392150879
      }
    },
    {
      "sequence": "EXPLAIN PROBABILITY THEORY.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.52232957922894,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.791007041931152,
        "model.layers.0.pre_feedforward_layernorm": 24.453584671020508,
        "model.layers.0.mlp.gate_proj": 30.372777938842773,
        "model.layers.0.mlp.up_proj": 13.933038711547852,
        "model.layers.0.mlp.down_proj": 2.891341209411621,
        "model.layers.0.mlp": 2.891341209411621,
        "model.layers.0.post_feedforward_layernorm": 11.707416534423828,
        "model.layers.0": 15.53105354309082,
        "model.layers.1.input_layernorm": 17.162395477294922,
        "model.layers.1.post_attention_layernorm": 16.035480499267578,
        "model.layers.1.pre_feedforward_layernorm": 30.068614959716797,
        "model.layers.1.mlp.gate_proj": 32.97847366333008,
        "model.layers.1.mlp.up_proj": 16.548519134521484,
        "model.layers.1.mlp.down_proj": 2.1718685626983643,
        "model.layers.1.mlp": 2.1718685626983643,
        "model.layers.1.post_feedforward_layernorm": 15.304792404174805,
        "model.layers.1": 18.395694732666016,
        "model.layers.2.input_layernorm": 18.98033905029297,
        "model.layers.2.post_attention_layernorm": 15.821059226989746,
        "model.layers.2.pre_feedforward_layernorm": 27.422998428344727
      }
    },
    {
      "sequence": "DESCRIBE THE PROCESS OF MACHINE LEARNING.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.657651382943857,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.321358680725098,
        "model.layers.0.pre_feedforward_layernorm": 22.955202102661133,
        "model.layers.0.mlp.gate_proj": 29.27109718322754,
        "model.layers.0.mlp.up_proj": 13.619161605834961,
        "model.layers.0.mlp.down_proj": 2.9755778312683105,
        "model.layers.0.mlp": 2.9755778312683105,
        "model.layers.0.post_feedforward_layernorm": 13.493795394897461,
        "model.layers.0": 15.775432586669922,
        "model.layers.1.input_layernorm": 17.604211807250977,
        "model.layers.1.post_attention_layernorm": 18.596778869628906,
        "model.layers.1.pre_feedforward_layernorm": 29.051694869995117,
        "model.layers.1.mlp.gate_proj": 31.283185958862305,
        "model.layers.1.mlp.up_proj": 17.574087142944336,
        "model.layers.1.mlp.down_proj": 2.816145658493042,
        "model.layers.1.mlp": 2.816145658493042,
        "model.layers.1.post_feedforward_layernorm": 19.72852325439453,
        "model.layers.1": 18.21523094177246,
        "model.layers.2.input_layernorm": 18.90296745300293,
        "model.layers.2.post_attention_layernorm": 16.493404388427734,
        "model.layers.2.pre_feedforward_layernorm": 26.993499755859375
      }
    },
    {
      "sequence": "Explain  the  entire  process  of  how  a  computer  processes  a  program  from  source  code  to  execution,  including  compilation,  memory  management,  and  CPU  operations.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.693061642024828,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 10.06553840637207,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 14.173789024353027,
        "model.layers.0.pre_feedforward_layernorm": 26.93093490600586,
        "model.layers.0.mlp.gate_proj": 28.455760955810547,
        "model.layers.0.mlp.up_proj": 12.537479400634766,
        "model.layers.0.mlp.down_proj": 3.136474132537842,
        "model.layers.0.mlp": 3.136474132537842,
        "model.layers.0.post_feedforward_layernorm": 11.864727020263672,
        "model.layers.0": 16.528409957885742,
        "model.layers.1.input_layernorm": 19.462505340576172,
        "model.layers.1.post_attention_layernorm": 15.25702953338623,
        "model.layers.1.pre_feedforward_layernorm": 30.906723022460938,
        "model.layers.1.mlp.gate_proj": 32.44556427001953,
        "model.layers.1.mlp.up_proj": 18.891265869140625,
        "model.layers.1.mlp.down_proj": 2.3653223514556885,
        "model.layers.1.mlp": 2.3653223514556885,
        "model.layers.1.post_feedforward_layernorm": 15.932697296142578,
        "model.layers.1": 20.833662033081055,
        "model.layers.2.input_layernorm": 21.616695404052734,
        "model.layers.2.post_attention_layernorm": 8.07206916809082,
        "model.layers.2.pre_feedforward_layernorm": 22.961973190307617
      }
    },
    {
      "sequence": "DESCRIBE IMPRESSIONIST PAINTING TECHNIQUES.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.73080475434013,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.756402015686035,
        "model.layers.0.pre_feedforward_layernorm": 21.449790954589844,
        "model.layers.0.mlp.gate_proj": 28.639049530029297,
        "model.layers.0.mlp.up_proj": 12.763026237487793,
        "model.layers.0.mlp.down_proj": 2.8549511432647705,
        "model.layers.0.mlp": 2.8549511432647705,
        "model.layers.0.post_feedforward_layernorm": 12.77255630493164,
        "model.layers.0": 16.0327205657959,
        "model.layers.1.input_layernorm": 16.36985969543457,
        "model.layers.1.post_attention_layernorm": 19.028257369995117,
        "model.layers.1.pre_feedforward_layernorm": 30.787479400634766,
        "model.layers.1.mlp.gate_proj": 33.6082649230957,
        "model.layers.1.mlp.up_proj": 17.40181541442871,
        "model.layers.1.mlp.down_proj": 2.7835850715637207,
        "model.layers.1.mlp": 2.7835850715637207,
        "model.layers.1.post_feedforward_layernorm": 20.917346954345703,
        "model.layers.1": 18.261865615844727,
        "model.layers.2.input_layernorm": 19.079368591308594,
        "model.layers.2.post_attention_layernorm": 15.23275375366211,
        "model.layers.2.pre_feedforward_layernorm": 27.686227798461914
      }
    },
    {
      "sequence": "EXPLAIN QUANTUM COMPUTING PRINCIPLES.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.822849999303402,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.469645500183105,
        "model.layers.0.pre_feedforward_layernorm": 23.450395584106445,
        "model.layers.0.mlp.gate_proj": 30.912778854370117,
        "model.layers.0.mlp.up_proj": 13.87286376953125,
        "model.layers.0.mlp.down_proj": 2.8326292037963867,
        "model.layers.0.mlp": 2.8326292037963867,
        "model.layers.0.post_feedforward_layernorm": 11.52726936340332,
        "model.layers.0": 15.35703182220459,
        "model.layers.1.input_layernorm": 16.804458618164062,
        "model.layers.1.post_attention_layernorm": 16.673572540283203,
        "model.layers.1.pre_feedforward_layernorm": 30.32575225830078,
        "model.layers.1.mlp.gate_proj": 34.467342376708984,
        "model.layers.1.mlp.up_proj": 16.67433738708496,
        "model.layers.1.mlp.down_proj": 2.3381142616271973,
        "model.layers.1.mlp": 2.3381142616271973,
        "model.layers.1.post_feedforward_layernorm": 19.261741638183594,
        "model.layers.1": 19.039657592773438,
        "model.layers.2.input_layernorm": 19.022977828979492,
        "model.layers.2.post_attention_layernorm": 15.596322059631348,
        "model.layers.2.pre_feedforward_layernorm": 27.583602905273438
      }
    },
    {
      "sequence": "WHAT IS VERSION CONTROL?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.831856541011645,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 0.0,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 18.187177658081055,
        "model.layers.0.pre_feedforward_layernorm": 29.594629287719727,
        "model.layers.0.mlp.gate_proj": 34.55978012084961,
        "model.layers.0.mlp.up_proj": 17.41773796081543,
        "model.layers.0.mlp.down_proj": 2.6026768684387207,
        "model.layers.0.mlp": 2.6026768684387207,
        "model.layers.0.post_feedforward_layernorm": 13.429747581481934,
        "model.layers.0": 18.44181251525879,
        "model.layers.1.input_layernorm": 20.381759643554688,
        "model.layers.1.post_attention_layernorm": 15.391816139221191,
        "model.layers.1.pre_feedforward_layernorm": 29.17449188232422,
        "model.layers.1.mlp.gate_proj": 28.594575881958008,
        "model.layers.1.mlp.up_proj": 16.77892303466797,
        "model.layers.1.mlp.down_proj": 2.2932546138763428,
        "model.layers.1.mlp": 2.2932546138763428,
        "model.layers.1.post_feedforward_layernorm": 13.507373809814453,
        "model.layers.1": 20.406530380249023,
        "model.layers.2.input_layernorm": 20.53776741027832,
        "model.layers.2.post_attention_layernorm": 12.220380783081055,
        "model.layers.2.pre_feedforward_layernorm": 22.716333389282227
      }
    },
    {
      "sequence": "WHAT IS THE MEANING OF LIFE?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.8365943328194,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 18.12241554260254,
        "model.layers.0.pre_feedforward_layernorm": 32.956336975097656,
        "model.layers.0.mlp.gate_proj": 35.41648864746094,
        "model.layers.0.mlp.up_proj": 18.20511245727539,
        "model.layers.0.mlp.down_proj": 2.6160619258880615,
        "model.layers.0.mlp": 2.6160619258880615,
        "model.layers.0.post_feedforward_layernorm": 12.52202320098877,
        "model.layers.0": 17.713459014892578,
        "model.layers.1.input_layernorm": 20.48052406311035,
        "model.layers.1.post_attention_layernorm": 14.02480697631836,
        "model.layers.1.pre_feedforward_layernorm": 28.17178726196289,
        "model.layers.1.mlp.gate_proj": 28.077407836914062,
        "model.layers.1.mlp.up_proj": 16.120052337646484,
        "model.layers.1.mlp.down_proj": 2.51349139213562,
        "model.layers.1.mlp": 2.51349139213562,
        "model.layers.1.post_feedforward_layernorm": 13.706343650817871,
        "model.layers.1": 19.08254623413086,
        "model.layers.2.input_layernorm": 18.9018611907959,
        "model.layers.2.post_attention_layernorm": 13.080169677734375,
        "model.layers.2.pre_feedforward_layernorm": 21.738325119018555
      }
    },
    {
      "sequence": "Explain ethical dilemmas. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.867512848066246,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.948148727416992,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.562946319580078,
        "model.layers.0.pre_feedforward_layernorm": 23.00422477722168,
        "model.layers.0.mlp.gate_proj": 28.40056037902832,
        "model.layers.0.mlp.up_proj": 12.779691696166992,
        "model.layers.0.mlp.down_proj": 2.544842004776001,
        "model.layers.0.mlp": 2.544842004776001,
        "model.layers.0.post_feedforward_layernorm": 8.877609252929688,
        "model.layers.0": 17.990341186523438,
        "model.layers.1.input_layernorm": 15.373932838439941,
        "model.layers.1.post_attention_layernorm": 14.151618003845215,
        "model.layers.1.pre_feedforward_layernorm": 30.74434471130371,
        "model.layers.1.mlp.gate_proj": 34.256988525390625,
        "model.layers.1.mlp.up_proj": 16.77715492248535,
        "model.layers.1.mlp.down_proj": 3.3558311462402344,
        "model.layers.1.mlp": 3.3558311462402344,
        "model.layers.1.post_feedforward_layernorm": 15.509519577026367,
        "model.layers.1": 19.398441314697266,
        "model.layers.2.input_layernorm": 20.204952239990234,
        "model.layers.2.post_attention_layernorm": 14.84943962097168,
        "model.layers.2.pre_feedforward_layernorm": 30.321535110473633
      }
    },
    {
      "sequence": "I  need  detailed  information  about  this  topic.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.934188884237539,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.294113159179688,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 16.316999435424805,
        "model.layers.0.pre_feedforward_layernorm": 29.357023239135742,
        "model.layers.0.mlp.gate_proj": 30.987316131591797,
        "model.layers.0.mlp.up_proj": 14.508583068847656,
        "model.layers.0.mlp.down_proj": 3.1309783458709717,
        "model.layers.0.mlp": 3.1309783458709717,
        "model.layers.0.post_feedforward_layernorm": 14.62773609161377,
        "model.layers.0": 16.904285430908203,
        "model.layers.1.input_layernorm": 18.975332260131836,
        "model.layers.1.post_attention_layernorm": 13.632555961608887,
        "model.layers.1.pre_feedforward_layernorm": 29.26113510131836,
        "model.layers.1.mlp.gate_proj": 27.516983032226562,
        "model.layers.1.mlp.up_proj": 17.707271575927734,
        "model.layers.1.mlp.down_proj": 2.6023948192596436,
        "model.layers.1.mlp": 2.6023948192596436,
        "model.layers.1.post_feedforward_layernorm": 16.3327579498291,
        "model.layers.1": 21.551681518554688,
        "model.layers.2.input_layernorm": 22.072467803955078,
        "model.layers.2.post_attention_layernorm": 10.515012741088867,
        "model.layers.2.pre_feedforward_layernorm": 23.458343505859375
      }
    },
    {
      "sequence": "HOW DO TRANSFORMERS PROCESS LANGUAGE?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 14.94913187234298,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 17.164167404174805,
        "model.layers.0.pre_feedforward_layernorm": 29.38336753845215,
        "model.layers.0.mlp.gate_proj": 34.67509841918945,
        "model.layers.0.mlp.up_proj": 16.338876724243164,
        "model.layers.0.mlp.down_proj": 2.8190910816192627,
        "model.layers.0.mlp": 2.8190910816192627,
        "model.layers.0.post_feedforward_layernorm": 13.292176246643066,
        "model.layers.0": 18.04155731201172,
        "model.layers.1.input_layernorm": 20.03676414489746,
        "model.layers.1.post_attention_layernorm": 16.650182723999023,
        "model.layers.1.pre_feedforward_layernorm": 29.66643714904785,
        "model.layers.1.mlp.gate_proj": 29.05379295349121,
        "model.layers.1.mlp.up_proj": 17.193750381469727,
        "model.layers.1.mlp.down_proj": 2.079840898513794,
        "model.layers.1.mlp": 2.079840898513794,
        "model.layers.1.post_feedforward_layernorm": 15.528524398803711,
        "model.layers.1": 20.490814208984375,
        "model.layers.2.input_layernorm": 19.289230346679688,
        "model.layers.2.post_attention_layernorm": 12.2874116897583,
        "model.layers.2.pre_feedforward_layernorm": 22.42177963256836
      }
    },
    {
      "sequence": "EXPLAIN CALCULUS CONCEPTS.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.051707018976627,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 14.702689170837402,
        "model.layers.0.pre_feedforward_layernorm": 27.292980194091797,
        "model.layers.0.mlp.gate_proj": 35.595855712890625,
        "model.layers.0.mlp.up_proj": 15.25804615020752,
        "model.layers.0.mlp.down_proj": 2.805605411529541,
        "model.layers.0.mlp": 2.805605411529541,
        "model.layers.0.post_feedforward_layernorm": 11.864636421203613,
        "model.layers.0": 15.346113204956055,
        "model.layers.1.input_layernorm": 17.6451416015625,
        "model.layers.1.post_attention_layernorm": 15.422558784484863,
        "model.layers.1.pre_feedforward_layernorm": 31.21004295349121,
        "model.layers.1.mlp.gate_proj": 32.83603286743164,
        "model.layers.1.mlp.up_proj": 17.088130950927734,
        "model.layers.1.mlp.down_proj": 2.2763941287994385,
        "model.layers.1.mlp": 2.2763941287994385,
        "model.layers.1.post_feedforward_layernorm": 15.609047889709473,
        "model.layers.1": 18.84971809387207,
        "model.layers.2.input_layernorm": 19.86469078063965,
        "model.layers.2.post_attention_layernorm": 15.01380443572998,
        "model.layers.2.pre_feedforward_layernorm": 27.04585838317871
      }
    },
    {
      "sequence": "Compare and contrast these concepts. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.166007788284965,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.28824234008789,
        "model.layers.0.pre_feedforward_layernorm": 24.151514053344727,
        "model.layers.0.mlp.gate_proj": 30.124025344848633,
        "model.layers.0.mlp.up_proj": 13.049517631530762,
        "model.layers.0.mlp.down_proj": 2.5089449882507324,
        "model.layers.0.mlp": 2.5089449882507324,
        "model.layers.0.post_feedforward_layernorm": 9.58664608001709,
        "model.layers.0": 16.996774673461914,
        "model.layers.1.input_layernorm": 15.869380950927734,
        "model.layers.1.post_attention_layernorm": 13.585556983947754,
        "model.layers.1.pre_feedforward_layernorm": 32.07471466064453,
        "model.layers.1.mlp.gate_proj": 33.813011169433594,
        "model.layers.1.mlp.up_proj": 18.154733657836914,
        "model.layers.1.mlp.down_proj": 3.247802257537842,
        "model.layers.1.mlp": 3.247802257537842,
        "model.layers.1.post_feedforward_layernorm": 13.40842342376709,
        "model.layers.1": 19.9739990234375,
        "model.layers.2.input_layernorm": 20.974149703979492,
        "model.layers.2.post_attention_layernorm": 16.08188247680664,
        "model.layers.2.pre_feedforward_layernorm": 32.248313903808594
      }
    },
    {
      "sequence": "HOW DO I CHANGE A TIRE?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.182047159775443,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 18.592327117919922,
        "model.layers.0.pre_feedforward_layernorm": 30.184593200683594,
        "model.layers.0.mlp.gate_proj": 36.869422912597656,
        "model.layers.0.mlp.up_proj": 17.225196838378906,
        "model.layers.0.mlp.down_proj": 3.1434831619262695,
        "model.layers.0.mlp": 3.1434831619262695,
        "model.layers.0.post_feedforward_layernorm": 14.574542045593262,
        "model.layers.0": 18.579824447631836,
        "model.layers.1.input_layernorm": 20.740530014038086,
        "model.layers.1.post_attention_layernorm": 16.294034957885742,
        "model.layers.1.pre_feedforward_layernorm": 30.316017150878906,
        "model.layers.1.mlp.gate_proj": 29.209352493286133,
        "model.layers.1.mlp.up_proj": 17.317045211791992,
        "model.layers.1.mlp.down_proj": 2.018791913986206,
        "model.layers.1.mlp": 2.018791913986206,
        "model.layers.1.post_feedforward_layernorm": 14.20325756072998,
        "model.layers.1": 20.277122497558594,
        "model.layers.2.input_layernorm": 20.100322723388672,
        "model.layers.2.post_attention_layernorm": 11.11291217803955,
        "model.layers.2.pre_feedforward_layernorm": 20.603130340576172
      }
    },
    {
      "sequence": "HOW DOES PHOTOSYNTHESIS WORK?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.311103250669396,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 16.313396453857422,
        "model.layers.0.pre_feedforward_layernorm": 28.637908935546875,
        "model.layers.0.mlp.gate_proj": 35.85538864135742,
        "model.layers.0.mlp.up_proj": 16.706789016723633,
        "model.layers.0.mlp.down_proj": 2.7264676094055176,
        "model.layers.0.mlp": 2.7264676094055176,
        "model.layers.0.post_feedforward_layernorm": 13.150323867797852,
        "model.layers.0": 17.094146728515625,
        "model.layers.1.input_layernorm": 19.34693717956543,
        "model.layers.1.post_attention_layernorm": 15.538836479187012,
        "model.layers.1.pre_feedforward_layernorm": 31.67015266418457,
        "model.layers.1.mlp.gate_proj": 35.226985931396484,
        "model.layers.1.mlp.up_proj": 17.331912994384766,
        "model.layers.1.mlp.down_proj": 1.9500333070755005,
        "model.layers.1.mlp": 1.9500333070755005,
        "model.layers.1.post_feedforward_layernorm": 15.405965805053711,
        "model.layers.1": 20.50809669494629,
        "model.layers.2.input_layernorm": 20.069372177124023,
        "model.layers.2.post_attention_layernorm": 12.543431282043457,
        "model.layers.2.pre_feedforward_layernorm": 22.324588775634766
      }
    },
    {
      "sequence": "Explain how neural networks work. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.353556798852008,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.04863166809082,
        "model.layers.0.pre_feedforward_layernorm": 24.869001388549805,
        "model.layers.0.mlp.gate_proj": 30.0107421875,
        "model.layers.0.mlp.up_proj": 13.152332305908203,
        "model.layers.0.mlp.down_proj": 2.384323835372925,
        "model.layers.0.mlp": 2.384323835372925,
        "model.layers.0.post_feedforward_layernorm": 8.45787239074707,
        "model.layers.0": 17.600919723510742,
        "model.layers.1.input_layernorm": 15.362424850463867,
        "model.layers.1.post_attention_layernorm": 14.525121688842773,
        "model.layers.1.pre_feedforward_layernorm": 31.384057998657227,
        "model.layers.1.mlp.gate_proj": 36.0634765625,
        "model.layers.1.mlp.up_proj": 17.55464744567871,
        "model.layers.1.mlp.down_proj": 3.369830369949341,
        "model.layers.1.mlp": 3.369830369949341,
        "model.layers.1.post_feedforward_layernorm": 14.392691612243652,
        "model.layers.1": 20.414268493652344,
        "model.layers.2.input_layernorm": 20.86369514465332,
        "model.layers.2.post_attention_layernorm": 15.987372398376465,
        "model.layers.2.pre_feedforward_layernorm": 34.01244354248047
      }
    },
    {
      "sequence": "Analyze Shakespeare's writing style. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.358728864918584,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.852492332458496,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 17.992319107055664,
        "model.layers.0.pre_feedforward_layernorm": 24.64185333251953,
        "model.layers.0.mlp.gate_proj": 29.637617111206055,
        "model.layers.0.mlp.up_proj": 13.77313232421875,
        "model.layers.0.mlp.down_proj": 2.3666248321533203,
        "model.layers.0.mlp": 2.3666248321533203,
        "model.layers.0.post_feedforward_layernorm": 9.580460548400879,
        "model.layers.0": 15.13439655303955,
        "model.layers.1.input_layernorm": 15.599170684814453,
        "model.layers.1.post_attention_layernorm": 14.630531311035156,
        "model.layers.1.pre_feedforward_layernorm": 33.20307922363281,
        "model.layers.1.mlp.gate_proj": 36.98304748535156,
        "model.layers.1.mlp.up_proj": 18.415836334228516,
        "model.layers.1.mlp.down_proj": 3.0243921279907227,
        "model.layers.1.mlp": 3.0243921279907227,
        "model.layers.1.post_feedforward_layernorm": 14.318716049194336,
        "model.layers.1": 20.011070251464844,
        "model.layers.2.input_layernorm": 21.350887298583984,
        "model.layers.2.post_attention_layernorm": 16.611793518066406,
        "model.layers.2.pre_feedforward_layernorm": 32.73232650756836
      }
    },
    {
      "sequence": "DESCRIBE FREE WILL VS DETERMINISM.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.363515086795973,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 12.945056915283203,
        "model.layers.0.pre_feedforward_layernorm": 24.526063919067383,
        "model.layers.0.mlp.gate_proj": 29.35960578918457,
        "model.layers.0.mlp.up_proj": 14.13712215423584,
        "model.layers.0.mlp.down_proj": 2.9081079959869385,
        "model.layers.0.mlp": 2.9081079959869385,
        "model.layers.0.post_feedforward_layernorm": 11.507835388183594,
        "model.layers.0": 15.31812858581543,
        "model.layers.1.input_layernorm": 17.73921012878418,
        "model.layers.1.post_attention_layernorm": 19.309072494506836,
        "model.layers.1.pre_feedforward_layernorm": 34.11701583862305,
        "model.layers.1.mlp.gate_proj": 37.60633850097656,
        "model.layers.1.mlp.up_proj": 19.428125381469727,
        "model.layers.1.mlp.down_proj": 2.867380142211914,
        "model.layers.1.mlp": 2.867380142211914,
        "model.layers.1.post_feedforward_layernorm": 19.47110939025879,
        "model.layers.1": 19.94866180419922,
        "model.layers.2.input_layernorm": 20.778751373291016,
        "model.layers.2.post_attention_layernorm": 14.271812438964844,
        "model.layers.2.pre_feedforward_layernorm": 28.68305778503418
      }
    },
    {
      "sequence": "HOW DO WE DEFINE TRUTH?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.391901544902636,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 17.259784698486328,
        "model.layers.0.pre_feedforward_layernorm": 29.729310989379883,
        "model.layers.0.mlp.gate_proj": 37.683380126953125,
        "model.layers.0.mlp.up_proj": 17.340087890625,
        "model.layers.0.mlp.down_proj": 2.8269972801208496,
        "model.layers.0.mlp": 2.8269972801208496,
        "model.layers.0.post_feedforward_layernorm": 13.866601943969727,
        "model.layers.0": 18.027700424194336,
        "model.layers.1.input_layernorm": 20.615829467773438,
        "model.layers.1.post_attention_layernorm": 16.698225021362305,
        "model.layers.1.pre_feedforward_layernorm": 31.65624237060547,
        "model.layers.1.mlp.gate_proj": 32.31650161743164,
        "model.layers.1.mlp.up_proj": 17.354257583618164,
        "model.layers.1.mlp.down_proj": 1.9499597549438477,
        "model.layers.1.mlp": 1.9499597549438477,
        "model.layers.1.post_feedforward_layernorm": 16.53866958618164,
        "model.layers.1": 20.13404083251953,
        "model.layers.2.input_layernorm": 19.641759872436523,
        "model.layers.2.post_attention_layernorm": 11.465383529663086,
        "model.layers.2.pre_feedforward_layernorm": 21.613807678222656
      }
    },
    {
      "sequence": "Describe the complete lifecycle of a star from formation to death, including all stages and physical processes involved. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.413707380709441,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.659636497497559,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.426250457763672,
        "model.layers.0.pre_feedforward_layernorm": 21.551605224609375,
        "model.layers.0.mlp.gate_proj": 24.539161682128906,
        "model.layers.0.mlp.up_proj": 11.796504974365234,
        "model.layers.0.mlp.down_proj": 2.629063367843628,
        "model.layers.0.mlp": 2.629063367843628,
        "model.layers.0.post_feedforward_layernorm": 10.041801452636719,
        "model.layers.0": 14.418228149414062,
        "model.layers.1.input_layernorm": 15.590812683105469,
        "model.layers.1.post_attention_layernorm": 15.050935745239258,
        "model.layers.1.pre_feedforward_layernorm": 35.68836212158203,
        "model.layers.1.mlp.gate_proj": 40.30681228637695,
        "model.layers.1.mlp.up_proj": 20.359582901000977,
        "model.layers.1.mlp.down_proj": 3.0925498008728027,
        "model.layers.1.mlp": 3.0925498008728027,
        "model.layers.1.post_feedforward_layernorm": 15.382145881652832,
        "model.layers.1": 22.428691864013672,
        "model.layers.2.input_layernorm": 23.123443603515625,
        "model.layers.2.post_attention_layernorm": 14.697115898132324,
        "model.layers.2.pre_feedforward_layernorm": 35.01095199584961
      }
    },
    {
      "sequence": "COMPARE AND CONTRAST THESE CONCEPTS.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.448762033296669,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.847043991088867,
        "model.layers.0.pre_feedforward_layernorm": 24.215465545654297,
        "model.layers.0.mlp.gate_proj": 29.75385093688965,
        "model.layers.0.mlp.up_proj": 13.767860412597656,
        "model.layers.0.mlp.down_proj": 2.5906929969787598,
        "model.layers.0.mlp": 2.5906929969787598,
        "model.layers.0.post_feedforward_layernorm": 11.480682373046875,
        "model.layers.0": 14.218277931213379,
        "model.layers.1.input_layernorm": 16.534400939941406,
        "model.layers.1.post_attention_layernorm": 19.714859008789062,
        "model.layers.1.pre_feedforward_layernorm": 33.7341194152832,
        "model.layers.1.mlp.gate_proj": 36.476993560791016,
        "model.layers.1.mlp.up_proj": 19.44659423828125,
        "model.layers.1.mlp.down_proj": 3.2196366786956787,
        "model.layers.1.mlp": 3.2196366786956787,
        "model.layers.1.post_feedforward_layernorm": 21.70440101623535,
        "model.layers.1": 20.98698616027832,
        "model.layers.2.input_layernorm": 22.36205291748047,
        "model.layers.2.post_attention_layernorm": 14.362882614135742,
        "model.layers.2.pre_feedforward_layernorm": 28.57615852355957
      }
    },
    {
      "sequence": "WHAT ARE THE PROS AND CONS?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.455078456712807,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 18.888427734375,
        "model.layers.0.pre_feedforward_layernorm": 31.031173706054688,
        "model.layers.0.mlp.gate_proj": 35.457908630371094,
        "model.layers.0.mlp.up_proj": 18.700807571411133,
        "model.layers.0.mlp.down_proj": 2.78190279006958,
        "model.layers.0.mlp": 2.78190279006958,
        "model.layers.0.post_feedforward_layernorm": 14.15967845916748,
        "model.layers.0": 18.52863311767578,
        "model.layers.1.input_layernorm": 21.396474838256836,
        "model.layers.1.post_attention_layernorm": 15.360312461853027,
        "model.layers.1.pre_feedforward_layernorm": 30.283620834350586,
        "model.layers.1.mlp.gate_proj": 30.34393310546875,
        "model.layers.1.mlp.up_proj": 17.886856079101562,
        "model.layers.1.mlp.down_proj": 2.287039279937744,
        "model.layers.1.mlp": 2.287039279937744,
        "model.layers.1.post_feedforward_layernorm": 15.48586368560791,
        "model.layers.1": 21.532875061035156,
        "model.layers.2.input_layernorm": 20.805776596069336,
        "model.layers.2.post_attention_layernorm": 11.007819175720215,
        "model.layers.2.pre_feedforward_layernorm": 21.795856475830078
      }
    },
    {
      "sequence": "ANALYZE SHAKESPEARE'S WRITING STYLE.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.492638940396516,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.206338882446289,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.346301078796387,
        "model.layers.0.pre_feedforward_layernorm": 22.948688507080078,
        "model.layers.0.mlp.gate_proj": 29.702774047851562,
        "model.layers.0.mlp.up_proj": 14.453547477722168,
        "model.layers.0.mlp.down_proj": 3.0067737102508545,
        "model.layers.0.mlp": 3.0067737102508545,
        "model.layers.0.post_feedforward_layernorm": 12.27402114868164,
        "model.layers.0": 16.33868408203125,
        "model.layers.1.input_layernorm": 18.256179809570312,
        "model.layers.1.post_attention_layernorm": 18.008625030517578,
        "model.layers.1.pre_feedforward_layernorm": 32.15077209472656,
        "model.layers.1.mlp.gate_proj": 40.94925308227539,
        "model.layers.1.mlp.up_proj": 19.016998291015625,
        "model.layers.1.mlp.down_proj": 2.788893222808838,
        "model.layers.1.mlp": 2.788893222808838,
        "model.layers.1.post_feedforward_layernorm": 20.524301528930664,
        "model.layers.1": 18.583181381225586,
        "model.layers.2.input_layernorm": 20.645166397094727,
        "model.layers.2.post_attention_layernorm": 12.959099769592285,
        "model.layers.2.pre_feedforward_layernorm": 26.375429153442383
      }
    },
    {
      "sequence": "Explain cooking techniques. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.517620376918627,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.948148727416992,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 20.46514892578125,
        "model.layers.0.pre_feedforward_layernorm": 24.645559310913086,
        "model.layers.0.mlp.gate_proj": 30.416250228881836,
        "model.layers.0.mlp.up_proj": 13.525891304016113,
        "model.layers.0.mlp.down_proj": 2.6400840282440186,
        "model.layers.0.mlp": 2.6400840282440186,
        "model.layers.0.post_feedforward_layernorm": 9.27051067352295,
        "model.layers.0": 19.240943908691406,
        "model.layers.1.input_layernorm": 16.190105438232422,
        "model.layers.1.post_attention_layernorm": 14.076923370361328,
        "model.layers.1.pre_feedforward_layernorm": 31.857383728027344,
        "model.layers.1.mlp.gate_proj": 35.32621383666992,
        "model.layers.1.mlp.up_proj": 17.647083282470703,
        "model.layers.1.mlp.down_proj": 3.3605310916900635,
        "model.layers.1.mlp": 3.3605310916900635,
        "model.layers.1.post_feedforward_layernorm": 13.970088005065918,
        "model.layers.1": 21.387008666992188,
        "model.layers.2.input_layernorm": 21.096527099609375,
        "model.layers.2.post_attention_layernorm": 15.578266143798828,
        "model.layers.2.pre_feedforward_layernorm": 32.261985778808594
      }
    },
    {
      "sequence": "Explain the entire process of how a computer processes a program from source code to execution, including compilation, memory management, and CPU operations. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.569735734359078,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.807018756866455,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 14.272579193115234,
        "model.layers.0.pre_feedforward_layernorm": 21.939809799194336,
        "model.layers.0.mlp.gate_proj": 26.74704933166504,
        "model.layers.0.mlp.up_proj": 12.530540466308594,
        "model.layers.0.mlp.down_proj": 2.2805707454681396,
        "model.layers.0.mlp": 2.2805707454681396,
        "model.layers.0.post_feedforward_layernorm": 8.334874153137207,
        "model.layers.0": 13.792221069335938,
        "model.layers.1.input_layernorm": 15.526103973388672,
        "model.layers.1.post_attention_layernorm": 15.963371276855469,
        "model.layers.1.pre_feedforward_layernorm": 34.21421813964844,
        "model.layers.1.mlp.gate_proj": 39.939109802246094,
        "model.layers.1.mlp.up_proj": 19.905059814453125,
        "model.layers.1.mlp.down_proj": 3.282299518585205,
        "model.layers.1.mlp": 3.282299518585205,
        "model.layers.1.post_feedforward_layernorm": 16.78005027770996,
        "model.layers.1": 22.86083221435547,
        "model.layers.2.input_layernorm": 23.401031494140625,
        "model.layers.2.post_attention_layernorm": 17.067758560180664,
        "model.layers.2.pre_feedforward_layernorm": 35.89655303955078
      }
    },
    {
      "sequence": "HOW DOES ENCRYPTION WORK?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.611318878505541,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.02157974243164,
        "model.layers.0.pre_feedforward_layernorm": 33.45736312866211,
        "model.layers.0.mlp.gate_proj": 38.674072265625,
        "model.layers.0.mlp.up_proj": 18.40961456298828,
        "model.layers.0.mlp.down_proj": 2.663536548614502,
        "model.layers.0.mlp": 2.663536548614502,
        "model.layers.0.post_feedforward_layernorm": 13.364236831665039,
        "model.layers.0": 17.61134147644043,
        "model.layers.1.input_layernorm": 19.844449996948242,
        "model.layers.1.post_attention_layernorm": 15.956539154052734,
        "model.layers.1.pre_feedforward_layernorm": 30.372207641601562,
        "model.layers.1.mlp.gate_proj": 31.799314498901367,
        "model.layers.1.mlp.up_proj": 17.42661476135254,
        "model.layers.1.mlp.down_proj": 2.1117258071899414,
        "model.layers.1.mlp": 2.1117258071899414,
        "model.layers.1.post_feedforward_layernorm": 15.776999473571777,
        "model.layers.1": 19.836427688598633,
        "model.layers.2.input_layernorm": 19.523029327392578,
        "model.layers.2.post_attention_layernorm": 11.54364013671875,
        "model.layers.2.pre_feedforward_layernorm": 21.814239501953125
      }
    },
    {
      "sequence": "Describe free will vs determinism. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.612958472707998,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.852492332458496,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 18.8726806640625,
        "model.layers.0.pre_feedforward_layernorm": 24.656572341918945,
        "model.layers.0.mlp.gate_proj": 29.296293258666992,
        "model.layers.0.mlp.up_proj": 13.676429748535156,
        "model.layers.0.mlp.down_proj": 2.4396705627441406,
        "model.layers.0.mlp": 2.4396705627441406,
        "model.layers.0.post_feedforward_layernorm": 8.66091537475586,
        "model.layers.0": 17.897642135620117,
        "model.layers.1.input_layernorm": 16.12563133239746,
        "model.layers.1.post_attention_layernorm": 15.373455047607422,
        "model.layers.1.pre_feedforward_layernorm": 32.981170654296875,
        "model.layers.1.mlp.gate_proj": 36.59798812866211,
        "model.layers.1.mlp.up_proj": 18.739404678344727,
        "model.layers.1.mlp.down_proj": 3.469576120376587,
        "model.layers.1.mlp": 3.469576120376587,
        "model.layers.1.post_feedforward_layernorm": 14.239794731140137,
        "model.layers.1": 21.712791442871094,
        "model.layers.2.input_layernorm": 22.313827514648438,
        "model.layers.2.post_attention_layernorm": 15.252303123474121,
        "model.layers.2.pre_feedforward_layernorm": 33.03015899658203
      }
    },
    {
      "sequence": "CAN YOU HELP ME UNDERSTAND THIS?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.61634055427883,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.332557678222656,
        "model.layers.0.pre_feedforward_layernorm": 29.70575523376465,
        "model.layers.0.mlp.gate_proj": 34.41141128540039,
        "model.layers.0.mlp.up_proj": 17.249813079833984,
        "model.layers.0.mlp.down_proj": 2.6808290481567383,
        "model.layers.0.mlp": 2.6808290481567383,
        "model.layers.0.post_feedforward_layernorm": 13.826068878173828,
        "model.layers.0": 19.857332229614258,
        "model.layers.1.input_layernorm": 21.733125686645508,
        "model.layers.1.post_attention_layernorm": 15.477392196655273,
        "model.layers.1.pre_feedforward_layernorm": 30.03502655029297,
        "model.layers.1.mlp.gate_proj": 28.81576156616211,
        "model.layers.1.mlp.up_proj": 18.495113372802734,
        "model.layers.1.mlp.down_proj": 2.5600571632385254,
        "model.layers.1.mlp": 2.5600571632385254,
        "model.layers.1.post_feedforward_layernorm": 16.493192672729492,
        "model.layers.1": 23.00516128540039,
        "model.layers.2.input_layernorm": 22.21467399597168,
        "model.layers.2.post_attention_layernorm": 12.124445915222168,
        "model.layers.2.pre_feedforward_layernorm": 23.25432586669922
      }
    },
    {
      "sequence": "WHAT IS ARTIFICIAL INTELLIGENCE?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.7500378359919,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.544312953948975,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.069570541381836,
        "model.layers.0.pre_feedforward_layernorm": 33.54033660888672,
        "model.layers.0.mlp.gate_proj": 41.49247360229492,
        "model.layers.0.mlp.up_proj": 18.988054275512695,
        "model.layers.0.mlp.down_proj": 2.878878355026245,
        "model.layers.0.mlp": 2.878878355026245,
        "model.layers.0.post_feedforward_layernorm": 15.442072868347168,
        "model.layers.0": 19.313440322875977,
        "model.layers.1.input_layernorm": 21.25602149963379,
        "model.layers.1.post_attention_layernorm": 16.014480590820312,
        "model.layers.1.pre_feedforward_layernorm": 27.278926849365234,
        "model.layers.1.mlp.gate_proj": 26.854801177978516,
        "model.layers.1.mlp.up_proj": 16.426130294799805,
        "model.layers.1.mlp.down_proj": 2.3444125652313232,
        "model.layers.1.mlp": 2.3444125652313232,
        "model.layers.1.post_feedforward_layernorm": 14.099648475646973,
        "model.layers.1": 20.63605308532715,
        "model.layers.2.input_layernorm": 19.734907150268555,
        "model.layers.2.post_attention_layernorm": 13.282129287719727,
        "model.layers.2.pre_feedforward_layernorm": 21.830928802490234
      }
    },
    {
      "sequence": "Explain calculus concepts. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.753899429155434,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.948148727416992,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.899993896484375,
        "model.layers.0.pre_feedforward_layernorm": 24.452316284179688,
        "model.layers.0.mlp.gate_proj": 30.758270263671875,
        "model.layers.0.mlp.up_proj": 13.348702430725098,
        "model.layers.0.mlp.down_proj": 2.7217133045196533,
        "model.layers.0.mlp": 2.7217133045196533,
        "model.layers.0.post_feedforward_layernorm": 9.124689102172852,
        "model.layers.0": 18.555578231811523,
        "model.layers.1.input_layernorm": 16.163389205932617,
        "model.layers.1.post_attention_layernorm": 14.733171463012695,
        "model.layers.1.pre_feedforward_layernorm": 31.99169921875,
        "model.layers.1.mlp.gate_proj": 35.0400390625,
        "model.layers.1.mlp.up_proj": 18.107650756835938,
        "model.layers.1.mlp.down_proj": 4.040038108825684,
        "model.layers.1.mlp": 4.040038108825684,
        "model.layers.1.post_feedforward_layernorm": 15.272075653076172,
        "model.layers.1": 21.941848754882812,
        "model.layers.2.input_layernorm": 22.013093948364258,
        "model.layers.2.post_attention_layernorm": 16.34688377380371,
        "model.layers.2.pre_feedforward_layernorm": 33.11863327026367
      }
    },
    {
      "sequence": "Explain probability theory. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.764852938444719,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.948148727416992,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.65419578552246,
        "model.layers.0.pre_feedforward_layernorm": 24.52326202392578,
        "model.layers.0.mlp.gate_proj": 30.12346649169922,
        "model.layers.0.mlp.up_proj": 13.095419883728027,
        "model.layers.0.mlp.down_proj": 2.654458522796631,
        "model.layers.0.mlp": 2.654458522796631,
        "model.layers.0.post_feedforward_layernorm": 9.088255882263184,
        "model.layers.0": 19.52657699584961,
        "model.layers.1.input_layernorm": 15.943435668945312,
        "model.layers.1.post_attention_layernorm": 14.791340827941895,
        "model.layers.1.pre_feedforward_layernorm": 32.36894607543945,
        "model.layers.1.mlp.gate_proj": 37.228614807128906,
        "model.layers.1.mlp.up_proj": 18.02437973022461,
        "model.layers.1.mlp.down_proj": 3.765944004058838,
        "model.layers.1.mlp": 3.765944004058838,
        "model.layers.1.post_feedforward_layernorm": 14.749979972839355,
        "model.layers.1": 21.459741592407227,
        "model.layers.2.input_layernorm": 21.105188369750977,
        "model.layers.2.post_attention_layernorm": 16.366846084594727,
        "model.layers.2.pre_feedforward_layernorm": 33.753013610839844
      }
    },
    {
      "sequence": "DESCRIBE ANCIENT EGYPTIAN CIVILIZATION.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.766661892766537,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 14.69760513305664,
        "model.layers.0.pre_feedforward_layernorm": 24.815147399902344,
        "model.layers.0.mlp.gate_proj": 31.1842098236084,
        "model.layers.0.mlp.up_proj": 14.331932067871094,
        "model.layers.0.mlp.down_proj": 2.997908592224121,
        "model.layers.0.mlp": 2.997908592224121,
        "model.layers.0.post_feedforward_layernorm": 12.326216697692871,
        "model.layers.0": 16.898643493652344,
        "model.layers.1.input_layernorm": 18.26064109802246,
        "model.layers.1.post_attention_layernorm": 18.64222526550293,
        "model.layers.1.pre_feedforward_layernorm": 31.868221282958984,
        "model.layers.1.mlp.gate_proj": 35.9818229675293,
        "model.layers.1.mlp.up_proj": 17.585350036621094,
        "model.layers.1.mlp.down_proj": 2.78425669670105,
        "model.layers.1.mlp": 2.78425669670105,
        "model.layers.1.post_feedforward_layernorm": 22.00874137878418,
        "model.layers.1": 20.002281188964844,
        "model.layers.2.input_layernorm": 20.807126998901367,
        "model.layers.2.post_attention_layernorm": 15.890247344970703,
        "model.layers.2.pre_feedforward_layernorm": 28.137828826904297
      }
    },
    {
      "sequence": "Describe time management methods. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.831962233004363,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 21.941835403442383,
        "model.layers.0.pre_feedforward_layernorm": 25.611164093017578,
        "model.layers.0.mlp.gate_proj": 31.757034301757812,
        "model.layers.0.mlp.up_proj": 14.011445999145508,
        "model.layers.0.mlp.down_proj": 2.8669304847717285,
        "model.layers.0.mlp": 2.8669304847717285,
        "model.layers.0.post_feedforward_layernorm": 10.028858184814453,
        "model.layers.0": 20.047014236450195,
        "model.layers.1.input_layernorm": 16.947731018066406,
        "model.layers.1.post_attention_layernorm": 14.566479682922363,
        "model.layers.1.pre_feedforward_layernorm": 32.27465057373047,
        "model.layers.1.mlp.gate_proj": 34.5146369934082,
        "model.layers.1.mlp.up_proj": 18.361852645874023,
        "model.layers.1.mlp.down_proj": 3.2779417037963867,
        "model.layers.1.mlp": 3.2779417037963867,
        "model.layers.1.post_feedforward_layernorm": 14.344578742980957,
        "model.layers.1": 22.250568389892578,
        "model.layers.2.input_layernorm": 22.607397079467773,
        "model.layers.2.post_attention_layernorm": 14.277942657470703,
        "model.layers.2.pre_feedforward_layernorm": 30.671545028686523
      }
    },
    {
      "sequence": "Describe API design principles. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.841598552206289,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 20.82257080078125,
        "model.layers.0.pre_feedforward_layernorm": 25.084320068359375,
        "model.layers.0.mlp.gate_proj": 32.29838562011719,
        "model.layers.0.mlp.up_proj": 14.023419380187988,
        "model.layers.0.mlp.down_proj": 2.7274270057678223,
        "model.layers.0.mlp": 2.7274270057678223,
        "model.layers.0.post_feedforward_layernorm": 9.369821548461914,
        "model.layers.0": 19.08852195739746,
        "model.layers.1.input_layernorm": 16.679574966430664,
        "model.layers.1.post_attention_layernorm": 14.785516738891602,
        "model.layers.1.pre_feedforward_layernorm": 32.834617614746094,
        "model.layers.1.mlp.gate_proj": 35.75321960449219,
        "model.layers.1.mlp.up_proj": 18.555143356323242,
        "model.layers.1.mlp.down_proj": 3.3891398906707764,
        "model.layers.1.mlp": 3.3891398906707764,
        "model.layers.1.post_feedforward_layernorm": 14.124751091003418,
        "model.layers.1": 22.74378204345703,
        "model.layers.2.input_layernorm": 22.31350326538086,
        "model.layers.2.post_attention_layernorm": 14.864204406738281,
        "model.layers.2.pre_feedforward_layernorm": 31.151628494262695
      }
    },
    {
      "sequence": "Describe impressionist painting techniques. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 15.90375804901123,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.36248207092285,
        "model.layers.0.pre_feedforward_layernorm": 24.65558433532715,
        "model.layers.0.mlp.gate_proj": 29.46817970275879,
        "model.layers.0.mlp.up_proj": 13.711420059204102,
        "model.layers.0.mlp.down_proj": 2.6372082233428955,
        "model.layers.0.mlp": 2.6372082233428955,
        "model.layers.0.post_feedforward_layernorm": 10.018662452697754,
        "model.layers.0": 17.725671768188477,
        "model.layers.1.input_layernorm": 16.579837799072266,
        "model.layers.1.post_attention_layernorm": 15.268067359924316,
        "model.layers.1.pre_feedforward_layernorm": 34.66483688354492,
        "model.layers.1.mlp.gate_proj": 37.681983947753906,
        "model.layers.1.mlp.up_proj": 18.906885147094727,
        "model.layers.1.mlp.down_proj": 3.354120969772339,
        "model.layers.1.mlp": 3.354120969772339,
        "model.layers.1.post_feedforward_layernorm": 14.96882152557373,
        "model.layers.1": 21.521251678466797,
        "model.layers.2.input_layernorm": 22.465383529663086,
        "model.layers.2.post_attention_layernorm": 15.635746955871582,
        "model.layers.2.pre_feedforward_layernorm": 33.24516296386719
      }
    },
    {
      "sequence": "Explain film editing principles. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 16.081516452457596,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.673133850097656,
        "model.layers.0.pre_feedforward_layernorm": 24.55603790283203,
        "model.layers.0.mlp.gate_proj": 30.02281951904297,
        "model.layers.0.mlp.up_proj": 13.679600715637207,
        "model.layers.0.mlp.down_proj": 2.6839542388916016,
        "model.layers.0.mlp": 2.6839542388916016,
        "model.layers.0.post_feedforward_layernorm": 9.359871864318848,
        "model.layers.0": 18.1297550201416,
        "model.layers.1.input_layernorm": 16.205528259277344,
        "model.layers.1.post_attention_layernorm": 16.17388343811035,
        "model.layers.1.pre_feedforward_layernorm": 34.91843032836914,
        "model.layers.1.mlp.gate_proj": 39.30708312988281,
        "model.layers.1.mlp.up_proj": 18.86111831665039,
        "model.layers.1.mlp.down_proj": 3.8837051391601562,
        "model.layers.1.mlp": 3.8837051391601562,
        "model.layers.1.post_feedforward_layernorm": 17.14151954650879,
        "model.layers.1": 20.52362632751465,
        "model.layers.2.input_layernorm": 21.854612350463867,
        "model.layers.2.post_attention_layernorm": 15.74189567565918,
        "model.layers.2.pre_feedforward_layernorm": 32.959991455078125
      }
    },
    {
      "sequence": "DESCRIBE THE COMPLETE LIFECYCLE OF A STAR FROM FORMATION TO DEATH, INCLUDING ALL STAGES AND PHYSICAL PROCESSES INVOLVED.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 16.099032816679582,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.163751602172852,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 13.935863494873047,
        "model.layers.0.pre_feedforward_layernorm": 24.371824264526367,
        "model.layers.0.mlp.gate_proj": 32.531795501708984,
        "model.layers.0.mlp.up_proj": 15.156085968017578,
        "model.layers.0.mlp.down_proj": 2.8493010997772217,
        "model.layers.0.mlp": 2.8493010997772217,
        "model.layers.0.post_feedforward_layernorm": 10.556060791015625,
        "model.layers.0": 13.753552436828613,
        "model.layers.1.input_layernorm": 16.516923904418945,
        "model.layers.1.post_attention_layernorm": 22.53040885925293,
        "model.layers.1.pre_feedforward_layernorm": 35.00393295288086,
        "model.layers.1.mlp.gate_proj": 38.334815979003906,
        "model.layers.1.mlp.up_proj": 20.05653190612793,
        "model.layers.1.mlp.down_proj": 3.2887632846832275,
        "model.layers.1.mlp": 3.2887632846832275,
        "model.layers.1.post_feedforward_layernorm": 26.088319778442383,
        "model.layers.1": 19.67306137084961,
        "model.layers.2.input_layernorm": 20.699783325195312,
        "model.layers.2.post_attention_layernorm": 12.028133392333984,
        "model.layers.2.pre_feedforward_layernorm": 28.600780487060547
      }
    },
    {
      "sequence": "Prove the Pythagorean theorem. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 16.110637167225715,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 17.726154327392578,
        "model.layers.0.pre_feedforward_layernorm": 24.258058547973633,
        "model.layers.0.mlp.gate_proj": 30.419967651367188,
        "model.layers.0.mlp.up_proj": 13.284509658813477,
        "model.layers.0.mlp.down_proj": 2.7168242931365967,
        "model.layers.0.mlp": 2.7168242931365967,
        "model.layers.0.post_feedforward_layernorm": 9.844810485839844,
        "model.layers.0": 17.964792251586914,
        "model.layers.1.input_layernorm": 16.139888763427734,
        "model.layers.1.post_attention_layernorm": 15.414645195007324,
        "model.layers.1.pre_feedforward_layernorm": 32.877777099609375,
        "model.layers.1.mlp.gate_proj": 37.57775115966797,
        "model.layers.1.mlp.up_proj": 18.642108917236328,
        "model.layers.1.mlp.down_proj": 4.059239864349365,
        "model.layers.1.mlp": 4.059239864349365,
        "model.layers.1.post_feedforward_layernorm": 15.411083221435547,
        "model.layers.1": 22.60839080810547,
        "model.layers.2.input_layernorm": 21.734111785888672,
        "model.layers.2.post_attention_layernorm": 18.805511474609375,
        "model.layers.2.pre_feedforward_layernorm": 36.652313232421875
      }
    },
    {
      "sequence": "Explain database normalization. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 16.138186827949855,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.948148727416992,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 20.195829391479492,
        "model.layers.0.pre_feedforward_layernorm": 25.838321685791016,
        "model.layers.0.mlp.gate_proj": 34.85791015625,
        "model.layers.0.mlp.up_proj": 14.138153076171875,
        "model.layers.0.mlp.down_proj": 2.8492238521575928,
        "model.layers.0.mlp": 2.8492238521575928,
        "model.layers.0.post_feedforward_layernorm": 9.937431335449219,
        "model.layers.0": 19.14179229736328,
        "model.layers.1.input_layernorm": 16.538410186767578,
        "model.layers.1.post_attention_layernorm": 14.430120468139648,
        "model.layers.1.pre_feedforward_layernorm": 32.39111328125,
        "model.layers.1.mlp.gate_proj": 36.81718826293945,
        "model.layers.1.mlp.up_proj": 18.299161911010742,
        "model.layers.1.mlp.down_proj": 3.9758193492889404,
        "model.layers.1.mlp": 3.9758193492889404,
        "model.layers.1.post_feedforward_layernorm": 15.905545234680176,
        "model.layers.1": 20.971858978271484,
        "model.layers.2.input_layernorm": 22.04010009765625,
        "model.layers.2.post_attention_layernorm": 15.85770034790039,
        "model.layers.2.pre_feedforward_layernorm": 32.219425201416016
      }
    },
    {
      "sequence": "Describe the water cycle. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 16.17510642176089,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 22.142457962036133,
        "model.layers.0.pre_feedforward_layernorm": 27.281402587890625,
        "model.layers.0.mlp.gate_proj": 35.369693756103516,
        "model.layers.0.mlp.up_proj": 14.834604263305664,
        "model.layers.0.mlp.down_proj": 2.6747374534606934,
        "model.layers.0.mlp": 2.6747374534606934,
        "model.layers.0.post_feedforward_layernorm": 10.05388069152832,
        "model.layers.0": 18.64790153503418,
        "model.layers.1.input_layernorm": 16.366199493408203,
        "model.layers.1.post_attention_layernorm": 14.288721084594727,
        "model.layers.1.pre_feedforward_layernorm": 31.956764221191406,
        "model.layers.1.mlp.gate_proj": 36.2092170715332,
        "model.layers.1.mlp.up_proj": 18.203357696533203,
        "model.layers.1.mlp.down_proj": 3.1377618312835693,
        "model.layers.1.mlp": 3.1377618312835693,
        "model.layers.1.post_feedforward_layernorm": 13.920633316040039,
        "model.layers.1": 21.428659439086914,
        "model.layers.2.input_layernorm": 22.401039123535156,
        "model.layers.2.post_attention_layernorm": 16.471384048461914,
        "model.layers.2.pre_feedforward_layernorm": 33.19588088989258
      }
    },
    {
      "sequence": "Describe the process of machine learning. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 16.272010347117547,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.852492332458496,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 20.598073959350586,
        "model.layers.0.pre_feedforward_layernorm": 25.87141990661621,
        "model.layers.0.mlp.gate_proj": 32.130435943603516,
        "model.layers.0.mlp.up_proj": 14.07802963256836,
        "model.layers.0.mlp.down_proj": 2.734302043914795,
        "model.layers.0.mlp": 2.734302043914795,
        "model.layers.0.post_feedforward_layernorm": 10.186468124389648,
        "model.layers.0": 18.32942771911621,
        "model.layers.1.input_layernorm": 16.783849716186523,
        "model.layers.1.post_attention_layernorm": 15.468883514404297,
        "model.layers.1.pre_feedforward_layernorm": 34.33804702758789,
        "model.layers.1.mlp.gate_proj": 37.451988220214844,
        "model.layers.1.mlp.up_proj": 19.435400009155273,
        "model.layers.1.mlp.down_proj": 3.0244226455688477,
        "model.layers.1.mlp": 3.0244226455688477,
        "model.layers.1.post_feedforward_layernorm": 13.751608848571777,
        "model.layers.1": 22.327695846557617,
        "model.layers.2.input_layernorm": 22.569013595581055,
        "model.layers.2.post_attention_layernorm": 16.914674758911133,
        "model.layers.2.pre_feedforward_layernorm": 34.65127944946289
      }
    },
    {
      "sequence": "Explain quantum computing principles. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 16.30513556107231,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.352445602416992,
        "model.layers.0.pre_feedforward_layernorm": 24.608610153198242,
        "model.layers.0.mlp.gate_proj": 31.57430076599121,
        "model.layers.0.mlp.up_proj": 13.834527015686035,
        "model.layers.0.mlp.down_proj": 2.6704659461975098,
        "model.layers.0.mlp": 2.6704659461975098,
        "model.layers.0.post_feedforward_layernorm": 9.298135757446289,
        "model.layers.0": 18.444501876831055,
        "model.layers.1.input_layernorm": 16.484909057617188,
        "model.layers.1.post_attention_layernorm": 15.506366729736328,
        "model.layers.1.pre_feedforward_layernorm": 34.74674606323242,
        "model.layers.1.mlp.gate_proj": 39.684505462646484,
        "model.layers.1.mlp.up_proj": 18.726280212402344,
        "model.layers.1.mlp.down_proj": 3.806858777999878,
        "model.layers.1.mlp": 3.806858777999878,
        "model.layers.1.post_feedforward_layernorm": 15.240537643432617,
        "model.layers.1": 22.308713912963867,
        "model.layers.2.input_layernorm": 21.963754653930664,
        "model.layers.2.post_attention_layernorm": 17.32366180419922,
        "model.layers.2.pre_feedforward_layernorm": 35.33481979370117
      }
    },
    {
      "sequence": "EXPLAIN HOW NEURAL NETWORKS WORK.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 16.44468230786531,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.706790447235107,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.437054634094238,
        "model.layers.0.pre_feedforward_layernorm": 27.683866500854492,
        "model.layers.0.mlp.gate_proj": 36.64818572998047,
        "model.layers.0.mlp.up_proj": 16.58289909362793,
        "model.layers.0.mlp.down_proj": 3.1173131465911865,
        "model.layers.0.mlp": 3.1173131465911865,
        "model.layers.0.post_feedforward_layernorm": 12.995655059814453,
        "model.layers.0": 16.668262481689453,
        "model.layers.1.input_layernorm": 19.13849639892578,
        "model.layers.1.post_attention_layernorm": 16.883331298828125,
        "model.layers.1.pre_feedforward_layernorm": 34.63231658935547,
        "model.layers.1.mlp.gate_proj": 39.98890686035156,
        "model.layers.1.mlp.up_proj": 18.554006576538086,
        "model.layers.1.mlp.down_proj": 2.364603281021118,
        "model.layers.1.mlp": 2.364603281021118,
        "model.layers.1.post_feedforward_layernorm": 18.27931785583496,
        "model.layers.1": 20.192367553710938,
        "model.layers.2.input_layernorm": 21.88102912902832,
        "model.layers.2.post_attention_layernorm": 15.863381385803223,
        "model.layers.2.pre_feedforward_layernorm": 29.127992630004883
      }
    },
    {
      "sequence": "Explain the Renaissance period. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 16.53687929070514,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.54103660583496,
        "model.layers.0.pre_feedforward_layernorm": 25.81422996520996,
        "model.layers.0.mlp.gate_proj": 31.862598419189453,
        "model.layers.0.mlp.up_proj": 14.515541076660156,
        "model.layers.0.mlp.down_proj": 2.6037139892578125,
        "model.layers.0.mlp": 2.6037139892578125,
        "model.layers.0.post_feedforward_layernorm": 9.634164810180664,
        "model.layers.0": 17.10152244567871,
        "model.layers.1.input_layernorm": 16.479087829589844,
        "model.layers.1.post_attention_layernorm": 16.97538948059082,
        "model.layers.1.pre_feedforward_layernorm": 36.15793228149414,
        "model.layers.1.mlp.gate_proj": 42.24403381347656,
        "model.layers.1.mlp.up_proj": 19.81415367126465,
        "model.layers.1.mlp.down_proj": 3.4645397663116455,
        "model.layers.1.mlp": 3.4645397663116455,
        "model.layers.1.post_feedforward_layernorm": 16.377485275268555,
        "model.layers.1": 20.745628356933594,
        "model.layers.2.input_layernorm": 22.44233512878418,
        "model.layers.2.post_attention_layernorm": 16.112424850463867,
        "model.layers.2.pre_feedforward_layernorm": 34.76350021362305
      }
    },
    {
      "sequence": "Describe  the  complete  lifecycle  of  a  star  from  formation  to  death,  including  all  stages  and  physical  processes  involved.",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 16.777764382569686,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 9.929709434509277,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.544868469238281,
        "model.layers.0.pre_feedforward_layernorm": 30.17801856994629,
        "model.layers.0.mlp.gate_proj": 33.555904388427734,
        "model.layers.0.mlp.up_proj": 14.368927955627441,
        "model.layers.0.mlp.down_proj": 3.6281771659851074,
        "model.layers.0.mlp": 3.6281771659851074,
        "model.layers.0.post_feedforward_layernorm": 14.146578788757324,
        "model.layers.0": 18.40919303894043,
        "model.layers.1.input_layernorm": 20.253936767578125,
        "model.layers.1.post_attention_layernorm": 18.631675720214844,
        "model.layers.1.pre_feedforward_layernorm": 34.1505012512207,
        "model.layers.1.mlp.gate_proj": 38.33794403076172,
        "model.layers.1.mlp.up_proj": 20.867961883544922,
        "model.layers.1.mlp.down_proj": 2.9289767742156982,
        "model.layers.1.mlp": 2.9289767742156982,
        "model.layers.1.post_feedforward_layernorm": 21.71727180480957,
        "model.layers.1": 22.199560165405273,
        "model.layers.2.input_layernorm": 23.51766586303711,
        "model.layers.2.post_attention_layernorm": 10.822981834411621,
        "model.layers.2.pre_feedforward_layernorm": 26.141572952270508
      }
    },
    {
      "sequence": "HOW DO VACCINES WORK?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 16.9693983119467,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 20.574594497680664,
        "model.layers.0.pre_feedforward_layernorm": 35.43891143798828,
        "model.layers.0.mlp.gate_proj": 42.03996658325195,
        "model.layers.0.mlp.up_proj": 19.177627563476562,
        "model.layers.0.mlp.down_proj": 3.217090606689453,
        "model.layers.0.mlp": 3.217090606689453,
        "model.layers.0.post_feedforward_layernorm": 15.635354995727539,
        "model.layers.0": 20.198793411254883,
        "model.layers.1.input_layernorm": 22.45382308959961,
        "model.layers.1.post_attention_layernorm": 15.87337875366211,
        "model.layers.1.pre_feedforward_layernorm": 34.83173751831055,
        "model.layers.1.mlp.gate_proj": 35.709869384765625,
        "model.layers.1.mlp.up_proj": 18.90528106689453,
        "model.layers.1.mlp.down_proj": 2.170874834060669,
        "model.layers.1.mlp": 2.170874834060669,
        "model.layers.1.post_feedforward_layernorm": 16.269481658935547,
        "model.layers.1": 21.546600341796875,
        "model.layers.2.input_layernorm": 21.175052642822266,
        "model.layers.2.post_attention_layernorm": 11.625185012817383,
        "model.layers.2.pre_feedforward_layernorm": 22.986433029174805
      }
    },
    {
      "sequence": "Describe ancient Egyptian civilization. Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 17.023192384968635,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.244762420654297,
        "model.layers.0.pre_feedforward_layernorm": 25.14641761779785,
        "model.layers.0.mlp.gate_proj": 30.541976928710938,
        "model.layers.0.mlp.up_proj": 14.13105583190918,
        "model.layers.0.mlp.down_proj": 2.5031442642211914,
        "model.layers.0.mlp": 2.5031442642211914,
        "model.layers.0.post_feedforward_layernorm": 9.03872013092041,
        "model.layers.0": 17.495107650756836,
        "model.layers.1.input_layernorm": 16.633405685424805,
        "model.layers.1.post_attention_layernorm": 17.48964500427246,
        "model.layers.1.pre_feedforward_layernorm": 39.10517120361328,
        "model.layers.1.mlp.gate_proj": 46.096622467041016,
        "model.layers.1.mlp.up_proj": 20.770645141601562,
        "model.layers.1.mlp.down_proj": 3.7450194358825684,
        "model.layers.1.mlp": 3.7450194358825684,
        "model.layers.1.post_feedforward_layernorm": 17.229610443115234,
        "model.layers.1": 22.91119956970215,
        "model.layers.2.input_layernorm": 23.228347778320312,
        "model.layers.2.post_attention_layernorm": 16.81259536743164,
        "model.layers.2.pre_feedforward_layernorm": 35.53116226196289
      }
    },
    {
      "sequence": "EXPLAIN THE RENAISSANCE PERIOD.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 17.16491342627484,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 15.049653053283691,
        "model.layers.0.pre_feedforward_layernorm": 26.354646682739258,
        "model.layers.0.mlp.gate_proj": 34.37834930419922,
        "model.layers.0.mlp.up_proj": 15.779582977294922,
        "model.layers.0.mlp.down_proj": 3.1459248065948486,
        "model.layers.0.mlp": 3.1459248065948486,
        "model.layers.0.post_feedforward_layernorm": 13.481476783752441,
        "model.layers.0": 17.14866828918457,
        "model.layers.1.input_layernorm": 18.637601852416992,
        "model.layers.1.post_attention_layernorm": 21.000417709350586,
        "model.layers.1.pre_feedforward_layernorm": 35.851966857910156,
        "model.layers.1.mlp.gate_proj": 41.853599548339844,
        "model.layers.1.mlp.up_proj": 20.830102920532227,
        "model.layers.1.mlp.down_proj": 2.9363017082214355,
        "model.layers.1.mlp": 2.9363017082214355,
        "model.layers.1.post_feedforward_layernorm": 21.02849769592285,
        "model.layers.1": 21.685314178466797,
        "model.layers.2.input_layernorm": 23.728652954101562,
        "model.layers.2.post_attention_layernorm": 15.597633361816406,
        "model.layers.2.pre_feedforward_layernorm": 32.591739654541016
      }
    },
    {
      "sequence": "PROVE THE PYTHAGOREAN THEOREM.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 17.98487373020338,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.226203918457031,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 17.008991241455078,
        "model.layers.0.pre_feedforward_layernorm": 32.59603500366211,
        "model.layers.0.mlp.gate_proj": 39.995975494384766,
        "model.layers.0.mlp.up_proj": 18.459157943725586,
        "model.layers.0.mlp.down_proj": 3.823434829711914,
        "model.layers.0.mlp": 3.823434829711914,
        "model.layers.0.post_feedforward_layernorm": 14.827548027038574,
        "model.layers.0": 19.72724151611328,
        "model.layers.1.input_layernorm": 21.29437255859375,
        "model.layers.1.post_attention_layernorm": 18.793790817260742,
        "model.layers.1.pre_feedforward_layernorm": 35.96728515625,
        "model.layers.1.mlp.gate_proj": 38.87550354003906,
        "model.layers.1.mlp.up_proj": 19.58998680114746,
        "model.layers.1.mlp.down_proj": 2.7346253395080566,
        "model.layers.1.mlp": 2.7346253395080566,
        "model.layers.1.post_feedforward_layernorm": 19.774538040161133,
        "model.layers.1": 22.76709747314453,
        "model.layers.2.input_layernorm": 23.66510772705078,
        "model.layers.2.post_attention_layernorm": 17.968862533569336,
        "model.layers.2.pre_feedforward_layernorm": 30.99827766418457
      }
    },
    {
      "sequence": "WHAT IS CONSCIOUSNESS?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 18.02471169181492,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.152703285217285,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 21.850513458251953,
        "model.layers.0.pre_feedforward_layernorm": 38.4404411315918,
        "model.layers.0.mlp.gate_proj": 41.638328552246094,
        "model.layers.0.mlp.up_proj": 19.74923324584961,
        "model.layers.0.mlp.down_proj": 3.307803153991699,
        "model.layers.0.mlp": 3.307803153991699,
        "model.layers.0.post_feedforward_layernorm": 15.323773384094238,
        "model.layers.0": 20.688682556152344,
        "model.layers.1.input_layernorm": 23.442964553833008,
        "model.layers.1.post_attention_layernorm": 17.146347045898438,
        "model.layers.1.pre_feedforward_layernorm": 35.981727600097656,
        "model.layers.1.mlp.gate_proj": 38.11469268798828,
        "model.layers.1.mlp.up_proj": 19.673381805419922,
        "model.layers.1.mlp.down_proj": 2.31117582321167,
        "model.layers.1.mlp": 2.31117582321167,
        "model.layers.1.post_feedforward_layernorm": 16.75047492980957,
        "model.layers.1": 23.378026962280273,
        "model.layers.2.input_layernorm": 23.50341796875,
        "model.layers.2.post_attention_layernorm": 14.649275779724121,
        "model.layers.2.pre_feedforward_layernorm": 25.846426010131836
      }
    },
    {
      "sequence": "HOW DO YOU SOLVE QUADRATIC EQUATIONS?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 18.041829855545707,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 7.852492332458496,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 20.48328971862793,
        "model.layers.0.pre_feedforward_layernorm": 35.98271942138672,
        "model.layers.0.mlp.gate_proj": 40.846309661865234,
        "model.layers.0.mlp.up_proj": 20.669898986816406,
        "model.layers.0.mlp.down_proj": 3.3131895065307617,
        "model.layers.0.mlp": 3.3131895065307617,
        "model.layers.0.post_feedforward_layernorm": 15.417126655578613,
        "model.layers.0": 20.447769165039062,
        "model.layers.1.input_layernorm": 22.911230087280273,
        "model.layers.1.post_attention_layernorm": 19.179035186767578,
        "model.layers.1.pre_feedforward_layernorm": 35.30571746826172,
        "model.layers.1.mlp.gate_proj": 38.717098236083984,
        "model.layers.1.mlp.up_proj": 19.99678611755371,
        "model.layers.1.mlp.down_proj": 2.488443374633789,
        "model.layers.1.mlp": 2.488443374633789,
        "model.layers.1.post_feedforward_layernorm": 20.019359588623047,
        "model.layers.1": 24.70012855529785,
        "model.layers.2.input_layernorm": 23.03264045715332,
        "model.layers.2.post_attention_layernorm": 13.142561912536621,
        "model.layers.2.pre_feedforward_layernorm": 24.6546573638916
      }
    },
    {
      "sequence": "What  is  the  structure  of  a  sonnet?",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 18.11927907363228,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.373211860656738,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 20.029705047607422,
        "model.layers.0.pre_feedforward_layernorm": 32.565696716308594,
        "model.layers.0.mlp.gate_proj": 33.81330108642578,
        "model.layers.0.mlp.up_proj": 15.481388092041016,
        "model.layers.0.mlp.down_proj": 3.0754547119140625,
        "model.layers.0.mlp": 3.0754547119140625,
        "model.layers.0.post_feedforward_layernorm": 13.901030540466309,
        "model.layers.0": 18.72220802307129,
        "model.layers.1.input_layernorm": 20.935455322265625,
        "model.layers.1.post_attention_layernorm": 21.34933853149414,
        "model.layers.1.pre_feedforward_layernorm": 40.37598419189453,
        "model.layers.1.mlp.gate_proj": 50.31035614013672,
        "model.layers.1.mlp.up_proj": 22.201566696166992,
        "model.layers.1.mlp.down_proj": 2.8764514923095703,
        "model.layers.1.mlp": 2.8764514923095703,
        "model.layers.1.post_feedforward_layernorm": 21.48064422607422,
        "model.layers.1": 24.378366470336914,
        "model.layers.2.input_layernorm": 25.04127311706543,
        "model.layers.2.post_attention_layernorm": 11.093506813049316,
        "model.layers.2.pre_feedforward_layernorm": 24.78657341003418
      }
    },
    {
      "sequence": "WHAT IS THE FIBONACCI SEQUENCE?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 18.17926678450211,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 6.706790447235107,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 19.845491409301758,
        "model.layers.0.pre_feedforward_layernorm": 39.00729751586914,
        "model.layers.0.mlp.gate_proj": 41.71410369873047,
        "model.layers.0.mlp.up_proj": 20.027076721191406,
        "model.layers.0.mlp.down_proj": 3.4978411197662354,
        "model.layers.0.mlp": 3.4978411197662354,
        "model.layers.0.post_feedforward_layernorm": 16.03287124633789,
        "model.layers.0": 20.37350082397461,
        "model.layers.1.input_layernorm": 22.743820190429688,
        "model.layers.1.post_attention_layernorm": 16.593271255493164,
        "model.layers.1.pre_feedforward_layernorm": 37.2320442199707,
        "model.layers.1.mlp.gate_proj": 40.827545166015625,
        "model.layers.1.mlp.up_proj": 20.556793212890625,
        "model.layers.1.mlp.down_proj": 2.5523297786712646,
        "model.layers.1.mlp": 2.5523297786712646,
        "model.layers.1.post_feedforward_layernorm": 15.74818229675293,
        "model.layers.1": 23.81089973449707,
        "model.layers.2.input_layernorm": 23.289207458496094,
        "model.layers.2.post_attention_layernorm": 14.558258056640625,
        "model.layers.2.pre_feedforward_layernorm": 26.95564079284668
      }
    },
    {
      "sequence": "WHAT IS THE STRUCTURE OF A SONNET?",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 19.509102759153947,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 2.8216638565063477,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 22.471851348876953,
        "model.layers.0.pre_feedforward_layernorm": 40.18075942993164,
        "model.layers.0.mlp.gate_proj": 40.5161247253418,
        "model.layers.0.mlp.up_proj": 21.44570541381836,
        "model.layers.0.mlp.down_proj": 3.3887217044830322,
        "model.layers.0.mlp": 3.3887217044830322,
        "model.layers.0.post_feedforward_layernorm": 15.32849407196045,
        "model.layers.0": 20.852642059326172,
        "model.layers.1.input_layernorm": 23.990123748779297,
        "model.layers.1.post_attention_layernorm": 20.004526138305664,
        "model.layers.1.pre_feedforward_layernorm": 41.86146926879883,
        "model.layers.1.mlp.gate_proj": 53.142765045166016,
        "model.layers.1.mlp.up_proj": 22.50144386291504,
        "model.layers.1.mlp.down_proj": 2.7008328437805176,
        "model.layers.1.mlp": 2.7008328437805176,
        "model.layers.1.post_feedforward_layernorm": 20.645872116088867,
        "model.layers.1": 25.23472023010254,
        "model.layers.2.input_layernorm": 24.597640991210938,
        "model.layers.2.post_attention_layernorm": 13.970430374145508,
        "model.layers.2.pre_feedforward_layernorm": 26.964021682739258
      }
    },
    {
      "sequence": "EXPLAIN ETHICAL DILEMMAS.",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 20.202817336372707,
      "layer_variances": {
        "model.embed_tokens": 0.0,
        "model.rotary_emb": 8.290679931640625,
        "model.layers.0.input_layernorm": 0.0,
        "model.layers.0.post_attention_layernorm": 21.630266189575195,
        "model.layers.0.pre_feedforward_layernorm": 35.63075637817383,
        "model.layers.0.mlp.gate_proj": 46.014278411865234,
        "model.layers.0.mlp.up_proj": 21.730026245117188,
        "model.layers.0.mlp.down_proj": 3.695298910140991,
        "model.layers.0.mlp": 3.695298910140991,
        "model.layers.0.post_feedforward_layernorm": 15.8944673538208,
        "model.layers.0": 21.82809829711914,
        "model.layers.1.input_layernorm": 24.50688934326172,
        "model.layers.1.post_attention_layernorm": 21.72777557373047,
        "model.layers.1.pre_feedforward_layernorm": 39.884849548339844,
        "model.layers.1.mlp.gate_proj": 44.98265075683594,
        "model.layers.1.mlp.up_proj": 21.959514617919922,
        "model.layers.1.mlp.down_proj": 3.229034185409546,
        "model.layers.1.mlp": 3.229034185409546,
        "model.layers.1.post_feedforward_layernorm": 19.74370765686035,
        "model.layers.1": 26.22265625,
        "model.layers.2.input_layernorm": 27.891170501708984,
        "model.layers.2.post_attention_layernorm": 17.36273193359375,
        "model.layers.2.pre_feedforward_layernorm": 35.5156135559082
      }
    },
    {
      "sequence": "Repeat  this  word:  hello  hello  hello  hello",
      "variation_type": "whitespace_variation",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 25.488600653150808,
      "layer_variances": {
        "model.embed_tokens": 1.0251470804214478,
        "model.rotary_emb": 8.294113159179688,
        "model.layers.0.input_layernorm": 37.359012603759766,
        "model.layers.0.post_attention_layernorm": 19.283355712890625,
        "model.layers.0.pre_feedforward_layernorm": 44.642662048339844,
        "model.layers.0.mlp.gate_proj": 43.00140380859375,
        "model.layers.0.mlp.up_proj": 30.545438766479492,
        "model.layers.0.mlp.down_proj": 3.8738603591918945,
        "model.layers.0.mlp": 3.8738603591918945,
        "model.layers.0.post_feedforward_layernorm": 26.914016723632812,
        "model.layers.0": 46.95110321044922,
        "model.layers.1.input_layernorm": 44.3869743347168,
        "model.layers.1.post_attention_layernorm": 12.904579162597656,
        "model.layers.1.pre_feedforward_layernorm": 39.06075668334961,
        "model.layers.1.mlp.gate_proj": 33.84968566894531,
        "model.layers.1.mlp.up_proj": 27.518131256103516,
        "model.layers.1.mlp.down_proj": 2.579665422439575,
        "model.layers.1.mlp": 2.579665422439575,
        "model.layers.1.post_feedforward_layernorm": 22.087705612182617,
        "model.layers.1": 45.431331634521484,
        "model.layers.2.input_layernorm": 38.78886795043945,
        "model.layers.2.post_attention_layernorm": 21.901269912719727,
        "model.layers.2.pre_feedforward_layernorm": 29.385208129882812
      }
    },
    {
      "sequence": "Can you help me understand this? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 32.29513033576634,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.852492332458496,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 33.625083923339844,
        "model.layers.0.pre_feedforward_layernorm": 56.30733108520508,
        "model.layers.0.mlp.gate_proj": 58.272438049316406,
        "model.layers.0.mlp.up_proj": 45.4947395324707,
        "model.layers.0.mlp.down_proj": 12.036959648132324,
        "model.layers.0.mlp": 12.036959648132324,
        "model.layers.0.post_feedforward_layernorm": 45.69890594482422,
        "model.layers.0": 50.769569396972656,
        "model.layers.1.input_layernorm": 51.062320709228516,
        "model.layers.1.post_attention_layernorm": 13.181620597839355,
        "model.layers.1.pre_feedforward_layernorm": 49.646156311035156,
        "model.layers.1.mlp.gate_proj": 40.52019119262695,
        "model.layers.1.mlp.up_proj": 37.1993408203125,
        "model.layers.1.mlp.down_proj": 3.2333922386169434,
        "model.layers.1.mlp": 3.2333922386169434,
        "model.layers.1.post_feedforward_layernorm": 19.595212936401367,
        "model.layers.1": 47.31840896606445,
        "model.layers.2.input_layernorm": 44.833560943603516,
        "model.layers.2.post_attention_layernorm": 19.23284149169922,
        "model.layers.2.pre_feedforward_layernorm": 39.01289367675781
      }
    },
    {
      "sequence": "REPEAT THIS WORD: HELLO HELLO HELLO HELLO",
      "variation_type": "uppercase",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 32.30699051981387,
      "layer_variances": {
        "model.embed_tokens": 1.4137449264526367,
        "model.rotary_emb": 0.0,
        "model.layers.0.input_layernorm": 67.60310363769531,
        "model.layers.0.post_attention_layernorm": 19.279048919677734,
        "model.layers.0.pre_feedforward_layernorm": 55.13443374633789,
        "model.layers.0.mlp.gate_proj": 42.96904373168945,
        "model.layers.0.mlp.up_proj": 36.19110870361328,
        "model.layers.0.mlp.down_proj": 4.407529830932617,
        "model.layers.0.mlp": 4.407529830932617,
        "model.layers.0.post_feedforward_layernorm": 36.26377868652344,
        "model.layers.0": 60.67416000366211,
        "model.layers.1.input_layernorm": 57.683937072753906,
        "model.layers.1.post_attention_layernorm": 19.843175888061523,
        "model.layers.1.pre_feedforward_layernorm": 50.254295349121094,
        "model.layers.1.mlp.gate_proj": 42.164024353027344,
        "model.layers.1.mlp.up_proj": 36.289058685302734,
        "model.layers.1.mlp.down_proj": 3.0975120067596436,
        "model.layers.1.mlp": 3.0975120067596436,
        "model.layers.1.post_feedforward_layernorm": 28.337818145751953,
        "model.layers.1": 58.48389434814453,
        "model.layers.2.input_layernorm": 51.986968994140625,
        "model.layers.2.post_attention_layernorm": 24.366783142089844,
        "model.layers.2.pre_feedforward_layernorm": 39.11231994628906
      }
    },
    {
      "sequence": "How did the internet develop? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 33.216088087662406,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 31.822383880615234,
        "model.layers.0.pre_feedforward_layernorm": 58.443267822265625,
        "model.layers.0.mlp.gate_proj": 61.35348129272461,
        "model.layers.0.mlp.up_proj": 46.72642517089844,
        "model.layers.0.mlp.down_proj": 12.56045150756836,
        "model.layers.0.mlp": 12.56045150756836,
        "model.layers.0.post_feedforward_layernorm": 47.3477783203125,
        "model.layers.0": 50.08344268798828,
        "model.layers.1.input_layernorm": 52.067039489746094,
        "model.layers.1.post_attention_layernorm": 15.628937721252441,
        "model.layers.1.pre_feedforward_layernorm": 51.723148345947266,
        "model.layers.1.mlp.gate_proj": 44.948184967041016,
        "model.layers.1.mlp.up_proj": 37.9057731628418,
        "model.layers.1.mlp.down_proj": 3.2219951152801514,
        "model.layers.1.mlp": 3.2219951152801514,
        "model.layers.1.post_feedforward_layernorm": 18.571422576904297,
        "model.layers.1": 46.053993225097656,
        "model.layers.2.input_layernorm": 45.64548873901367,
        "model.layers.2.post_attention_layernorm": 20.821758270263672,
        "model.layers.2.pre_feedforward_layernorm": 42.714622497558594
      }
    },
    {
      "sequence": "How do we define truth? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 33.24631344753763,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 30.032299041748047,
        "model.layers.0.pre_feedforward_layernorm": 59.66622543334961,
        "model.layers.0.mlp.gate_proj": 61.38233947753906,
        "model.layers.0.mlp.up_proj": 46.982627868652344,
        "model.layers.0.mlp.down_proj": 13.32330322265625,
        "model.layers.0.mlp": 13.32330322265625,
        "model.layers.0.post_feedforward_layernorm": 48.08199691772461,
        "model.layers.0": 49.73505401611328,
        "model.layers.1.input_layernorm": 51.94719696044922,
        "model.layers.1.post_attention_layernorm": 16.12938690185547,
        "model.layers.1.pre_feedforward_layernorm": 50.523826599121094,
        "model.layers.1.mlp.gate_proj": 43.419273376464844,
        "model.layers.1.mlp.up_proj": 37.4992790222168,
        "model.layers.1.mlp.down_proj": 3.2955050468444824,
        "model.layers.1.mlp": 3.2955050468444824,
        "model.layers.1.post_feedforward_layernorm": 20.224822998046875,
        "model.layers.1": 46.094879150390625,
        "model.layers.2.input_layernorm": 44.84230041503906,
        "model.layers.2.post_attention_layernorm": 21.476581573486328,
        "model.layers.2.pre_feedforward_layernorm": 42.84151840209961
      }
    },
    {
      "sequence": "How do I change a tire? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 33.48611659589021,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.852492332458496,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 30.963064193725586,
        "model.layers.0.pre_feedforward_layernorm": 60.579673767089844,
        "model.layers.0.mlp.gate_proj": 64.02893829345703,
        "model.layers.0.mlp.up_proj": 47.998714447021484,
        "model.layers.0.mlp.down_proj": 13.331181526184082,
        "model.layers.0.mlp": 13.331181526184082,
        "model.layers.0.post_feedforward_layernorm": 47.48939514160156,
        "model.layers.0": 50.47378921508789,
        "model.layers.1.input_layernorm": 52.04148864746094,
        "model.layers.1.post_attention_layernorm": 14.489280700683594,
        "model.layers.1.pre_feedforward_layernorm": 51.69756317138672,
        "model.layers.1.mlp.gate_proj": 44.58757019042969,
        "model.layers.1.mlp.up_proj": 38.22996520996094,
        "model.layers.1.mlp.down_proj": 3.2229247093200684,
        "model.layers.1.mlp": 3.2229247093200684,
        "model.layers.1.post_feedforward_layernorm": 17.913742065429688,
        "model.layers.1": 46.8878173828125,
        "model.layers.2.input_layernorm": 46.3541145324707,
        "model.layers.2.post_attention_layernorm": 20.49412727355957,
        "model.layers.2.pre_feedforward_layernorm": 42.366546630859375
      }
    },
    {
      "sequence": "What are the pros and cons? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 33.49651276546976,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.852492332458496,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 30.246089935302734,
        "model.layers.0.pre_feedforward_layernorm": 60.36973571777344,
        "model.layers.0.mlp.gate_proj": 62.351966857910156,
        "model.layers.0.mlp.up_proj": 47.82741928100586,
        "model.layers.0.mlp.down_proj": 12.67324161529541,
        "model.layers.0.mlp": 12.67324161529541,
        "model.layers.0.post_feedforward_layernorm": 46.631221771240234,
        "model.layers.0": 50.74285125732422,
        "model.layers.1.input_layernorm": 52.99245071411133,
        "model.layers.1.post_attention_layernorm": 15.76730728149414,
        "model.layers.1.pre_feedforward_layernorm": 52.5124626159668,
        "model.layers.1.mlp.gate_proj": 45.779457092285156,
        "model.layers.1.mlp.up_proj": 38.332923889160156,
        "model.layers.1.mlp.down_proj": 3.2578163146972656,
        "model.layers.1.mlp": 3.2578163146972656,
        "model.layers.1.post_feedforward_layernorm": 18.381805419921875,
        "model.layers.1": 46.880897521972656,
        "model.layers.2.input_layernorm": 45.59242248535156,
        "model.layers.2.post_attention_layernorm": 21.578161239624023,
        "model.layers.2.pre_feedforward_layernorm": 42.09382629394531
      }
    },
    {
      "sequence": "How do you solve quadratic equations? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 33.52257371985394,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.852492332458496,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 26.717527389526367,
        "model.layers.0.pre_feedforward_layernorm": 62.54762268066406,
        "model.layers.0.mlp.gate_proj": 62.46416473388672,
        "model.layers.0.mlp.up_proj": 48.90887451171875,
        "model.layers.0.mlp.down_proj": 13.92979621887207,
        "model.layers.0.mlp": 13.92979621887207,
        "model.layers.0.post_feedforward_layernorm": 47.308677673339844,
        "model.layers.0": 50.604515075683594,
        "model.layers.1.input_layernorm": 52.29570770263672,
        "model.layers.1.post_attention_layernorm": 14.978955268859863,
        "model.layers.1.pre_feedforward_layernorm": 51.050045013427734,
        "model.layers.1.mlp.gate_proj": 43.52396774291992,
        "model.layers.1.mlp.up_proj": 37.810707092285156,
        "model.layers.1.mlp.down_proj": 3.659503698348999,
        "model.layers.1.mlp": 3.659503698348999,
        "model.layers.1.post_feedforward_layernorm": 19.859256744384766,
        "model.layers.1": 47.65997314453125,
        "model.layers.2.input_layernorm": 45.47924041748047,
        "model.layers.2.post_attention_layernorm": 20.956893920898438,
        "model.layers.2.pre_feedforward_layernorm": 43.19778823852539
      }
    },
    {
      "sequence": "How does exercise affect health? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 33.735090152077056,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 31.149486541748047,
        "model.layers.0.pre_feedforward_layernorm": 61.46453857421875,
        "model.layers.0.mlp.gate_proj": 64.67020416259766,
        "model.layers.0.mlp.up_proj": 48.065757751464844,
        "model.layers.0.mlp.down_proj": 13.45152473449707,
        "model.layers.0.mlp": 13.45152473449707,
        "model.layers.0.post_feedforward_layernorm": 48.14745330810547,
        "model.layers.0": 50.920127868652344,
        "model.layers.1.input_layernorm": 52.08641815185547,
        "model.layers.1.post_attention_layernorm": 16.02764892578125,
        "model.layers.1.pre_feedforward_layernorm": 50.537872314453125,
        "model.layers.1.mlp.gate_proj": 44.81325149536133,
        "model.layers.1.mlp.up_proj": 36.76827621459961,
        "model.layers.1.mlp.down_proj": 3.2599802017211914,
        "model.layers.1.mlp": 3.2599802017211914,
        "model.layers.1.post_feedforward_layernorm": 18.38237762451172,
        "model.layers.1": 47.35196304321289,
        "model.layers.2.input_layernorm": 46.19387435913086,
        "model.layers.2.post_attention_layernorm": 21.667757034301758,
        "model.layers.2.pre_feedforward_layernorm": 43.68907165527344
      }
    },
    {
      "sequence": "How does photosynthesis work? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 33.83366045744523,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 28.613361358642578,
        "model.layers.0.pre_feedforward_layernorm": 61.846893310546875,
        "model.layers.0.mlp.gate_proj": 66.04745483398438,
        "model.layers.0.mlp.up_proj": 48.39374542236328,
        "model.layers.0.mlp.down_proj": 13.50663948059082,
        "model.layers.0.mlp": 13.50663948059082,
        "model.layers.0.post_feedforward_layernorm": 47.302616119384766,
        "model.layers.0": 50.684207916259766,
        "model.layers.1.input_layernorm": 52.5099983215332,
        "model.layers.1.post_attention_layernorm": 16.59735679626465,
        "model.layers.1.pre_feedforward_layernorm": 50.618934631347656,
        "model.layers.1.mlp.gate_proj": 44.29216384887695,
        "model.layers.1.mlp.up_proj": 37.38057327270508,
        "model.layers.1.mlp.down_proj": 3.179251194000244,
        "model.layers.1.mlp": 3.179251194000244,
        "model.layers.1.post_feedforward_layernorm": 19.918718338012695,
        "model.layers.1": 47.5842399597168,
        "model.layers.2.input_layernorm": 46.59809494018555,
        "model.layers.2.post_attention_layernorm": 21.32163429260254,
        "model.layers.2.pre_feedforward_layernorm": 44.83757781982422
      }
    },
    {
      "sequence": "How does music theory work? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 33.88792560411536,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 30.845550537109375,
        "model.layers.0.pre_feedforward_layernorm": 60.61555099487305,
        "model.layers.0.mlp.gate_proj": 63.49385070800781,
        "model.layers.0.mlp.up_proj": 47.46393966674805,
        "model.layers.0.mlp.down_proj": 13.15590763092041,
        "model.layers.0.mlp": 13.15590763092041,
        "model.layers.0.post_feedforward_layernorm": 47.749412536621094,
        "model.layers.0": 50.62624740600586,
        "model.layers.1.input_layernorm": 52.128639221191406,
        "model.layers.1.post_attention_layernorm": 17.694849014282227,
        "model.layers.1.pre_feedforward_layernorm": 52.92707061767578,
        "model.layers.1.mlp.gate_proj": 46.40559387207031,
        "model.layers.1.mlp.up_proj": 37.976070404052734,
        "model.layers.1.mlp.down_proj": 3.3705599308013916,
        "model.layers.1.mlp": 3.3705599308013916,
        "model.layers.1.post_feedforward_layernorm": 19.511648178100586,
        "model.layers.1": 47.220191955566406,
        "model.layers.2.input_layernorm": 45.839866638183594,
        "model.layers.2.post_attention_layernorm": 21.261871337890625,
        "model.layers.2.pre_feedforward_layernorm": 44.06101608276367
      }
    },
    {
      "sequence": "What is the meaning of life? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 34.116267618925676,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.852492332458496,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 30.406658172607422,
        "model.layers.0.pre_feedforward_layernorm": 62.51041030883789,
        "model.layers.0.mlp.gate_proj": 64.33340454101562,
        "model.layers.0.mlp.up_proj": 48.174468994140625,
        "model.layers.0.mlp.down_proj": 13.394279479980469,
        "model.layers.0.mlp": 13.394279479980469,
        "model.layers.0.post_feedforward_layernorm": 48.13425827026367,
        "model.layers.0": 50.41618728637695,
        "model.layers.1.input_layernorm": 52.45523452758789,
        "model.layers.1.post_attention_layernorm": 15.421871185302734,
        "model.layers.1.pre_feedforward_layernorm": 54.7293815612793,
        "model.layers.1.mlp.gate_proj": 48.18248748779297,
        "model.layers.1.mlp.up_proj": 39.18048858642578,
        "model.layers.1.mlp.down_proj": 3.2491166591644287,
        "model.layers.1.mlp": 3.2491166591644287,
        "model.layers.1.post_feedforward_layernorm": 18.752546310424805,
        "model.layers.1": 46.387359619140625,
        "model.layers.2.input_layernorm": 46.85090637207031,
        "model.layers.2.post_attention_layernorm": 20.52057647705078,
        "model.layers.2.pre_feedforward_layernorm": 44.454444885253906
      }
    },
    {
      "sequence": "What are investment strategies? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 34.139316330785334,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 29.09042739868164,
        "model.layers.0.pre_feedforward_layernorm": 63.654319763183594,
        "model.layers.0.mlp.gate_proj": 65.54008483886719,
        "model.layers.0.mlp.up_proj": 48.964115142822266,
        "model.layers.0.mlp.down_proj": 13.79117202758789,
        "model.layers.0.mlp": 13.79117202758789,
        "model.layers.0.post_feedforward_layernorm": 48.68260955810547,
        "model.layers.0": 51.398414611816406,
        "model.layers.1.input_layernorm": 52.494693756103516,
        "model.layers.1.post_attention_layernorm": 15.627392768859863,
        "model.layers.1.pre_feedforward_layernorm": 51.93624496459961,
        "model.layers.1.mlp.gate_proj": 45.71941375732422,
        "model.layers.1.mlp.up_proj": 37.19386291503906,
        "model.layers.1.mlp.down_proj": 3.369854211807251,
        "model.layers.1.mlp": 3.369854211807251,
        "model.layers.1.post_feedforward_layernorm": 18.63007164001465,
        "model.layers.1": 47.77900314331055,
        "model.layers.2.input_layernorm": 46.54247283935547,
        "model.layers.2.post_attention_layernorm": 23.601713180541992,
        "model.layers.2.pre_feedforward_layernorm": 43.772544860839844
      }
    },
    {
      "sequence": "How do transformers process language? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 34.210602179817535,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 29.43743133544922,
        "model.layers.0.pre_feedforward_layernorm": 62.976837158203125,
        "model.layers.0.mlp.gate_proj": 66.27818298339844,
        "model.layers.0.mlp.up_proj": 48.78339385986328,
        "model.layers.0.mlp.down_proj": 13.280129432678223,
        "model.layers.0.mlp": 13.280129432678223,
        "model.layers.0.post_feedforward_layernorm": 47.396480560302734,
        "model.layers.0": 51.004432678222656,
        "model.layers.1.input_layernorm": 53.11955261230469,
        "model.layers.1.post_attention_layernorm": 17.31104278564453,
        "model.layers.1.pre_feedforward_layernorm": 52.25096130371094,
        "model.layers.1.mlp.gate_proj": 45.29899597167969,
        "model.layers.1.mlp.up_proj": 38.412811279296875,
        "model.layers.1.mlp.down_proj": 3.387817144393921,
        "model.layers.1.mlp": 3.387817144393921,
        "model.layers.1.post_feedforward_layernorm": 19.751691818237305,
        "model.layers.1": 48.00537109375,
        "model.layers.2.input_layernorm": 45.91585922241211,
        "model.layers.2.post_attention_layernorm": 22.05912971496582,
        "model.layers.2.pre_feedforward_layernorm": 44.95779800415039
      }
    },
    {
      "sequence": "What causes climate change? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 34.22729734752489,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 30.213363647460938,
        "model.layers.0.pre_feedforward_layernorm": 62.79899978637695,
        "model.layers.0.mlp.gate_proj": 65.8679428100586,
        "model.layers.0.mlp.up_proj": 48.58066177368164,
        "model.layers.0.mlp.down_proj": 13.598313331604004,
        "model.layers.0.mlp": 13.598313331604004,
        "model.layers.0.post_feedforward_layernorm": 48.256996154785156,
        "model.layers.0": 51.2706413269043,
        "model.layers.1.input_layernorm": 53.28112030029297,
        "model.layers.1.post_attention_layernorm": 17.424823760986328,
        "model.layers.1.pre_feedforward_layernorm": 52.45661163330078,
        "model.layers.1.mlp.gate_proj": 47.461524963378906,
        "model.layers.1.mlp.up_proj": 37.451087951660156,
        "model.layers.1.mlp.down_proj": 3.3603742122650146,
        "model.layers.1.mlp": 3.3603742122650146,
        "model.layers.1.post_feedforward_layernorm": 19.704126358032227,
        "model.layers.1": 47.35933303833008,
        "model.layers.2.input_layernorm": 46.376956939697266,
        "model.layers.2.post_attention_layernorm": 20.247865676879883,
        "model.layers.2.pre_feedforward_layernorm": 44.30356979370117
      }
    },
    {
      "sequence": "What are the applications of deep learning? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 34.231534377388336,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.646179676055908,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 30.486726760864258,
        "model.layers.0.pre_feedforward_layernorm": 61.2779655456543,
        "model.layers.0.mlp.gate_proj": 64.67381286621094,
        "model.layers.0.mlp.up_proj": 48.00764846801758,
        "model.layers.0.mlp.down_proj": 12.853907585144043,
        "model.layers.0.mlp": 12.853907585144043,
        "model.layers.0.post_feedforward_layernorm": 46.98042678833008,
        "model.layers.0": 50.9862174987793,
        "model.layers.1.input_layernorm": 53.0790901184082,
        "model.layers.1.post_attention_layernorm": 16.477615356445312,
        "model.layers.1.pre_feedforward_layernorm": 54.70777893066406,
        "model.layers.1.mlp.gate_proj": 47.10102462768555,
        "model.layers.1.mlp.up_proj": 39.510276794433594,
        "model.layers.1.mlp.down_proj": 3.2954344749450684,
        "model.layers.1.mlp": 3.2954344749450684,
        "model.layers.1.post_feedforward_layernorm": 20.068254470825195,
        "model.layers.1": 48.05956268310547,
        "model.layers.2.input_layernorm": 46.31190490722656,
        "model.layers.2.post_attention_layernorm": 22.044822692871094,
        "model.layers.2.pre_feedforward_layernorm": 44.98311233520508
      }
    },
    {
      "sequence": "How do vaccines work? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 34.29503376587577,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 29.168426513671875,
        "model.layers.0.pre_feedforward_layernorm": 62.25400161743164,
        "model.layers.0.mlp.gate_proj": 64.59147644042969,
        "model.layers.0.mlp.up_proj": 48.68759536743164,
        "model.layers.0.mlp.down_proj": 13.399019241333008,
        "model.layers.0.mlp": 13.399019241333008,
        "model.layers.0.post_feedforward_layernorm": 47.78593063354492,
        "model.layers.0": 50.674991607666016,
        "model.layers.1.input_layernorm": 53.26045227050781,
        "model.layers.1.post_attention_layernorm": 16.9620304107666,
        "model.layers.1.pre_feedforward_layernorm": 54.594398498535156,
        "model.layers.1.mlp.gate_proj": 49.15448760986328,
        "model.layers.1.mlp.up_proj": 39.28061294555664,
        "model.layers.1.mlp.down_proj": 3.305046796798706,
        "model.layers.1.mlp": 3.305046796798706,
        "model.layers.1.post_feedforward_layernorm": 19.547502517700195,
        "model.layers.1": 47.272186279296875,
        "model.layers.2.input_layernorm": 46.753822326660156,
        "model.layers.2.post_attention_layernorm": 20.84105682373047,
        "model.layers.2.pre_feedforward_layernorm": 44.2938346862793
      }
    },
    {
      "sequence": "How does encryption work? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 34.386825354202934,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 29.278528213500977,
        "model.layers.0.pre_feedforward_layernorm": 62.113914489746094,
        "model.layers.0.mlp.gate_proj": 65.69770050048828,
        "model.layers.0.mlp.up_proj": 48.427490234375,
        "model.layers.0.mlp.down_proj": 13.209641456604004,
        "model.layers.0.mlp": 13.209641456604004,
        "model.layers.0.post_feedforward_layernorm": 47.4688720703125,
        "model.layers.0": 51.250370025634766,
        "model.layers.1.input_layernorm": 53.36573791503906,
        "model.layers.1.post_attention_layernorm": 17.48278045654297,
        "model.layers.1.pre_feedforward_layernorm": 53.25455856323242,
        "model.layers.1.mlp.gate_proj": 46.74873733520508,
        "model.layers.1.mlp.up_proj": 39.5091667175293,
        "model.layers.1.mlp.down_proj": 3.5709705352783203,
        "model.layers.1.mlp": 3.5709705352783203,
        "model.layers.1.post_feedforward_layernorm": 19.349414825439453,
        "model.layers.1": 47.72587203979492,
        "model.layers.2.input_layernorm": 47.32604217529297,
        "model.layers.2.post_attention_layernorm": 22.237802505493164,
        "model.layers.2.pre_feedforward_layernorm": 45.84393310546875
      }
    },
    {
      "sequence": "What is artificial intelligence? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 34.75592963591866,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 30.76056671142578,
        "model.layers.0.pre_feedforward_layernorm": 63.18146514892578,
        "model.layers.0.mlp.gate_proj": 67.44850158691406,
        "model.layers.0.mlp.up_proj": 48.92127227783203,
        "model.layers.0.mlp.down_proj": 13.405713081359863,
        "model.layers.0.mlp": 13.405713081359863,
        "model.layers.0.post_feedforward_layernorm": 48.09489059448242,
        "model.layers.0": 51.995853424072266,
        "model.layers.1.input_layernorm": 53.69858169555664,
        "model.layers.1.post_attention_layernorm": 16.289356231689453,
        "model.layers.1.pre_feedforward_layernorm": 54.24610900878906,
        "model.layers.1.mlp.gate_proj": 47.7875862121582,
        "model.layers.1.mlp.up_proj": 38.90886688232422,
        "model.layers.1.mlp.down_proj": 3.4192960262298584,
        "model.layers.1.mlp": 3.4192960262298584,
        "model.layers.1.post_feedforward_layernorm": 18.57961082458496,
        "model.layers.1": 47.518428802490234,
        "model.layers.2.input_layernorm": 47.26268768310547,
        "model.layers.2.post_attention_layernorm": 24.714658737182617,
        "model.layers.2.pre_feedforward_layernorm": 46.073089599609375
      }
    },
    {
      "sequence": "What is consciousness? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 34.76627938643746,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.948148727416992,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 28.3496036529541,
        "model.layers.0.pre_feedforward_layernorm": 64.28872680664062,
        "model.layers.0.mlp.gate_proj": 68.1522445678711,
        "model.layers.0.mlp.up_proj": 49.40247344970703,
        "model.layers.0.mlp.down_proj": 14.146541595458984,
        "model.layers.0.mlp": 14.146541595458984,
        "model.layers.0.post_feedforward_layernorm": 48.606143951416016,
        "model.layers.0": 51.28742218017578,
        "model.layers.1.input_layernorm": 53.30382537841797,
        "model.layers.1.post_attention_layernorm": 16.751340866088867,
        "model.layers.1.pre_feedforward_layernorm": 53.11286163330078,
        "model.layers.1.mlp.gate_proj": 47.32758712768555,
        "model.layers.1.mlp.up_proj": 38.39776611328125,
        "model.layers.1.mlp.down_proj": 3.2245404720306396,
        "model.layers.1.mlp": 3.2245404720306396,
        "model.layers.1.post_feedforward_layernorm": 19.45875358581543,
        "model.layers.1": 47.46088790893555,
        "model.layers.2.input_layernorm": 47.65623092651367,
        "model.layers.2.post_attention_layernorm": 24.327678680419922,
        "model.layers.2.pre_feedforward_layernorm": 46.4263801574707
      }
    },
    {
      "sequence": "What is the Fibonacci sequence? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 34.85472345352173,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 28.287668228149414,
        "model.layers.0.pre_feedforward_layernorm": 65.48028564453125,
        "model.layers.0.mlp.gate_proj": 69.09906768798828,
        "model.layers.0.mlp.up_proj": 49.59956741333008,
        "model.layers.0.mlp.down_proj": 13.464818954467773,
        "model.layers.0.mlp": 13.464818954467773,
        "model.layers.0.post_feedforward_layernorm": 47.64933776855469,
        "model.layers.0": 51.449100494384766,
        "model.layers.1.input_layernorm": 53.94855499267578,
        "model.layers.1.post_attention_layernorm": 15.653363227844238,
        "model.layers.1.pre_feedforward_layernorm": 54.25192642211914,
        "model.layers.1.mlp.gate_proj": 48.07832717895508,
        "model.layers.1.mlp.up_proj": 40.183502197265625,
        "model.layers.1.mlp.down_proj": 3.583446979522705,
        "model.layers.1.mlp": 3.583446979522705,
        "model.layers.1.post_feedforward_layernorm": 19.018381118774414,
        "model.layers.1": 47.57415008544922,
        "model.layers.2.input_layernorm": 48.11885452270508,
        "model.layers.2.post_attention_layernorm": 22.728797912597656,
        "model.layers.2.pre_feedforward_layernorm": 45.89323806762695
      }
    },
    {
      "sequence": "What is the structure of a sonnet? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 34.96397818689761,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.646179676055908,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 27.377681732177734,
        "model.layers.0.pre_feedforward_layernorm": 64.69119262695312,
        "model.layers.0.mlp.gate_proj": 68.2131576538086,
        "model.layers.0.mlp.up_proj": 49.822792053222656,
        "model.layers.0.mlp.down_proj": 13.705771446228027,
        "model.layers.0.mlp": 13.705771446228027,
        "model.layers.0.post_feedforward_layernorm": 47.1989860534668,
        "model.layers.0": 50.384281158447266,
        "model.layers.1.input_layernorm": 54.155757904052734,
        "model.layers.1.post_attention_layernorm": 17.03135871887207,
        "model.layers.1.pre_feedforward_layernorm": 54.683170318603516,
        "model.layers.1.mlp.gate_proj": 48.97358703613281,
        "model.layers.1.mlp.up_proj": 39.72542190551758,
        "model.layers.1.mlp.down_proj": 3.668097496032715,
        "model.layers.1.mlp": 3.668097496032715,
        "model.layers.1.post_feedforward_layernorm": 22.263111114501953,
        "model.layers.1": 48.158260345458984,
        "model.layers.2.input_layernorm": 47.5333137512207,
        "model.layers.2.post_attention_layernorm": 22.69959259033203,
        "model.layers.2.pre_feedforward_layernorm": 46.241729736328125
      }
    },
    {
      "sequence": "What is version control? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 35.03263288995494,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.630651950836182,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 32.58509063720703,
        "model.layers.0.pre_feedforward_layernorm": 61.73019027709961,
        "model.layers.0.mlp.gate_proj": 67.4099349975586,
        "model.layers.0.mlp.up_proj": 47.74738311767578,
        "model.layers.0.mlp.down_proj": 13.046760559082031,
        "model.layers.0.mlp": 13.046760559082031,
        "model.layers.0.post_feedforward_layernorm": 48.23708724975586,
        "model.layers.0": 52.341556549072266,
        "model.layers.1.input_layernorm": 53.529388427734375,
        "model.layers.1.post_attention_layernorm": 18.368383407592773,
        "model.layers.1.pre_feedforward_layernorm": 54.51286697387695,
        "model.layers.1.mlp.gate_proj": 49.08818817138672,
        "model.layers.1.mlp.up_proj": 40.04401397705078,
        "model.layers.1.mlp.down_proj": 3.4788734912872314,
        "model.layers.1.mlp": 3.4788734912872314,
        "model.layers.1.post_feedforward_layernorm": 18.550134658813477,
        "model.layers.1": 48.70619583129883,
        "model.layers.2.input_layernorm": 48.411991119384766,
        "model.layers.2.post_attention_layernorm": 24.912437438964844,
        "model.layers.2.pre_feedforward_layernorm": 46.26960754394531
      }
    },
    {
      "sequence": "What caused World War II? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 35.60414797326793,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 32.04713821411133,
        "model.layers.0.pre_feedforward_layernorm": 63.050968170166016,
        "model.layers.0.mlp.gate_proj": 68.74943542480469,
        "model.layers.0.mlp.up_proj": 49.74786376953125,
        "model.layers.0.mlp.down_proj": 12.675440788269043,
        "model.layers.0.mlp": 12.675440788269043,
        "model.layers.0.post_feedforward_layernorm": 46.78440856933594,
        "model.layers.0": 52.2109260559082,
        "model.layers.1.input_layernorm": 54.73183059692383,
        "model.layers.1.post_attention_layernorm": 18.51163673400879,
        "model.layers.1.pre_feedforward_layernorm": 58.65287399291992,
        "model.layers.1.mlp.gate_proj": 54.12195587158203,
        "model.layers.1.mlp.up_proj": 40.787559509277344,
        "model.layers.1.mlp.down_proj": 3.3372740745544434,
        "model.layers.1.mlp": 3.3372740745544434,
        "model.layers.1.post_feedforward_layernorm": 20.84016990661621,
        "model.layers.1": 49.10673904418945,
        "model.layers.2.input_layernorm": 49.06909942626953,
        "model.layers.2.post_attention_layernorm": 20.8975887298584,
        "model.layers.2.pre_feedforward_layernorm": 47.01179504394531
      }
    },
    {
      "sequence": "What was the Industrial Revolution? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 35.69155015116153,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 7.923798561096191,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 30.197771072387695,
        "model.layers.0.pre_feedforward_layernorm": 63.91199493408203,
        "model.layers.0.mlp.gate_proj": 69.01600646972656,
        "model.layers.0.mlp.up_proj": 49.665279388427734,
        "model.layers.0.mlp.down_proj": 13.239160537719727,
        "model.layers.0.mlp": 13.239160537719727,
        "model.layers.0.post_feedforward_layernorm": 47.39359664916992,
        "model.layers.0": 52.4565544128418,
        "model.layers.1.input_layernorm": 55.01917266845703,
        "model.layers.1.post_attention_layernorm": 17.744457244873047,
        "model.layers.1.pre_feedforward_layernorm": 57.48575210571289,
        "model.layers.1.mlp.gate_proj": 52.53662872314453,
        "model.layers.1.mlp.up_proj": 40.49110794067383,
        "model.layers.1.mlp.down_proj": 3.54579496383667,
        "model.layers.1.mlp": 3.54579496383667,
        "model.layers.1.post_feedforward_layernorm": 21.982698440551758,
        "model.layers.1": 49.48119354248047,
        "model.layers.2.input_layernorm": 49.19486618041992,
        "model.layers.2.post_attention_layernorm": 23.11096954345703,
        "model.layers.2.pre_feedforward_layernorm": 47.099708557128906
      }
    },
    {
      "sequence": "? ? ? ",
      "variation_type": "repeated_words",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 38.31006298894467,
      "layer_variances": {
        "model.embed_tokens": 2.147458076477051,
        "model.rotary_emb": 7.97651481628418,
        "model.layers.0.input_layernorm": 56.31801223754883,
        "model.layers.0.post_attention_layernorm": 41.97869873046875,
        "model.layers.0.pre_feedforward_layernorm": 55.55854415893555,
        "model.layers.0.mlp.gate_proj": 41.9289665222168,
        "model.layers.0.mlp.up_proj": 39.186588287353516,
        "model.layers.0.mlp.down_proj": 11.314801216125488,
        "model.layers.0.mlp": 11.314801216125488,
        "model.layers.0.post_feedforward_layernorm": 56.17283248901367,
        "model.layers.0": 78.05386352539062,
        "model.layers.1.input_layernorm": 58.38624954223633,
        "model.layers.1.post_attention_layernorm": 31.552350997924805,
        "model.layers.1.pre_feedforward_layernorm": 51.55461502075195,
        "model.layers.1.mlp.gate_proj": 53.86204528808594,
        "model.layers.1.mlp.up_proj": 34.776283264160156,
        "model.layers.1.mlp.down_proj": 4.282120227813721,
        "model.layers.1.mlp": 4.282120227813721,
        "model.layers.1.post_feedforward_layernorm": 30.823802947998047,
        "model.layers.1": 77.27803039550781,
        "model.layers.2.input_layernorm": 61.166297912597656,
        "model.layers.2.post_attention_layernorm": 20.01716423034668,
        "model.layers.2.pre_feedforward_layernorm": 51.19928741455078
      }
    },
    {
      "sequence": "The following is a test: ?",
      "variation_type": "prefix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 39.20285427570343,
      "layer_variances": {
        "model.embed_tokens": 1.4230164289474487,
        "model.rotary_emb": 9.092474937438965,
        "model.layers.0.input_layernorm": 46.11985778808594,
        "model.layers.0.post_attention_layernorm": 53.36570739746094,
        "model.layers.0.pre_feedforward_layernorm": 61.33468246459961,
        "model.layers.0.mlp.gate_proj": 51.95497512817383,
        "model.layers.0.mlp.up_proj": 39.439151763916016,
        "model.layers.0.mlp.down_proj": 7.323841094970703,
        "model.layers.0.mlp": 7.323841094970703,
        "model.layers.0.post_feedforward_layernorm": 56.57835388183594,
        "model.layers.0": 63.6344108581543,
        "model.layers.1.input_layernorm": 53.99164581298828,
        "model.layers.1.post_attention_layernorm": 36.932003021240234,
        "model.layers.1.pre_feedforward_layernorm": 59.328163146972656,
        "model.layers.1.mlp.gate_proj": 62.56783676147461,
        "model.layers.1.mlp.up_proj": 38.77792739868164,
        "model.layers.1.mlp.down_proj": 4.380538463592529,
        "model.layers.1.mlp": 4.380538463592529,
        "model.layers.1.post_feedforward_layernorm": 49.532958984375,
        "model.layers.1": 66.6334228515625,
        "model.layers.2.input_layernorm": 56.96296691894531,
        "model.layers.2.post_attention_layernorm": 22.261962890625,
        "model.layers.2.pre_feedforward_layernorm": 48.32537078857422
      }
    },
    {
      "sequence": "? Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 41.34301469637,
      "layer_variances": {
        "model.embed_tokens": 1.8393654823303223,
        "model.rotary_emb": 8.595183372497559,
        "model.layers.0.input_layernorm": 50.784820556640625,
        "model.layers.0.post_attention_layernorm": 64.15172576904297,
        "model.layers.0.pre_feedforward_layernorm": 58.72380828857422,
        "model.layers.0.mlp.gate_proj": 81.47185516357422,
        "model.layers.0.mlp.up_proj": 43.392372131347656,
        "model.layers.0.mlp.down_proj": 11.488749504089355,
        "model.layers.0.mlp": 11.488749504089355,
        "model.layers.0.post_feedforward_layernorm": 54.36765670776367,
        "model.layers.0": 72.62501525878906,
        "model.layers.1.input_layernorm": 57.97367477416992,
        "model.layers.1.post_attention_layernorm": 23.47972297668457,
        "model.layers.1.pre_feedforward_layernorm": 54.241737365722656,
        "model.layers.1.mlp.gate_proj": 73.42820739746094,
        "model.layers.1.mlp.up_proj": 38.38286209106445,
        "model.layers.1.mlp.down_proj": 4.956759929656982,
        "model.layers.1.mlp": 4.956759929656982,
        "model.layers.1.post_feedforward_layernorm": 34.59141159057617,
        "model.layers.1": 66.3313980102539,
        "model.layers.2.input_layernorm": 58.09444046020508,
        "model.layers.2.post_attention_layernorm": 23.083234786987305,
        "model.layers.2.pre_feedforward_layernorm": 52.43982696533203
      }
    },
    {
      "sequence": "?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 41.7983704338903,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 3.135873794555664,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 19.94576072692871,
        "model.layers.0.pre_feedforward_layernorm": 61.070980072021484,
        "model.layers.0.mlp.gate_proj": 47.4551887512207,
        "model.layers.0.mlp.up_proj": 43.60353088378906,
        "model.layers.0.mlp.down_proj": 7.296640396118164,
        "model.layers.0.mlp": 7.296640396118164,
        "model.layers.0.post_feedforward_layernorm": 54.30302810668945,
        "model.layers.0": 92.00304412841797,
        "model.layers.1.input_layernorm": 77.55945587158203,
        "model.layers.1.post_attention_layernorm": 22.80590057373047,
        "model.layers.1.pre_feedforward_layernorm": 64.52488708496094,
        "model.layers.1.mlp.gate_proj": 58.013160705566406,
        "model.layers.1.mlp.up_proj": 46.76548767089844,
        "model.layers.1.mlp.down_proj": 3.6291160583496094,
        "model.layers.1.mlp": 3.6291160583496094,
        "model.layers.1.post_feedforward_layernorm": 29.614646911621094,
        "model.layers.1": 87.57637786865234,
        "model.layers.2.input_layernorm": 74.6215591430664,
        "model.layers.2.post_attention_layernorm": 23.849950790405273,
        "model.layers.2.pre_feedforward_layernorm": 59.408267974853516
      }
    },
    {
      "sequence": "Write Python code to sort a list.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 42.627217966577284,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.8216638565063477,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 38.438602447509766,
        "model.layers.0.pre_feedforward_layernorm": 85.394775390625,
        "model.layers.0.mlp.gate_proj": 74.24173736572266,
        "model.layers.0.mlp.up_proj": 57.45701217651367,
        "model.layers.0.mlp.down_proj": 12.90707778930664,
        "model.layers.0.mlp": 12.90707778930664,
        "model.layers.0.post_feedforward_layernorm": 51.07893753051758,
        "model.layers.0": 80.03160095214844,
        "model.layers.1.input_layernorm": 74.76634216308594,
        "model.layers.1.post_attention_layernorm": 13.295879364013672,
        "model.layers.1.pre_feedforward_layernorm": 60.22882080078125,
        "model.layers.1.mlp.gate_proj": 53.08172607421875,
        "model.layers.1.mlp.up_proj": 45.23940658569336,
        "model.layers.1.mlp.down_proj": 3.9796206951141357,
        "model.layers.1.mlp": 3.9796206951141357,
        "model.layers.1.post_feedforward_layernorm": 27.14595603942871,
        "model.layers.1": 74.4278335571289,
        "model.layers.2.input_layernorm": 64.07762145996094,
        "model.layers.2.post_attention_layernorm": 19.60871124267578,
        "model.layers.2.pre_feedforward_layernorm": 48.77576446533203
      }
    },
    {
      "sequence": "Please explain step by step.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 42.775782108306885,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 42.071250915527344,
        "model.layers.0.pre_feedforward_layernorm": 81.23332977294922,
        "model.layers.0.mlp.gate_proj": 74.93038177490234,
        "model.layers.0.mlp.up_proj": 54.54868698120117,
        "model.layers.0.mlp.down_proj": 12.05203914642334,
        "model.layers.0.mlp": 12.05203914642334,
        "model.layers.0.post_feedforward_layernorm": 49.53944778442383,
        "model.layers.0": 81.01092529296875,
        "model.layers.1.input_layernorm": 74.67236328125,
        "model.layers.1.post_attention_layernorm": 14.706425666809082,
        "model.layers.1.pre_feedforward_layernorm": 61.817222595214844,
        "model.layers.1.mlp.gate_proj": 51.49666976928711,
        "model.layers.1.mlp.up_proj": 45.769779205322266,
        "model.layers.1.mlp.down_proj": 4.892477989196777,
        "model.layers.1.mlp": 4.892477989196777,
        "model.layers.1.post_feedforward_layernorm": 29.674560546875,
        "model.layers.1": 74.49547576904297,
        "model.layers.2.input_layernorm": 66.05023956298828,
        "model.layers.2.post_attention_layernorm": 19.72392463684082,
        "model.layers.2.pre_feedforward_layernorm": 49.154808044433594
      }
    },
    {
      "sequence": "Describe API design principles.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 42.78465789297353,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 38.459903717041016,
        "model.layers.0.pre_feedforward_layernorm": 84.51892852783203,
        "model.layers.0.mlp.gate_proj": 73.26532745361328,
        "model.layers.0.mlp.up_proj": 56.13376235961914,
        "model.layers.0.mlp.down_proj": 11.98863697052002,
        "model.layers.0.mlp": 11.98863697052002,
        "model.layers.0.post_feedforward_layernorm": 50.75725173950195,
        "model.layers.0": 80.10594177246094,
        "model.layers.1.input_layernorm": 72.93324279785156,
        "model.layers.1.post_attention_layernorm": 16.344194412231445,
        "model.layers.1.pre_feedforward_layernorm": 61.458797454833984,
        "model.layers.1.mlp.gate_proj": 51.7264518737793,
        "model.layers.1.mlp.up_proj": 45.647361755371094,
        "model.layers.1.mlp.down_proj": 4.055877208709717,
        "model.layers.1.mlp": 4.055877208709717,
        "model.layers.1.post_feedforward_layernorm": 29.42843246459961,
        "model.layers.1": 73.3519287109375,
        "model.layers.2.input_layernorm": 65.59259796142578,
        "model.layers.2.post_attention_layernorm": 21.061227798461914,
        "model.layers.2.pre_feedforward_layernorm": 51.73881149291992
      }
    },
    {
      "sequence": "Explain database normalization.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.1670690101126,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.6975889205932617,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 38.67351531982422,
        "model.layers.0.pre_feedforward_layernorm": 84.96493530273438,
        "model.layers.0.mlp.gate_proj": 73.3836441040039,
        "model.layers.0.mlp.up_proj": 56.60477828979492,
        "model.layers.0.mlp.down_proj": 12.403463363647461,
        "model.layers.0.mlp": 12.403463363647461,
        "model.layers.0.post_feedforward_layernorm": 50.5163459777832,
        "model.layers.0": 81.08426666259766,
        "model.layers.1.input_layernorm": 73.34146881103516,
        "model.layers.1.post_attention_layernorm": 15.913131713867188,
        "model.layers.1.pre_feedforward_layernorm": 62.235652923583984,
        "model.layers.1.mlp.gate_proj": 51.43629455566406,
        "model.layers.1.mlp.up_proj": 46.606048583984375,
        "model.layers.1.mlp.down_proj": 3.8525357246398926,
        "model.layers.1.mlp": 3.8525357246398926,
        "model.layers.1.post_feedforward_layernorm": 29.442441940307617,
        "model.layers.1": 73.93140411376953,
        "model.layers.2.input_layernorm": 66.57737731933594,
        "model.layers.2.post_attention_layernorm": 24.10073471069336,
        "model.layers.2.pre_feedforward_layernorm": 52.28073501586914
      }
    },
    {
      "sequence": "Describe time management methods.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.22254479449728,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 40.46272659301758,
        "model.layers.0.pre_feedforward_layernorm": 81.95816040039062,
        "model.layers.0.mlp.gate_proj": 72.20787048339844,
        "model.layers.0.mlp.up_proj": 54.62791061401367,
        "model.layers.0.mlp.down_proj": 12.135576248168945,
        "model.layers.0.mlp": 12.135576248168945,
        "model.layers.0.post_feedforward_layernorm": 50.76712417602539,
        "model.layers.0": 81.72237396240234,
        "model.layers.1.input_layernorm": 73.5811996459961,
        "model.layers.1.post_attention_layernorm": 17.97686195373535,
        "model.layers.1.pre_feedforward_layernorm": 62.68824768066406,
        "model.layers.1.mlp.gate_proj": 51.42458724975586,
        "model.layers.1.mlp.up_proj": 46.057945251464844,
        "model.layers.1.mlp.down_proj": 4.17991828918457,
        "model.layers.1.mlp": 4.17991828918457,
        "model.layers.1.post_feedforward_layernorm": 31.037250518798828,
        "model.layers.1": 75.92865753173828,
        "model.layers.2.input_layernorm": 67.79438781738281,
        "model.layers.2.post_attention_layernorm": 21.222261428833008,
        "model.layers.2.pre_feedforward_layernorm": 52.59603500366211
      }
    },
    {
      "sequence": "I need detailed information about this topic.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.355063552441806,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.8216638565063477,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 38.98109817504883,
        "model.layers.0.pre_feedforward_layernorm": 83.8786392211914,
        "model.layers.0.mlp.gate_proj": 72.70418548583984,
        "model.layers.0.mlp.up_proj": 55.70687484741211,
        "model.layers.0.mlp.down_proj": 12.372945785522461,
        "model.layers.0.mlp": 12.372945785522461,
        "model.layers.0.post_feedforward_layernorm": 50.83200454711914,
        "model.layers.0": 81.24749755859375,
        "model.layers.1.input_layernorm": 75.72050476074219,
        "model.layers.1.post_attention_layernorm": 13.018228530883789,
        "model.layers.1.pre_feedforward_layernorm": 64.162109375,
        "model.layers.1.mlp.gate_proj": 54.23624801635742,
        "model.layers.1.mlp.up_proj": 47.3018913269043,
        "model.layers.1.mlp.down_proj": 4.863491058349609,
        "model.layers.1.mlp": 4.863491058349609,
        "model.layers.1.post_feedforward_layernorm": 29.653343200683594,
        "model.layers.1": 75.49666595458984,
        "model.layers.2.input_layernorm": 67.88772583007812,
        "model.layers.2.post_attention_layernorm": 21.445737838745117,
        "model.layers.2.pre_feedforward_layernorm": 51.05894470214844
      }
    },
    {
      "sequence": "Explain how neural networks work.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.42414708759474,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 38.954689025878906,
        "model.layers.0.pre_feedforward_layernorm": 85.15746307373047,
        "model.layers.0.mlp.gate_proj": 74.43453216552734,
        "model.layers.0.mlp.up_proj": 56.39144515991211,
        "model.layers.0.mlp.down_proj": 12.695806503295898,
        "model.layers.0.mlp": 12.695806503295898,
        "model.layers.0.post_feedforward_layernorm": 50.3830680847168,
        "model.layers.0": 81.2835464477539,
        "model.layers.1.input_layernorm": 74.87985229492188,
        "model.layers.1.post_attention_layernorm": 14.848782539367676,
        "model.layers.1.pre_feedforward_layernorm": 63.11310577392578,
        "model.layers.1.mlp.gate_proj": 51.81825637817383,
        "model.layers.1.mlp.up_proj": 46.69226837158203,
        "model.layers.1.mlp.down_proj": 4.18899393081665,
        "model.layers.1.mlp": 4.18899393081665,
        "model.layers.1.post_feedforward_layernorm": 30.38239288330078,
        "model.layers.1": 74.2359619140625,
        "model.layers.2.input_layernorm": 67.68321990966797,
        "model.layers.2.post_attention_layernorm": 21.735790252685547,
        "model.layers.2.pre_feedforward_layernorm": 53.932945251464844
      }
    },
    {
      "sequence": "Explain probability theory.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.71879456354224,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.6975889205932617,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 37.29604721069336,
        "model.layers.0.pre_feedforward_layernorm": 84.35623931884766,
        "model.layers.0.mlp.gate_proj": 73.46078491210938,
        "model.layers.0.mlp.up_proj": 56.302494049072266,
        "model.layers.0.mlp.down_proj": 12.64991283416748,
        "model.layers.0.mlp": 12.64991283416748,
        "model.layers.0.post_feedforward_layernorm": 50.166046142578125,
        "model.layers.0": 81.77934265136719,
        "model.layers.1.input_layernorm": 74.73188018798828,
        "model.layers.1.post_attention_layernorm": 17.969661712646484,
        "model.layers.1.pre_feedforward_layernorm": 64.4388198852539,
        "model.layers.1.mlp.gate_proj": 52.7347526550293,
        "model.layers.1.mlp.up_proj": 47.60639953613281,
        "model.layers.1.mlp.down_proj": 4.3062005043029785,
        "model.layers.1.mlp": 4.3062005043029785,
        "model.layers.1.post_feedforward_layernorm": 32.05553436279297,
        "model.layers.1": 74.68805694580078,
        "model.layers.2.input_layernorm": 67.60102844238281,
        "model.layers.2.post_attention_layernorm": 22.677541732788086,
        "model.layers.2.pre_feedforward_layernorm": 54.51760482788086
      }
    },
    {
      "sequence": "Explain ethical dilemmas.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.763958547426306,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.6975889205932617,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 37.47907638549805,
        "model.layers.0.pre_feedforward_layernorm": 85.30329895019531,
        "model.layers.0.mlp.gate_proj": 73.11441802978516,
        "model.layers.0.mlp.up_proj": 56.43515396118164,
        "model.layers.0.mlp.down_proj": 12.398004531860352,
        "model.layers.0.mlp": 12.398004531860352,
        "model.layers.0.post_feedforward_layernorm": 50.50278091430664,
        "model.layers.0": 81.54448699951172,
        "model.layers.1.input_layernorm": 74.97467803955078,
        "model.layers.1.post_attention_layernorm": 16.189359664916992,
        "model.layers.1.pre_feedforward_layernorm": 64.16360473632812,
        "model.layers.1.mlp.gate_proj": 52.90761184692383,
        "model.layers.1.mlp.up_proj": 46.62825012207031,
        "model.layers.1.mlp.down_proj": 4.328679084777832,
        "model.layers.1.mlp": 4.328679084777832,
        "model.layers.1.post_feedforward_layernorm": 31.92511749267578,
        "model.layers.1": 75.79703521728516,
        "model.layers.2.input_layernorm": 68.81582641601562,
        "model.layers.2.post_attention_layernorm": 23.42825698852539,
        "model.layers.2.pre_feedforward_layernorm": 54.6709098815918
      }
    },
    {
      "sequence": "Describe the process of machine learning.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.773885509242184,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 41.48239517211914,
        "model.layers.0.pre_feedforward_layernorm": 82.77564239501953,
        "model.layers.0.mlp.gate_proj": 75.04186248779297,
        "model.layers.0.mlp.up_proj": 55.576839447021484,
        "model.layers.0.mlp.down_proj": 12.734736442565918,
        "model.layers.0.mlp": 12.734736442565918,
        "model.layers.0.post_feedforward_layernorm": 50.616573333740234,
        "model.layers.0": 81.3832015991211,
        "model.layers.1.input_layernorm": 74.20899963378906,
        "model.layers.1.post_attention_layernorm": 16.099550247192383,
        "model.layers.1.pre_feedforward_layernorm": 64.73259735107422,
        "model.layers.1.mlp.gate_proj": 55.33404541015625,
        "model.layers.1.mlp.up_proj": 47.861568450927734,
        "model.layers.1.mlp.down_proj": 4.7044572830200195,
        "model.layers.1.mlp": 4.7044572830200195,
        "model.layers.1.post_feedforward_layernorm": 30.111286163330078,
        "model.layers.1": 75.52816772460938,
        "model.layers.2.input_layernorm": 67.64958190917969,
        "model.layers.2.post_attention_layernorm": 20.945560455322266,
        "model.layers.2.pre_feedforward_layernorm": 53.36997985839844
      }
    },
    {
      "sequence": "Explain cooking techniques.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.7795194439266,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.6975889205932617,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 39.35429000854492,
        "model.layers.0.pre_feedforward_layernorm": 84.1157455444336,
        "model.layers.0.mlp.gate_proj": 73.65943908691406,
        "model.layers.0.mlp.up_proj": 55.86721420288086,
        "model.layers.0.mlp.down_proj": 12.557677268981934,
        "model.layers.0.mlp": 12.557677268981934,
        "model.layers.0.post_feedforward_layernorm": 50.621551513671875,
        "model.layers.0": 81.75703430175781,
        "model.layers.1.input_layernorm": 74.16830444335938,
        "model.layers.1.post_attention_layernorm": 17.71209716796875,
        "model.layers.1.pre_feedforward_layernorm": 65.03311920166016,
        "model.layers.1.mlp.gate_proj": 52.11331558227539,
        "model.layers.1.mlp.up_proj": 47.179264068603516,
        "model.layers.1.mlp.down_proj": 4.359670639038086,
        "model.layers.1.mlp": 4.359670639038086,
        "model.layers.1.post_feedforward_layernorm": 30.749662399291992,
        "model.layers.1": 76.6672592163086,
        "model.layers.2.input_layernorm": 68.19512939453125,
        "model.layers.2.post_attention_layernorm": 22.442232131958008,
        "model.layers.2.pre_feedforward_layernorm": 54.22077941894531
      }
    },
    {
      "sequence": "Describe free will vs determinism.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.827713209649794,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 42.58152770996094,
        "model.layers.0.pre_feedforward_layernorm": 82.1988754272461,
        "model.layers.0.mlp.gate_proj": 76.61033630371094,
        "model.layers.0.mlp.up_proj": 55.709232330322266,
        "model.layers.0.mlp.down_proj": 12.232248306274414,
        "model.layers.0.mlp": 12.232248306274414,
        "model.layers.0.post_feedforward_layernorm": 49.60777282714844,
        "model.layers.0": 80.71822357177734,
        "model.layers.1.input_layernorm": 73.84112548828125,
        "model.layers.1.post_attention_layernorm": 18.902828216552734,
        "model.layers.1.pre_feedforward_layernorm": 65.80815887451172,
        "model.layers.1.mlp.gate_proj": 56.83552551269531,
        "model.layers.1.mlp.up_proj": 48.095428466796875,
        "model.layers.1.mlp.down_proj": 4.370126724243164,
        "model.layers.1.mlp": 4.370126724243164,
        "model.layers.1.post_feedforward_layernorm": 29.51927375793457,
        "model.layers.1": 74.43486022949219,
        "model.layers.2.input_layernorm": 66.9826889038086,
        "model.layers.2.post_attention_layernorm": 20.14672088623047,
        "model.layers.2.pre_feedforward_layernorm": 53.63694763183594
      }
    },
    {
      "sequence": "Explain film editing principles.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.90142482259999,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 39.61076354980469,
        "model.layers.0.pre_feedforward_layernorm": 84.32897186279297,
        "model.layers.0.mlp.gate_proj": 74.66629028320312,
        "model.layers.0.mlp.up_proj": 56.465354919433594,
        "model.layers.0.mlp.down_proj": 12.478880882263184,
        "model.layers.0.mlp": 12.478880882263184,
        "model.layers.0.post_feedforward_layernorm": 51.07455062866211,
        "model.layers.0": 81.25299072265625,
        "model.layers.1.input_layernorm": 74.95600128173828,
        "model.layers.1.post_attention_layernorm": 16.55069923400879,
        "model.layers.1.pre_feedforward_layernorm": 64.18528747558594,
        "model.layers.1.mlp.gate_proj": 52.877349853515625,
        "model.layers.1.mlp.up_proj": 47.144203186035156,
        "model.layers.1.mlp.down_proj": 4.135615348815918,
        "model.layers.1.mlp": 4.135615348815918,
        "model.layers.1.post_feedforward_layernorm": 31.41617774963379,
        "model.layers.1": 75.546142578125,
        "model.layers.2.input_layernorm": 68.87733459472656,
        "model.layers.2.post_attention_layernorm": 23.23203468322754,
        "model.layers.2.pre_feedforward_layernorm": 54.885684967041016
      }
    },
    {
      "sequence": "Explain quantum computing principles.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.92446932585343,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 38.373329162597656,
        "model.layers.0.pre_feedforward_layernorm": 85.8759536743164,
        "model.layers.0.mlp.gate_proj": 74.60533142089844,
        "model.layers.0.mlp.up_proj": 57.04716873168945,
        "model.layers.0.mlp.down_proj": 12.80352783203125,
        "model.layers.0.mlp": 12.80352783203125,
        "model.layers.0.post_feedforward_layernorm": 51.034324645996094,
        "model.layers.0": 81.79898071289062,
        "model.layers.1.input_layernorm": 75.55265808105469,
        "model.layers.1.post_attention_layernorm": 15.969735145568848,
        "model.layers.1.pre_feedforward_layernorm": 64.81916809082031,
        "model.layers.1.mlp.gate_proj": 52.57893371582031,
        "model.layers.1.mlp.up_proj": 47.86894607543945,
        "model.layers.1.mlp.down_proj": 4.329911708831787,
        "model.layers.1.mlp": 4.329911708831787,
        "model.layers.1.post_feedforward_layernorm": 31.056381225585938,
        "model.layers.1": 75.39971160888672,
        "model.layers.2.input_layernorm": 67.39717864990234,
        "model.layers.2.post_attention_layernorm": 22.98514175415039,
        "model.layers.2.pre_feedforward_layernorm": 54.199031829833984
      }
    },
    {
      "sequence": "Describe the water cycle.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 43.99766449306322,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 40.42845916748047,
        "model.layers.0.pre_feedforward_layernorm": 83.25718688964844,
        "model.layers.0.mlp.gate_proj": 73.8978271484375,
        "model.layers.0.mlp.up_proj": 55.38157272338867,
        "model.layers.0.mlp.down_proj": 12.36844539642334,
        "model.layers.0.mlp": 12.36844539642334,
        "model.layers.0.post_feedforward_layernorm": 50.35353469848633,
        "model.layers.0": 81.78620910644531,
        "model.layers.1.input_layernorm": 74.96289825439453,
        "model.layers.1.post_attention_layernorm": 16.9156436920166,
        "model.layers.1.pre_feedforward_layernorm": 64.70691680908203,
        "model.layers.1.mlp.gate_proj": 54.18610382080078,
        "model.layers.1.mlp.up_proj": 47.40072250366211,
        "model.layers.1.mlp.down_proj": 4.593779563903809,
        "model.layers.1.mlp": 4.593779563903809,
        "model.layers.1.post_feedforward_layernorm": 32.83341979980469,
        "model.layers.1": 77.31343841552734,
        "model.layers.2.input_layernorm": 69.13438415527344,
        "model.layers.2.post_attention_layernorm": 21.719947814941406,
        "model.layers.2.pre_feedforward_layernorm": 54.309627532958984
      }
    },
    {
      "sequence": "Analyze Shakespeare's writing style.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.000796971113786,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 42.134796142578125,
        "model.layers.0.pre_feedforward_layernorm": 85.12181854248047,
        "model.layers.0.mlp.gate_proj": 75.15058898925781,
        "model.layers.0.mlp.up_proj": 57.016971588134766,
        "model.layers.0.mlp.down_proj": 12.963318824768066,
        "model.layers.0.mlp": 12.963318824768066,
        "model.layers.0.post_feedforward_layernorm": 51.10157775878906,
        "model.layers.0": 81.61785125732422,
        "model.layers.1.input_layernorm": 75.37167358398438,
        "model.layers.1.post_attention_layernorm": 15.35075569152832,
        "model.layers.1.pre_feedforward_layernorm": 64.88275909423828,
        "model.layers.1.mlp.gate_proj": 56.717681884765625,
        "model.layers.1.mlp.up_proj": 47.57551574707031,
        "model.layers.1.mlp.down_proj": 4.177399635314941,
        "model.layers.1.mlp": 4.177399635314941,
        "model.layers.1.post_feedforward_layernorm": 27.79819107055664,
        "model.layers.1": 75.58909606933594,
        "model.layers.2.input_layernorm": 68.07964324951172,
        "model.layers.2.post_attention_layernorm": 22.39166259765625,
        "model.layers.2.pre_feedforward_layernorm": 52.633182525634766
      }
    },
    {
      "sequence": "Explain calculus concepts.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.11166277139083,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.6975889205932617,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 37.1949577331543,
        "model.layers.0.pre_feedforward_layernorm": 86.75041198730469,
        "model.layers.0.mlp.gate_proj": 74.0362777709961,
        "model.layers.0.mlp.up_proj": 57.02727127075195,
        "model.layers.0.mlp.down_proj": 12.901924133300781,
        "model.layers.0.mlp": 12.901924133300781,
        "model.layers.0.post_feedforward_layernorm": 50.779579162597656,
        "model.layers.0": 81.87913513183594,
        "model.layers.1.input_layernorm": 75.92868041992188,
        "model.layers.1.post_attention_layernorm": 17.017684936523438,
        "model.layers.1.pre_feedforward_layernorm": 65.10808563232422,
        "model.layers.1.mlp.gate_proj": 54.00654220581055,
        "model.layers.1.mlp.up_proj": 47.68121337890625,
        "model.layers.1.mlp.down_proj": 4.8054375648498535,
        "model.layers.1.mlp": 4.8054375648498535,
        "model.layers.1.post_feedforward_layernorm": 33.14592742919922,
        "model.layers.1": 75.84614562988281,
        "model.layers.2.input_layernorm": 68.50384521484375,
        "model.layers.2.post_attention_layernorm": 21.444599151611328,
        "model.layers.2.pre_feedforward_layernorm": 53.56534957885742
      }
    },
    {
      "sequence": "Explain the Renaissance period.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.15712406324304,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 38.52260971069336,
        "model.layers.0.pre_feedforward_layernorm": 85.73595428466797,
        "model.layers.0.mlp.gate_proj": 73.5073471069336,
        "model.layers.0.mlp.up_proj": 56.62034606933594,
        "model.layers.0.mlp.down_proj": 12.866692543029785,
        "model.layers.0.mlp": 12.866692543029785,
        "model.layers.0.post_feedforward_layernorm": 50.794612884521484,
        "model.layers.0": 81.77940368652344,
        "model.layers.1.input_layernorm": 75.40999603271484,
        "model.layers.1.post_attention_layernorm": 16.86940574645996,
        "model.layers.1.pre_feedforward_layernorm": 64.75582122802734,
        "model.layers.1.mlp.gate_proj": 55.0438346862793,
        "model.layers.1.mlp.up_proj": 47.04227828979492,
        "model.layers.1.mlp.down_proj": 4.398324966430664,
        "model.layers.1.mlp": 4.398324966430664,
        "model.layers.1.post_feedforward_layernorm": 32.14912414550781,
        "model.layers.1": 76.6042709350586,
        "model.layers.2.input_layernorm": 68.93797302246094,
        "model.layers.2.post_attention_layernorm": 22.872013092041016,
        "model.layers.2.pre_feedforward_layernorm": 55.004886627197266
      }
    },
    {
      "sequence": "Compare and contrast these concepts.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.18249613305797,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 39.66366195678711,
        "model.layers.0.pre_feedforward_layernorm": 82.42314910888672,
        "model.layers.0.mlp.gate_proj": 73.23877716064453,
        "model.layers.0.mlp.up_proj": 55.35490798950195,
        "model.layers.0.mlp.down_proj": 12.366311073303223,
        "model.layers.0.mlp": 12.366311073303223,
        "model.layers.0.post_feedforward_layernorm": 50.17919158935547,
        "model.layers.0": 81.30916595458984,
        "model.layers.1.input_layernorm": 75.75103759765625,
        "model.layers.1.post_attention_layernorm": 18.05451011657715,
        "model.layers.1.pre_feedforward_layernorm": 67.51934051513672,
        "model.layers.1.mlp.gate_proj": 57.78059387207031,
        "model.layers.1.mlp.up_proj": 48.52754211425781,
        "model.layers.1.mlp.down_proj": 4.897177696228027,
        "model.layers.1.mlp": 4.897177696228027,
        "model.layers.1.post_feedforward_layernorm": 31.1090145111084,
        "model.layers.1": 75.98820495605469,
        "model.layers.2.input_layernorm": 69.12984466552734,
        "model.layers.2.post_attention_layernorm": 21.238889694213867,
        "model.layers.2.pre_feedforward_layernorm": 55.344139099121094
      }
    },
    {
      "sequence": "Translate: Bonjour means hello in French.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.21962542119233,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.8216638565063477,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 38.51023483276367,
        "model.layers.0.pre_feedforward_layernorm": 89.92819213867188,
        "model.layers.0.mlp.gate_proj": 78.97949981689453,
        "model.layers.0.mlp.up_proj": 58.98575210571289,
        "model.layers.0.mlp.down_proj": 13.482361793518066,
        "model.layers.0.mlp": 13.482361793518066,
        "model.layers.0.post_feedforward_layernorm": 52.11021423339844,
        "model.layers.0": 81.21113586425781,
        "model.layers.1.input_layernorm": 76.16332244873047,
        "model.layers.1.post_attention_layernorm": 12.767781257629395,
        "model.layers.1.pre_feedforward_layernorm": 64.24343872070312,
        "model.layers.1.mlp.gate_proj": 55.5660400390625,
        "model.layers.1.mlp.up_proj": 47.967041015625,
        "model.layers.1.mlp.down_proj": 4.692435264587402,
        "model.layers.1.mlp": 4.692435264587402,
        "model.layers.1.post_feedforward_layernorm": 29.904565811157227,
        "model.layers.1": 75.93453216552734,
        "model.layers.2.input_layernorm": 67.07289123535156,
        "model.layers.2.post_attention_layernorm": 21.939271926879883,
        "model.layers.2.pre_feedforward_layernorm": 50.05598831176758
      }
    },
    {
      "sequence": "Prove the Pythagorean theorem.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.24239954741105,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 35.18115997314453,
        "model.layers.0.pre_feedforward_layernorm": 88.72795867919922,
        "model.layers.0.mlp.gate_proj": 74.10070037841797,
        "model.layers.0.mlp.up_proj": 58.62173080444336,
        "model.layers.0.mlp.down_proj": 13.5269193649292,
        "model.layers.0.mlp": 13.5269193649292,
        "model.layers.0.post_feedforward_layernorm": 51.275882720947266,
        "model.layers.0": 81.66696166992188,
        "model.layers.1.input_layernorm": 76.69561767578125,
        "model.layers.1.post_attention_layernorm": 15.299217224121094,
        "model.layers.1.pre_feedforward_layernorm": 63.99479675292969,
        "model.layers.1.mlp.gate_proj": 54.86965560913086,
        "model.layers.1.mlp.up_proj": 47.253814697265625,
        "model.layers.1.mlp.down_proj": 4.503066062927246,
        "model.layers.1.mlp": 4.503066062927246,
        "model.layers.1.post_feedforward_layernorm": 34.903709411621094,
        "model.layers.1": 76.84384155273438,
        "model.layers.2.input_layernorm": 68.0986099243164,
        "model.layers.2.post_attention_layernorm": 20.59401512145996,
        "model.layers.2.pre_feedforward_layernorm": 53.95360565185547
      }
    },
    {
      "sequence": "Explain the entire process of how a computer processes a program from source code to execution, including compilation, memory management, and CPU operations.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.30366320195405,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.6334915161132812,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 37.88203048706055,
        "model.layers.0.pre_feedforward_layernorm": 86.95946502685547,
        "model.layers.0.mlp.gate_proj": 77.12400817871094,
        "model.layers.0.mlp.up_proj": 59.00688934326172,
        "model.layers.0.mlp.down_proj": 14.30006217956543,
        "model.layers.0.mlp": 14.30006217956543,
        "model.layers.0.post_feedforward_layernorm": 51.815879821777344,
        "model.layers.0": 81.2945556640625,
        "model.layers.1.input_layernorm": 76.88385009765625,
        "model.layers.1.post_attention_layernorm": 13.998632431030273,
        "model.layers.1.pre_feedforward_layernorm": 65.18867492675781,
        "model.layers.1.mlp.gate_proj": 56.1151008605957,
        "model.layers.1.mlp.up_proj": 48.95954895019531,
        "model.layers.1.mlp.down_proj": 4.633454322814941,
        "model.layers.1.mlp": 4.633454322814941,
        "model.layers.1.post_feedforward_layernorm": 28.52297019958496,
        "model.layers.1": 75.43184661865234,
        "model.layers.2.input_layernorm": 67.17584228515625,
        "model.layers.2.post_attention_layernorm": 21.18737030029297,
        "model.layers.2.pre_feedforward_layernorm": 54.3968391418457
      }
    },
    {
      "sequence": "Describe impressionist painting techniques.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.327528269394584,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 41.091182708740234,
        "model.layers.0.pre_feedforward_layernorm": 84.42472076416016,
        "model.layers.0.mlp.gate_proj": 75.10205078125,
        "model.layers.0.mlp.up_proj": 56.47429656982422,
        "model.layers.0.mlp.down_proj": 13.110162734985352,
        "model.layers.0.mlp": 13.110162734985352,
        "model.layers.0.post_feedforward_layernorm": 51.71381378173828,
        "model.layers.0": 81.93788146972656,
        "model.layers.1.input_layernorm": 74.40403747558594,
        "model.layers.1.post_attention_layernorm": 18.173927307128906,
        "model.layers.1.pre_feedforward_layernorm": 66.36038208007812,
        "model.layers.1.mlp.gate_proj": 56.002716064453125,
        "model.layers.1.mlp.up_proj": 47.24467849731445,
        "model.layers.1.mlp.down_proj": 4.273395538330078,
        "model.layers.1.mlp": 4.273395538330078,
        "model.layers.1.post_feedforward_layernorm": 30.534595489501953,
        "model.layers.1": 76.66047668457031,
        "model.layers.2.input_layernorm": 68.83780670166016,
        "model.layers.2.post_attention_layernorm": 21.732927322387695,
        "model.layers.2.pre_feedforward_layernorm": 55.01207733154297
      }
    },
    {
      "sequence": "Describe ancient Egyptian civilization.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.559921181720235,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 37.40419387817383,
        "model.layers.0.pre_feedforward_layernorm": 89.89944458007812,
        "model.layers.0.mlp.gate_proj": 76.37899780273438,
        "model.layers.0.mlp.up_proj": 58.30809783935547,
        "model.layers.0.mlp.down_proj": 13.325061798095703,
        "model.layers.0.mlp": 13.325061798095703,
        "model.layers.0.post_feedforward_layernorm": 52.16007995605469,
        "model.layers.0": 82.15133666992188,
        "model.layers.1.input_layernorm": 75.82946014404297,
        "model.layers.1.post_attention_layernorm": 16.67791748046875,
        "model.layers.1.pre_feedforward_layernorm": 65.6458511352539,
        "model.layers.1.mlp.gate_proj": 56.12028884887695,
        "model.layers.1.mlp.up_proj": 47.33521270751953,
        "model.layers.1.mlp.down_proj": 4.270729064941406,
        "model.layers.1.mlp": 4.270729064941406,
        "model.layers.1.post_feedforward_layernorm": 31.32063865661621,
        "model.layers.1": 77.17678833007812,
        "model.layers.2.input_layernorm": 68.40650177001953,
        "model.layers.2.post_attention_layernorm": 21.84988021850586,
        "model.layers.2.pre_feedforward_layernorm": 53.587974548339844
      }
    },
    {
      "sequence": "How do we define truth?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.66728386671647,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 27.571792602539062,
        "model.layers.0.pre_feedforward_layernorm": 79.77227783203125,
        "model.layers.0.mlp.gate_proj": 67.50651550292969,
        "model.layers.0.mlp.up_proj": 57.73377990722656,
        "model.layers.0.mlp.down_proj": 12.786121368408203,
        "model.layers.0.mlp": 12.786121368408203,
        "model.layers.0.post_feedforward_layernorm": 57.82503128051758,
        "model.layers.0": 85.7974853515625,
        "model.layers.1.input_layernorm": 80.93484497070312,
        "model.layers.1.post_attention_layernorm": 15.803886413574219,
        "model.layers.1.pre_feedforward_layernorm": 71.42545318603516,
        "model.layers.1.mlp.gate_proj": 59.76498031616211,
        "model.layers.1.mlp.up_proj": 52.24362564086914,
        "model.layers.1.mlp.down_proj": 4.512545108795166,
        "model.layers.1.mlp": 4.512545108795166,
        "model.layers.1.post_feedforward_layernorm": 31.883800506591797,
        "model.layers.1": 79.5446548461914,
        "model.layers.2.input_layernorm": 70.2812271118164,
        "model.layers.2.post_attention_layernorm": 23.63533592224121,
        "model.layers.2.pre_feedforward_layernorm": 55.253360748291016
      }
    },
    {
      "sequence": "? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.85488711232724,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 6.2224602699279785,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 44.08763885498047,
        "model.layers.0.pre_feedforward_layernorm": 77.78472137451172,
        "model.layers.0.mlp.gate_proj": 50.953582763671875,
        "model.layers.0.mlp.up_proj": 45.52922058105469,
        "model.layers.0.mlp.down_proj": 6.805006980895996,
        "model.layers.0.mlp": 6.805006980895996,
        "model.layers.0.post_feedforward_layernorm": 55.2883415222168,
        "model.layers.0": 84.09517669677734,
        "model.layers.1.input_layernorm": 73.25048828125,
        "model.layers.1.post_attention_layernorm": 25.548608779907227,
        "model.layers.1.pre_feedforward_layernorm": 63.387210845947266,
        "model.layers.1.mlp.gate_proj": 52.86770248413086,
        "model.layers.1.mlp.up_proj": 43.81606674194336,
        "model.layers.1.mlp.down_proj": 3.945918321609497,
        "model.layers.1.mlp": 3.945918321609497,
        "model.layers.1.post_feedforward_layernorm": 42.051605224609375,
        "model.layers.1": 80.17993927001953,
        "model.layers.2.input_layernorm": 69.8431625366211,
        "model.layers.2.post_attention_layernorm": 26.418241500854492,
        "model.layers.2.pre_feedforward_layernorm": 54.897518157958984
      }
    },
    {
      "sequence": "Describe the complete lifecycle of a star from formation to death, including all stages and physical processes involved.          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.880606837894604,
      "layer_variances": {
        "model.embed_tokens": 2.2192790508270264,
        "model.rotary_emb": 2.7644670009613037,
        "model.layers.0.input_layernorm": 74.3209457397461,
        "model.layers.0.post_attention_layernorm": 36.123504638671875,
        "model.layers.0.pre_feedforward_layernorm": 86.02205657958984,
        "model.layers.0.mlp.gate_proj": 73.60824584960938,
        "model.layers.0.mlp.up_proj": 57.67454147338867,
        "model.layers.0.mlp.down_proj": 14.218546867370605,
        "model.layers.0.mlp": 14.218546867370605,
        "model.layers.0.post_feedforward_layernorm": 52.04123306274414,
        "model.layers.0": 82.43006896972656,
        "model.layers.1.input_layernorm": 78.0879898071289,
        "model.layers.1.post_attention_layernorm": 16.496435165405273,
        "model.layers.1.pre_feedforward_layernorm": 69.55071258544922,
        "model.layers.1.mlp.gate_proj": 59.89120101928711,
        "model.layers.1.mlp.up_proj": 50.998565673828125,
        "model.layers.1.mlp.down_proj": 5.076079845428467,
        "model.layers.1.mlp": 5.076079845428467,
        "model.layers.1.post_feedforward_layernorm": 29.84840202331543,
        "model.layers.1": 76.877197265625,
        "model.layers.2.input_layernorm": 69.05256652832031,
        "model.layers.2.post_attention_layernorm": 19.934741973876953,
        "model.layers.2.pre_feedforward_layernorm": 55.72254943847656
      }
    },
    {
      "sequence": "Can you help me understand this?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 44.95337709136631,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 27.8659725189209,
        "model.layers.0.pre_feedforward_layernorm": 80.60982513427734,
        "model.layers.0.mlp.gate_proj": 68.60050201416016,
        "model.layers.0.mlp.up_proj": 57.23292922973633,
        "model.layers.0.mlp.down_proj": 12.082818984985352,
        "model.layers.0.mlp": 12.082818984985352,
        "model.layers.0.post_feedforward_layernorm": 56.277706146240234,
        "model.layers.0": 87.6549072265625,
        "model.layers.1.input_layernorm": 80.52213287353516,
        "model.layers.1.post_attention_layernorm": 15.052396774291992,
        "model.layers.1.pre_feedforward_layernorm": 72.84657287597656,
        "model.layers.1.mlp.gate_proj": 59.28249740600586,
        "model.layers.1.mlp.up_proj": 52.36724853515625,
        "model.layers.1.mlp.down_proj": 4.978302001953125,
        "model.layers.1.mlp": 4.978302001953125,
        "model.layers.1.post_feedforward_layernorm": 32.99375534057617,
        "model.layers.1": 81.60277557373047,
        "model.layers.2.input_layernorm": 71.56201934814453,
        "model.layers.2.post_attention_layernorm": 23.940105438232422,
        "model.layers.2.pre_feedforward_layernorm": 55.47727584838867
      }
    },
    {
      "sequence": "How did the internet develop?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.185995371445365,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 29.90896224975586,
        "model.layers.0.pre_feedforward_layernorm": 80.96244812011719,
        "model.layers.0.mlp.gate_proj": 68.17902374267578,
        "model.layers.0.mlp.up_proj": 57.220481872558594,
        "model.layers.0.mlp.down_proj": 11.886975288391113,
        "model.layers.0.mlp": 11.886975288391113,
        "model.layers.0.post_feedforward_layernorm": 57.53190994262695,
        "model.layers.0": 87.26956176757812,
        "model.layers.1.input_layernorm": 80.73102569580078,
        "model.layers.1.post_attention_layernorm": 18.107215881347656,
        "model.layers.1.pre_feedforward_layernorm": 71.9365463256836,
        "model.layers.1.mlp.gate_proj": 56.97453689575195,
        "model.layers.1.mlp.up_proj": 52.45514678955078,
        "model.layers.1.mlp.down_proj": 4.770876407623291,
        "model.layers.1.mlp": 4.770876407623291,
        "model.layers.1.post_feedforward_layernorm": 33.19499206542969,
        "model.layers.1": 81.30723571777344,
        "model.layers.2.input_layernorm": 72.75621795654297,
        "model.layers.2.post_attention_layernorm": 24.702573776245117,
        "model.layers.2.pre_feedforward_layernorm": 56.95216751098633
      }
    },
    {
      "sequence": "How do transformers process language?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.20526871473893,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 29.054363250732422,
        "model.layers.0.pre_feedforward_layernorm": 83.15785217285156,
        "model.layers.0.mlp.gate_proj": 70.42218017578125,
        "model.layers.0.mlp.up_proj": 59.148956298828125,
        "model.layers.0.mlp.down_proj": 13.091382026672363,
        "model.layers.0.mlp": 13.091382026672363,
        "model.layers.0.post_feedforward_layernorm": 58.70656967163086,
        "model.layers.0": 87.78874969482422,
        "model.layers.1.input_layernorm": 80.5733871459961,
        "model.layers.1.post_attention_layernorm": 17.275943756103516,
        "model.layers.1.pre_feedforward_layernorm": 70.17810821533203,
        "model.layers.1.mlp.gate_proj": 56.1497688293457,
        "model.layers.1.mlp.up_proj": 52.34182357788086,
        "model.layers.1.mlp.down_proj": 4.428990840911865,
        "model.layers.1.mlp": 4.428990840911865,
        "model.layers.1.post_feedforward_layernorm": 32.04495620727539,
        "model.layers.1": 80.67868041992188,
        "model.layers.2.input_layernorm": 70.61766052246094,
        "model.layers.2.post_attention_layernorm": 24.399921417236328,
        "model.layers.2.pre_feedforward_layernorm": 56.36936950683594
      }
    },
    {
      "sequence": "What are the pros and cons?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.263186734655626,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 29.255746841430664,
        "model.layers.0.pre_feedforward_layernorm": 80.65250396728516,
        "model.layers.0.mlp.gate_proj": 70.57303619384766,
        "model.layers.0.mlp.up_proj": 58.40829086303711,
        "model.layers.0.mlp.down_proj": 12.793150901794434,
        "model.layers.0.mlp": 12.793150901794434,
        "model.layers.0.post_feedforward_layernorm": 57.3645133972168,
        "model.layers.0": 86.38848114013672,
        "model.layers.1.input_layernorm": 81.33013153076172,
        "model.layers.1.post_attention_layernorm": 17.123502731323242,
        "model.layers.1.pre_feedforward_layernorm": 72.9391098022461,
        "model.layers.1.mlp.gate_proj": 59.72134017944336,
        "model.layers.1.mlp.up_proj": 53.15202331542969,
        "model.layers.1.mlp.down_proj": 4.965139389038086,
        "model.layers.1.mlp": 4.965139389038086,
        "model.layers.1.post_feedforward_layernorm": 31.120378494262695,
        "model.layers.1": 80.64581298828125,
        "model.layers.2.input_layernorm": 71.3444595336914,
        "model.layers.2.post_attention_layernorm": 23.830318450927734,
        "model.layers.2.pre_feedforward_layernorm": 55.77025604248047
      }
    },
    {
      "sequence": "What causes climate change?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.2682985222858,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 32.69973373413086,
        "model.layers.0.pre_feedforward_layernorm": 82.1919174194336,
        "model.layers.0.mlp.gate_proj": 70.97942352294922,
        "model.layers.0.mlp.up_proj": 58.89288330078125,
        "model.layers.0.mlp.down_proj": 12.916935920715332,
        "model.layers.0.mlp": 12.916935920715332,
        "model.layers.0.post_feedforward_layernorm": 58.035980224609375,
        "model.layers.0": 87.98490905761719,
        "model.layers.1.input_layernorm": 81.14732360839844,
        "model.layers.1.post_attention_layernorm": 15.930282592773438,
        "model.layers.1.pre_feedforward_layernorm": 69.87936401367188,
        "model.layers.1.mlp.gate_proj": 55.26250076293945,
        "model.layers.1.mlp.up_proj": 51.3050422668457,
        "model.layers.1.mlp.down_proj": 4.2965569496154785,
        "model.layers.1.mlp": 4.2965569496154785,
        "model.layers.1.post_feedforward_layernorm": 33.08341979980469,
        "model.layers.1": 80.90191650390625,
        "model.layers.2.input_layernorm": 71.86463928222656,
        "model.layers.2.post_attention_layernorm": 23.321718215942383,
        "model.layers.2.pre_feedforward_layernorm": 57.115203857421875
      }
    },
    {
      "sequence": "How does exercise affect health?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.35510309882786,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 28.185575485229492,
        "model.layers.0.pre_feedforward_layernorm": 82.13792419433594,
        "model.layers.0.mlp.gate_proj": 70.56317138671875,
        "model.layers.0.mlp.up_proj": 58.70470428466797,
        "model.layers.0.mlp.down_proj": 12.856045722961426,
        "model.layers.0.mlp": 12.856045722961426,
        "model.layers.0.post_feedforward_layernorm": 58.167232513427734,
        "model.layers.0": 87.55204772949219,
        "model.layers.1.input_layernorm": 80.52716064453125,
        "model.layers.1.post_attention_layernorm": 18.678081512451172,
        "model.layers.1.pre_feedforward_layernorm": 70.23353576660156,
        "model.layers.1.mlp.gate_proj": 57.25751495361328,
        "model.layers.1.mlp.up_proj": 51.625614166259766,
        "model.layers.1.mlp.down_proj": 4.651034355163574,
        "model.layers.1.mlp": 4.651034355163574,
        "model.layers.1.post_feedforward_layernorm": 32.6078987121582,
        "model.layers.1": 82.01404571533203,
        "model.layers.2.input_layernorm": 72.42720031738281,
        "model.layers.2.post_attention_layernorm": 24.205699920654297,
        "model.layers.2.pre_feedforward_layernorm": 57.49365997314453
      }
    },
    {
      "sequence": "How does photosynthesis work?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.381906094758406,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 27.39444923400879,
        "model.layers.0.pre_feedforward_layernorm": 82.95350646972656,
        "model.layers.0.mlp.gate_proj": 69.8639907836914,
        "model.layers.0.mlp.up_proj": 59.451725006103516,
        "model.layers.0.mlp.down_proj": 13.128389358520508,
        "model.layers.0.mlp": 13.128389358520508,
        "model.layers.0.post_feedforward_layernorm": 58.581581115722656,
        "model.layers.0": 87.39802551269531,
        "model.layers.1.input_layernorm": 81.97197723388672,
        "model.layers.1.post_attention_layernorm": 16.7713623046875,
        "model.layers.1.pre_feedforward_layernorm": 71.27706909179688,
        "model.layers.1.mlp.gate_proj": 58.28273391723633,
        "model.layers.1.mlp.up_proj": 52.28507995605469,
        "model.layers.1.mlp.down_proj": 4.227950572967529,
        "model.layers.1.mlp": 4.227950572967529,
        "model.layers.1.post_feedforward_layernorm": 33.61339569091797,
        "model.layers.1": 81.29412078857422,
        "model.layers.2.input_layernorm": 72.10847473144531,
        "model.layers.2.post_attention_layernorm": 22.468687057495117,
        "model.layers.2.pre_feedforward_layernorm": 57.207359313964844
      }
    },
    {
      "sequence": "How does photosynthesis work? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.50670352189437,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 39.3719596862793,
        "model.layers.0.pre_feedforward_layernorm": 85.71119689941406,
        "model.layers.0.mlp.gate_proj": 66.77589416503906,
        "model.layers.0.mlp.up_proj": 53.751136779785156,
        "model.layers.0.mlp.down_proj": 10.638134002685547,
        "model.layers.0.mlp": 10.638134002685547,
        "model.layers.0.post_feedforward_layernorm": 54.83601379394531,
        "model.layers.0": 73.77605438232422,
        "model.layers.1.input_layernorm": 74.17266845703125,
        "model.layers.1.post_attention_layernorm": 20.74761199951172,
        "model.layers.1.pre_feedforward_layernorm": 68.61007690429688,
        "model.layers.1.mlp.gate_proj": 64.4827651977539,
        "model.layers.1.mlp.up_proj": 47.9907112121582,
        "model.layers.1.mlp.down_proj": 4.136113166809082,
        "model.layers.1.mlp": 4.136113166809082,
        "model.layers.1.post_feedforward_layernorm": 31.95082664489746,
        "model.layers.1": 70.91618347167969,
        "model.layers.2.input_layernorm": 66.75862884521484,
        "model.layers.2.post_attention_layernorm": 23.186479568481445,
        "model.layers.2.pre_feedforward_layernorm": 55.050472259521484
      }
    },
    {
      "sequence": "How does music theory work?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.551393529643185,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 27.134824752807617,
        "model.layers.0.pre_feedforward_layernorm": 81.33277893066406,
        "model.layers.0.mlp.gate_proj": 68.96302032470703,
        "model.layers.0.mlp.up_proj": 58.42869567871094,
        "model.layers.0.mlp.down_proj": 12.843344688415527,
        "model.layers.0.mlp": 12.843344688415527,
        "model.layers.0.post_feedforward_layernorm": 58.6658821105957,
        "model.layers.0": 87.68191528320312,
        "model.layers.1.input_layernorm": 82.0234603881836,
        "model.layers.1.post_attention_layernorm": 18.203937530517578,
        "model.layers.1.pre_feedforward_layernorm": 73.24872589111328,
        "model.layers.1.mlp.gate_proj": 59.63203048706055,
        "model.layers.1.mlp.up_proj": 53.061397552490234,
        "model.layers.1.mlp.down_proj": 4.5794806480407715,
        "model.layers.1.mlp": 4.5794806480407715,
        "model.layers.1.post_feedforward_layernorm": 33.09651184082031,
        "model.layers.1": 82.0710678100586,
        "model.layers.2.input_layernorm": 72.43133544921875,
        "model.layers.2.post_attention_layernorm": 23.56879425048828,
        "model.layers.2.pre_feedforward_layernorm": 57.51987838745117
      }
    },
    {
      "sequence": "What is the meaning of life?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.59135290850764,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 28.73343276977539,
        "model.layers.0.pre_feedforward_layernorm": 82.63898468017578,
        "model.layers.0.mlp.gate_proj": 70.81839752197266,
        "model.layers.0.mlp.up_proj": 58.926090240478516,
        "model.layers.0.mlp.down_proj": 13.341233253479004,
        "model.layers.0.mlp": 13.341233253479004,
        "model.layers.0.post_feedforward_layernorm": 58.04893112182617,
        "model.layers.0": 86.71245574951172,
        "model.layers.1.input_layernorm": 82.07615661621094,
        "model.layers.1.post_attention_layernorm": 15.21054744720459,
        "model.layers.1.pre_feedforward_layernorm": 73.77989196777344,
        "model.layers.1.mlp.gate_proj": 60.53256607055664,
        "model.layers.1.mlp.up_proj": 53.66853713989258,
        "model.layers.1.mlp.down_proj": 5.017346382141113,
        "model.layers.1.mlp": 5.017346382141113,
        "model.layers.1.post_feedforward_layernorm": 29.70703125,
        "model.layers.1": 80.9760513305664,
        "model.layers.2.input_layernorm": 72.54037475585938,
        "model.layers.2.post_attention_layernorm": 23.74373435974121,
        "model.layers.2.pre_feedforward_layernorm": 57.853965759277344
      }
    },
    {
      "sequence": "What are investment strategies?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.613057509712554,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 31.988624572753906,
        "model.layers.0.pre_feedforward_layernorm": 82.01529693603516,
        "model.layers.0.mlp.gate_proj": 70.7297134399414,
        "model.layers.0.mlp.up_proj": 59.10028076171875,
        "model.layers.0.mlp.down_proj": 13.391242980957031,
        "model.layers.0.mlp": 13.391242980957031,
        "model.layers.0.post_feedforward_layernorm": 58.66252136230469,
        "model.layers.0": 87.86968231201172,
        "model.layers.1.input_layernorm": 81.1062240600586,
        "model.layers.1.post_attention_layernorm": 17.72504425048828,
        "model.layers.1.pre_feedforward_layernorm": 71.06748962402344,
        "model.layers.1.mlp.gate_proj": 56.39888000488281,
        "model.layers.1.mlp.up_proj": 52.02692413330078,
        "model.layers.1.mlp.down_proj": 4.598068714141846,
        "model.layers.1.mlp": 4.598068714141846,
        "model.layers.1.post_feedforward_layernorm": 31.86463737487793,
        "model.layers.1": 81.95993041992188,
        "model.layers.2.input_layernorm": 71.69950103759766,
        "model.layers.2.post_attention_layernorm": 26.140792846679688,
        "model.layers.2.pre_feedforward_layernorm": 56.618534088134766
      }
    },
    {
      "sequence": "What are the applications of deep learning?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.65935237511344,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.8216638565063477,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 30.480323791503906,
        "model.layers.0.pre_feedforward_layernorm": 81.59507751464844,
        "model.layers.0.mlp.gate_proj": 72.06263732910156,
        "model.layers.0.mlp.up_proj": 58.94976043701172,
        "model.layers.0.mlp.down_proj": 12.889444351196289,
        "model.layers.0.mlp": 12.889444351196289,
        "model.layers.0.post_feedforward_layernorm": 58.221702575683594,
        "model.layers.0": 87.42328643798828,
        "model.layers.1.input_layernorm": 81.0965805053711,
        "model.layers.1.post_attention_layernorm": 17.15018653869629,
        "model.layers.1.pre_feedforward_layernorm": 73.01252746582031,
        "model.layers.1.mlp.gate_proj": 58.981178283691406,
        "model.layers.1.mlp.up_proj": 53.684654235839844,
        "model.layers.1.mlp.down_proj": 5.257463455200195,
        "model.layers.1.mlp": 5.257463455200195,
        "model.layers.1.post_feedforward_layernorm": 32.116539001464844,
        "model.layers.1": 81.44103240966797,
        "model.layers.2.input_layernorm": 70.99700927734375,
        "model.layers.2.post_attention_layernorm": 24.090402603149414,
        "model.layers.2.pre_feedforward_layernorm": 56.492820739746094
      }
    },
    {
      "sequence": "How does encryption work?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.702933021213695,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 28.94034194946289,
        "model.layers.0.pre_feedforward_layernorm": 82.80286407470703,
        "model.layers.0.mlp.gate_proj": 69.98558044433594,
        "model.layers.0.mlp.up_proj": 59.07564926147461,
        "model.layers.0.mlp.down_proj": 12.849945068359375,
        "model.layers.0.mlp": 12.849945068359375,
        "model.layers.0.post_feedforward_layernorm": 58.083351135253906,
        "model.layers.0": 88.0447006225586,
        "model.layers.1.input_layernorm": 81.0936050415039,
        "model.layers.1.post_attention_layernorm": 18.005964279174805,
        "model.layers.1.pre_feedforward_layernorm": 72.95529174804688,
        "model.layers.1.mlp.gate_proj": 60.399959564208984,
        "model.layers.1.mlp.up_proj": 53.57719421386719,
        "model.layers.1.mlp.down_proj": 4.666450023651123,
        "model.layers.1.mlp": 4.666450023651123,
        "model.layers.1.post_feedforward_layernorm": 32.73509979248047,
        "model.layers.1": 80.93234252929688,
        "model.layers.2.input_layernorm": 72.49298095703125,
        "model.layers.2.post_attention_layernorm": 23.77634048461914,
        "model.layers.2.pre_feedforward_layernorm": 57.08578109741211
      }
    },
    {
      "sequence": "How do you solve quadratic equations?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.77904596536056,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 26.940406799316406,
        "model.layers.0.pre_feedforward_layernorm": 84.23638153076172,
        "model.layers.0.mlp.gate_proj": 70.3484115600586,
        "model.layers.0.mlp.up_proj": 60.30656814575195,
        "model.layers.0.mlp.down_proj": 13.707199096679688,
        "model.layers.0.mlp": 13.707199096679688,
        "model.layers.0.post_feedforward_layernorm": 58.44740295410156,
        "model.layers.0": 87.0976791381836,
        "model.layers.1.input_layernorm": 81.68094635009766,
        "model.layers.1.post_attention_layernorm": 15.871463775634766,
        "model.layers.1.pre_feedforward_layernorm": 73.3347396850586,
        "model.layers.1.mlp.gate_proj": 62.09903335571289,
        "model.layers.1.mlp.up_proj": 53.71649169921875,
        "model.layers.1.mlp.down_proj": 4.745572566986084,
        "model.layers.1.mlp": 4.745572566986084,
        "model.layers.1.post_feedforward_layernorm": 35.12009811401367,
        "model.layers.1": 81.40081024169922,
        "model.layers.2.input_layernorm": 71.10618591308594,
        "model.layers.2.post_attention_layernorm": 22.0878849029541,
        "model.layers.2.pre_feedforward_layernorm": 56.30120086669922
      }
    },
    {
      "sequence": "How does encryption work? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.79272993751194,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 41.041603088378906,
        "model.layers.0.pre_feedforward_layernorm": 84.84136199951172,
        "model.layers.0.mlp.gate_proj": 67.14932250976562,
        "model.layers.0.mlp.up_proj": 53.50629425048828,
        "model.layers.0.mlp.down_proj": 10.51580810546875,
        "model.layers.0.mlp": 10.51580810546875,
        "model.layers.0.post_feedforward_layernorm": 55.0814094543457,
        "model.layers.0": 73.2613754272461,
        "model.layers.1.input_layernorm": 74.33555603027344,
        "model.layers.1.post_attention_layernorm": 21.542800903320312,
        "model.layers.1.pre_feedforward_layernorm": 69.34634399414062,
        "model.layers.1.mlp.gate_proj": 63.56462860107422,
        "model.layers.1.mlp.up_proj": 49.53448486328125,
        "model.layers.1.mlp.down_proj": 4.527314186096191,
        "model.layers.1.mlp": 4.527314186096191,
        "model.layers.1.post_feedforward_layernorm": 30.42466163635254,
        "model.layers.1": 71.35338592529297,
        "model.layers.2.input_layernorm": 67.86763763427734,
        "model.layers.2.post_attention_layernorm": 24.994285583496094,
        "model.layers.2.pre_feedforward_layernorm": 56.284385681152344
      }
    },
    {
      "sequence": "What causes climate change? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.79983170136161,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 42.24247741699219,
        "model.layers.0.pre_feedforward_layernorm": 85.14751434326172,
        "model.layers.0.mlp.gate_proj": 68.00167083740234,
        "model.layers.0.mlp.up_proj": 53.93107986450195,
        "model.layers.0.mlp.down_proj": 10.763729095458984,
        "model.layers.0.mlp": 10.763729095458984,
        "model.layers.0.post_feedforward_layernorm": 54.20673370361328,
        "model.layers.0": 73.17957305908203,
        "model.layers.1.input_layernorm": 74.7388687133789,
        "model.layers.1.post_attention_layernorm": 21.503055572509766,
        "model.layers.1.pre_feedforward_layernorm": 70.36576080322266,
        "model.layers.1.mlp.gate_proj": 65.14894104003906,
        "model.layers.1.mlp.up_proj": 48.166629791259766,
        "model.layers.1.mlp.down_proj": 4.29321813583374,
        "model.layers.1.mlp": 4.29321813583374,
        "model.layers.1.post_feedforward_layernorm": 31.486875534057617,
        "model.layers.1": 70.33147430419922,
        "model.layers.2.input_layernorm": 66.58538818359375,
        "model.layers.2.post_attention_layernorm": 23.683679580688477,
        "model.layers.2.pre_feedforward_layernorm": 55.54550552368164
      }
    },
    {
      "sequence": "How do I change a tire?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.80888320052105,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.66290283203125,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 27.94044303894043,
        "model.layers.0.pre_feedforward_layernorm": 82.573974609375,
        "model.layers.0.mlp.gate_proj": 69.46648406982422,
        "model.layers.0.mlp.up_proj": 58.7675666809082,
        "model.layers.0.mlp.down_proj": 13.178333282470703,
        "model.layers.0.mlp": 13.178333282470703,
        "model.layers.0.post_feedforward_layernorm": 58.257789611816406,
        "model.layers.0": 87.79351806640625,
        "model.layers.1.input_layernorm": 81.13907623291016,
        "model.layers.1.post_attention_layernorm": 17.039775848388672,
        "model.layers.1.pre_feedforward_layernorm": 73.76342010498047,
        "model.layers.1.mlp.gate_proj": 59.55061340332031,
        "model.layers.1.mlp.up_proj": 53.511104583740234,
        "model.layers.1.mlp.down_proj": 5.08552885055542,
        "model.layers.1.mlp": 5.08552885055542,
        "model.layers.1.post_feedforward_layernorm": 33.55048751831055,
        "model.layers.1": 82.33213806152344,
        "model.layers.2.input_layernorm": 73.80484771728516,
        "model.layers.2.post_attention_layernorm": 23.67754364013672,
        "model.layers.2.pre_feedforward_layernorm": 57.990997314453125
      }
    },
    {
      "sequence": "How do transformers process language? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.81517773089202,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 41.948856353759766,
        "model.layers.0.pre_feedforward_layernorm": 85.91338348388672,
        "model.layers.0.mlp.gate_proj": 68.80122375488281,
        "model.layers.0.mlp.up_proj": 53.98955154418945,
        "model.layers.0.mlp.down_proj": 10.714886665344238,
        "model.layers.0.mlp": 10.714886665344238,
        "model.layers.0.post_feedforward_layernorm": 55.44050598144531,
        "model.layers.0": 73.90083312988281,
        "model.layers.1.input_layernorm": 74.880859375,
        "model.layers.1.post_attention_layernorm": 20.350801467895508,
        "model.layers.1.pre_feedforward_layernorm": 69.08116149902344,
        "model.layers.1.mlp.gate_proj": 62.73045349121094,
        "model.layers.1.mlp.up_proj": 48.893985748291016,
        "model.layers.1.mlp.down_proj": 4.381538391113281,
        "model.layers.1.mlp": 4.381538391113281,
        "model.layers.1.post_feedforward_layernorm": 29.976322174072266,
        "model.layers.1": 71.53641510009766,
        "model.layers.2.input_layernorm": 66.60189819335938,
        "model.layers.2.post_attention_layernorm": 25.107027053833008,
        "model.layers.2.pre_feedforward_layernorm": 55.71944046020508
      }
    },
    {
      "sequence": "What is the Fibonacci sequence?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.89139662618222,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 30.205516815185547,
        "model.layers.0.pre_feedforward_layernorm": 84.36116790771484,
        "model.layers.0.mlp.gate_proj": 72.60232543945312,
        "model.layers.0.mlp.up_proj": 60.27532196044922,
        "model.layers.0.mlp.down_proj": 13.51974868774414,
        "model.layers.0.mlp": 13.51974868774414,
        "model.layers.0.post_feedforward_layernorm": 58.236385345458984,
        "model.layers.0": 88.0946273803711,
        "model.layers.1.input_layernorm": 81.02223205566406,
        "model.layers.1.post_attention_layernorm": 16.222381591796875,
        "model.layers.1.pre_feedforward_layernorm": 73.08660888671875,
        "model.layers.1.mlp.gate_proj": 60.64116287231445,
        "model.layers.1.mlp.up_proj": 54.08412551879883,
        "model.layers.1.mlp.down_proj": 4.687348365783691,
        "model.layers.1.mlp": 4.687348365783691,
        "model.layers.1.post_feedforward_layernorm": 32.38783645629883,
        "model.layers.1": 79.85054779052734,
        "model.layers.2.input_layernorm": 71.05590057373047,
        "model.layers.2.post_attention_layernorm": 24.015026092529297,
        "model.layers.2.pre_feedforward_layernorm": 57.174617767333984
      }
    },
    {
      "sequence": "How do I change a tire? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.899128976075545,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 40.538883209228516,
        "model.layers.0.pre_feedforward_layernorm": 86.33452606201172,
        "model.layers.0.mlp.gate_proj": 67.52842712402344,
        "model.layers.0.mlp.up_proj": 53.68803787231445,
        "model.layers.0.mlp.down_proj": 10.654539108276367,
        "model.layers.0.mlp": 10.654539108276367,
        "model.layers.0.post_feedforward_layernorm": 53.8149528503418,
        "model.layers.0": 73.21076202392578,
        "model.layers.1.input_layernorm": 74.39903259277344,
        "model.layers.1.post_attention_layernorm": 20.128448486328125,
        "model.layers.1.pre_feedforward_layernorm": 70.42218780517578,
        "model.layers.1.mlp.gate_proj": 64.86331939697266,
        "model.layers.1.mlp.up_proj": 48.73771286010742,
        "model.layers.1.mlp.down_proj": 4.880859375,
        "model.layers.1.mlp": 4.880859375,
        "model.layers.1.post_feedforward_layernorm": 31.424175262451172,
        "model.layers.1": 70.64054107666016,
        "model.layers.2.input_layernorm": 68.09684753417969,
        "model.layers.2.post_attention_layernorm": 24.908124923706055,
        "model.layers.2.pre_feedforward_layernorm": 56.713932037353516
      }
    },
    {
      "sequence": "What is the structure of a sonnet?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.90087562022002,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.8216638565063477,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 30.048290252685547,
        "model.layers.0.pre_feedforward_layernorm": 83.43526458740234,
        "model.layers.0.mlp.gate_proj": 71.03516387939453,
        "model.layers.0.mlp.up_proj": 60.06986999511719,
        "model.layers.0.mlp.down_proj": 13.81277084350586,
        "model.layers.0.mlp": 13.81277084350586,
        "model.layers.0.post_feedforward_layernorm": 58.106224060058594,
        "model.layers.0": 86.71825408935547,
        "model.layers.1.input_layernorm": 82.18242645263672,
        "model.layers.1.post_attention_layernorm": 14.721991539001465,
        "model.layers.1.pre_feedforward_layernorm": 72.42474365234375,
        "model.layers.1.mlp.gate_proj": 61.46969223022461,
        "model.layers.1.mlp.up_proj": 53.470149993896484,
        "model.layers.1.mlp.down_proj": 4.402967929840088,
        "model.layers.1.mlp": 4.402967929840088,
        "model.layers.1.post_feedforward_layernorm": 32.07152557373047,
        "model.layers.1": 81.50078582763672,
        "model.layers.2.input_layernorm": 72.11895751953125,
        "model.layers.2.post_attention_layernorm": 25.769378662109375,
        "model.layers.2.pre_feedforward_layernorm": 58.07037353515625
      }
    },
    {
      "sequence": "How do we define truth? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.910081220709756,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 38.04191589355469,
        "model.layers.0.pre_feedforward_layernorm": 85.36370849609375,
        "model.layers.0.mlp.gate_proj": 65.92475891113281,
        "model.layers.0.mlp.up_proj": 53.081939697265625,
        "model.layers.0.mlp.down_proj": 10.681248664855957,
        "model.layers.0.mlp": 10.681248664855957,
        "model.layers.0.post_feedforward_layernorm": 54.701026916503906,
        "model.layers.0": 72.97858428955078,
        "model.layers.1.input_layernorm": 75.2445068359375,
        "model.layers.1.post_attention_layernorm": 21.1683406829834,
        "model.layers.1.pre_feedforward_layernorm": 70.75128173828125,
        "model.layers.1.mlp.gate_proj": 69.38645935058594,
        "model.layers.1.mlp.up_proj": 49.280948638916016,
        "model.layers.1.mlp.down_proj": 4.494455814361572,
        "model.layers.1.mlp": 4.494455814361572,
        "model.layers.1.post_feedforward_layernorm": 31.170827865600586,
        "model.layers.1": 70.66321563720703,
        "model.layers.2.input_layernorm": 67.39820861816406,
        "model.layers.2.post_attention_layernorm": 25.378530502319336,
        "model.layers.2.pre_feedforward_layernorm": 56.36268615722656
      }
    },
    {
      "sequence": "How did the internet develop? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 45.929845872132674,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 38.666622161865234,
        "model.layers.0.pre_feedforward_layernorm": 87.3571548461914,
        "model.layers.0.mlp.gate_proj": 69.75180053710938,
        "model.layers.0.mlp.up_proj": 53.82242202758789,
        "model.layers.0.mlp.down_proj": 10.087727546691895,
        "model.layers.0.mlp": 10.087727546691895,
        "model.layers.0.post_feedforward_layernorm": 54.882957458496094,
        "model.layers.0": 73.49319458007812,
        "model.layers.1.input_layernorm": 75.4118881225586,
        "model.layers.1.post_attention_layernorm": 20.85227394104004,
        "model.layers.1.pre_feedforward_layernorm": 70.96551513671875,
        "model.layers.1.mlp.gate_proj": 62.19219970703125,
        "model.layers.1.mlp.up_proj": 49.57547378540039,
        "model.layers.1.mlp.down_proj": 4.731666564941406,
        "model.layers.1.mlp": 4.731666564941406,
        "model.layers.1.post_feedforward_layernorm": 30.97344970703125,
        "model.layers.1": 71.156494140625,
        "model.layers.2.input_layernorm": 68.39014434814453,
        "model.layers.2.post_attention_layernorm": 24.716646194458008,
        "model.layers.2.pre_feedforward_layernorm": 55.85591125488281
      }
    },
    {
      "sequence": "What is consciousness? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.00791835784912,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 41.50248718261719,
        "model.layers.0.pre_feedforward_layernorm": 84.79356384277344,
        "model.layers.0.mlp.gate_proj": 65.89557647705078,
        "model.layers.0.mlp.up_proj": 53.983219146728516,
        "model.layers.0.mlp.down_proj": 11.10218334197998,
        "model.layers.0.mlp": 11.10218334197998,
        "model.layers.0.post_feedforward_layernorm": 54.29874038696289,
        "model.layers.0": 73.31033325195312,
        "model.layers.1.input_layernorm": 75.37571716308594,
        "model.layers.1.post_attention_layernorm": 20.721345901489258,
        "model.layers.1.pre_feedforward_layernorm": 69.66645050048828,
        "model.layers.1.mlp.gate_proj": 66.89160919189453,
        "model.layers.1.mlp.up_proj": 49.05229949951172,
        "model.layers.1.mlp.down_proj": 4.471641540527344,
        "model.layers.1.mlp": 4.471641540527344,
        "model.layers.1.post_feedforward_layernorm": 31.389841079711914,
        "model.layers.1": 69.86146545410156,
        "model.layers.2.input_layernorm": 66.8491439819336,
        "model.layers.2.post_attention_layernorm": 26.801555633544922,
        "model.layers.2.pre_feedforward_layernorm": 57.32234191894531
      }
    },
    {
      "sequence": "What are the pros and cons? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.041483941285506,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 41.2564811706543,
        "model.layers.0.pre_feedforward_layernorm": 84.48373413085938,
        "model.layers.0.mlp.gate_proj": 70.32764434814453,
        "model.layers.0.mlp.up_proj": 53.325439453125,
        "model.layers.0.mlp.down_proj": 10.242383003234863,
        "model.layers.0.mlp": 10.242383003234863,
        "model.layers.0.post_feedforward_layernorm": 53.61207962036133,
        "model.layers.0": 72.60179901123047,
        "model.layers.1.input_layernorm": 75.37545013427734,
        "model.layers.1.post_attention_layernorm": 20.746997833251953,
        "model.layers.1.pre_feedforward_layernorm": 70.712158203125,
        "model.layers.1.mlp.gate_proj": 65.95381927490234,
        "model.layers.1.mlp.up_proj": 49.611202239990234,
        "model.layers.1.mlp.down_proj": 5.0257978439331055,
        "model.layers.1.mlp": 5.0257978439331055,
        "model.layers.1.post_feedforward_layernorm": 30.724079132080078,
        "model.layers.1": 70.5575180053711,
        "model.layers.2.input_layernorm": 67.71759796142578,
        "model.layers.2.post_attention_layernorm": 26.197589874267578,
        "model.layers.2.pre_feedforward_layernorm": 56.0549201965332
      }
    },
    {
      "sequence": "How do vaccines work?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.074104143225625,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 28.947275161743164,
        "model.layers.0.pre_feedforward_layernorm": 84.20841979980469,
        "model.layers.0.mlp.gate_proj": 70.48562622070312,
        "model.layers.0.mlp.up_proj": 59.47819900512695,
        "model.layers.0.mlp.down_proj": 12.959528923034668,
        "model.layers.0.mlp": 12.959528923034668,
        "model.layers.0.post_feedforward_layernorm": 58.476654052734375,
        "model.layers.0": 87.96998596191406,
        "model.layers.1.input_layernorm": 81.7616195678711,
        "model.layers.1.post_attention_layernorm": 18.760648727416992,
        "model.layers.1.pre_feedforward_layernorm": 73.39417266845703,
        "model.layers.1.mlp.gate_proj": 60.22654342651367,
        "model.layers.1.mlp.up_proj": 53.70773696899414,
        "model.layers.1.mlp.down_proj": 4.974112033843994,
        "model.layers.1.mlp": 4.974112033843994,
        "model.layers.1.post_feedforward_layernorm": 33.760948181152344,
        "model.layers.1": 82.4028549194336,
        "model.layers.2.input_layernorm": 73.22071075439453,
        "model.layers.2.post_attention_layernorm": 23.549407958984375,
        "model.layers.2.pre_feedforward_layernorm": 57.338687896728516
      }
    },
    {
      "sequence": "How does music theory work? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.107076126596205,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 41.85995101928711,
        "model.layers.0.pre_feedforward_layernorm": 84.33472442626953,
        "model.layers.0.mlp.gate_proj": 68.29922485351562,
        "model.layers.0.mlp.up_proj": 53.2133903503418,
        "model.layers.0.mlp.down_proj": 10.426387786865234,
        "model.layers.0.mlp": 10.426387786865234,
        "model.layers.0.post_feedforward_layernorm": 54.654945373535156,
        "model.layers.0": 74.0606460571289,
        "model.layers.1.input_layernorm": 75.20912170410156,
        "model.layers.1.post_attention_layernorm": 22.392024993896484,
        "model.layers.1.pre_feedforward_layernorm": 71.31375885009766,
        "model.layers.1.mlp.gate_proj": 66.2983627319336,
        "model.layers.1.mlp.up_proj": 49.309505462646484,
        "model.layers.1.mlp.down_proj": 4.421535491943359,
        "model.layers.1.mlp": 4.421535491943359,
        "model.layers.1.post_feedforward_layernorm": 30.940336227416992,
        "model.layers.1": 71.92134094238281,
        "model.layers.2.input_layernorm": 67.77455139160156,
        "model.layers.2.post_attention_layernorm": 24.338857650756836,
        "model.layers.2.pre_feedforward_layernorm": 56.16264343261719
      }
    },
    {
      "sequence": "What is artificial intelligence? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.117503435715385,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 43.05112075805664,
        "model.layers.0.pre_feedforward_layernorm": 84.29086303710938,
        "model.layers.0.mlp.gate_proj": 69.56474304199219,
        "model.layers.0.mlp.up_proj": 53.867855072021484,
        "model.layers.0.mlp.down_proj": 10.64426326751709,
        "model.layers.0.mlp": 10.64426326751709,
        "model.layers.0.post_feedforward_layernorm": 54.036991119384766,
        "model.layers.0": 73.45474243164062,
        "model.layers.1.input_layernorm": 75.38597106933594,
        "model.layers.1.post_attention_layernorm": 21.1710147857666,
        "model.layers.1.pre_feedforward_layernorm": 70.02159118652344,
        "model.layers.1.mlp.gate_proj": 64.670654296875,
        "model.layers.1.mlp.up_proj": 49.2116813659668,
        "model.layers.1.mlp.down_proj": 4.809244632720947,
        "model.layers.1.mlp": 4.809244632720947,
        "model.layers.1.post_feedforward_layernorm": 30.63296127319336,
        "model.layers.1": 69.87309265136719,
        "model.layers.2.input_layernorm": 67.02811431884766,
        "model.layers.2.post_attention_layernorm": 27.27858543395996,
        "model.layers.2.pre_feedforward_layernorm": 57.23857498168945
      }
    },
    {
      "sequence": "What caused World War II?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.13225627982098,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 31.038330078125,
        "model.layers.0.pre_feedforward_layernorm": 84.5640640258789,
        "model.layers.0.mlp.gate_proj": 74.37040710449219,
        "model.layers.0.mlp.up_proj": 59.98357391357422,
        "model.layers.0.mlp.down_proj": 12.6848783493042,
        "model.layers.0.mlp": 12.6848783493042,
        "model.layers.0.post_feedforward_layernorm": 57.91321563720703,
        "model.layers.0": 87.50712585449219,
        "model.layers.1.input_layernorm": 82.0838623046875,
        "model.layers.1.post_attention_layernorm": 16.913536071777344,
        "model.layers.1.pre_feedforward_layernorm": 74.65416717529297,
        "model.layers.1.mlp.gate_proj": 60.03103256225586,
        "model.layers.1.mlp.up_proj": 53.7236213684082,
        "model.layers.1.mlp.down_proj": 4.968447685241699,
        "model.layers.1.mlp": 4.968447685241699,
        "model.layers.1.post_feedforward_layernorm": 32.35441589355469,
        "model.layers.1": 80.3357162475586,
        "model.layers.2.input_layernorm": 72.88260650634766,
        "model.layers.2.post_attention_layernorm": 23.550024032592773,
        "model.layers.2.pre_feedforward_layernorm": 58.05739974975586
      }
    },
    {
      "sequence": "What is consciousness?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.15613123644953,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.6975889205932617,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 30.80280303955078,
        "model.layers.0.pre_feedforward_layernorm": 84.47404479980469,
        "model.layers.0.mlp.gate_proj": 72.092041015625,
        "model.layers.0.mlp.up_proj": 60.39896774291992,
        "model.layers.0.mlp.down_proj": 13.780625343322754,
        "model.layers.0.mlp": 13.780625343322754,
        "model.layers.0.post_feedforward_layernorm": 58.65270233154297,
        "model.layers.0": 87.6727523803711,
        "model.layers.1.input_layernorm": 82.25715637207031,
        "model.layers.1.post_attention_layernorm": 16.12541389465332,
        "model.layers.1.pre_feedforward_layernorm": 72.32369995117188,
        "model.layers.1.mlp.gate_proj": 60.070316314697266,
        "model.layers.1.mlp.up_proj": 52.86408996582031,
        "model.layers.1.mlp.down_proj": 4.501030445098877,
        "model.layers.1.mlp": 4.501030445098877,
        "model.layers.1.post_feedforward_layernorm": 32.29010772705078,
        "model.layers.1": 81.20472717285156,
        "model.layers.2.input_layernorm": 72.46595001220703,
        "model.layers.2.post_attention_layernorm": 27.08176040649414,
        "model.layers.2.pre_feedforward_layernorm": 58.299678802490234
      }
    },
    {
      "sequence": "What is artificial intelligence?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.16917701389479,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 31.520599365234375,
        "model.layers.0.pre_feedforward_layernorm": 84.38240814208984,
        "model.layers.0.mlp.gate_proj": 73.4887924194336,
        "model.layers.0.mlp.up_proj": 59.88140869140625,
        "model.layers.0.mlp.down_proj": 13.260870933532715,
        "model.layers.0.mlp": 13.260870933532715,
        "model.layers.0.post_feedforward_layernorm": 58.394004821777344,
        "model.layers.0": 87.80827331542969,
        "model.layers.1.input_layernorm": 82.02357482910156,
        "model.layers.1.post_attention_layernorm": 17.50071907043457,
        "model.layers.1.pre_feedforward_layernorm": 72.7110595703125,
        "model.layers.1.mlp.gate_proj": 58.52790451049805,
        "model.layers.1.mlp.up_proj": 53.244747161865234,
        "model.layers.1.mlp.down_proj": 4.800218105316162,
        "model.layers.1.mlp": 4.800218105316162,
        "model.layers.1.post_feedforward_layernorm": 31.566410064697266,
        "model.layers.1": 81.77654266357422,
        "model.layers.2.input_layernorm": 71.93051147460938,
        "model.layers.2.post_attention_layernorm": 27.273639678955078,
        "model.layers.2.pre_feedforward_layernorm": 57.590675354003906
      }
    },
    {
      "sequence": "How does exercise affect health? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.179980257283084,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 42.40665054321289,
        "model.layers.0.pre_feedforward_layernorm": 84.07769012451172,
        "model.layers.0.mlp.gate_proj": 70.48350524902344,
        "model.layers.0.mlp.up_proj": 53.45511245727539,
        "model.layers.0.mlp.down_proj": 10.456645965576172,
        "model.layers.0.mlp": 10.456645965576172,
        "model.layers.0.post_feedforward_layernorm": 54.28561782836914,
        "model.layers.0": 74.47821807861328,
        "model.layers.1.input_layernorm": 74.60044860839844,
        "model.layers.1.post_attention_layernorm": 22.441478729248047,
        "model.layers.1.pre_feedforward_layernorm": 70.32845306396484,
        "model.layers.1.mlp.gate_proj": 67.23335266113281,
        "model.layers.1.mlp.up_proj": 48.546451568603516,
        "model.layers.1.mlp.down_proj": 4.561524868011475,
        "model.layers.1.mlp": 4.561524868011475,
        "model.layers.1.post_feedforward_layernorm": 31.114892959594727,
        "model.layers.1": 71.65050506591797,
        "model.layers.2.input_layernorm": 67.53118896484375,
        "model.layers.2.post_attention_layernorm": 24.560758590698242,
        "model.layers.2.pre_feedforward_layernorm": 56.22536087036133
      }
    },
    {
      "sequence": "How do vaccines work? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.20331778733627,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 41.99802780151367,
        "model.layers.0.pre_feedforward_layernorm": 84.92374420166016,
        "model.layers.0.mlp.gate_proj": 68.13896179199219,
        "model.layers.0.mlp.up_proj": 53.846778869628906,
        "model.layers.0.mlp.down_proj": 10.548152923583984,
        "model.layers.0.mlp": 10.548152923583984,
        "model.layers.0.post_feedforward_layernorm": 54.69740676879883,
        "model.layers.0": 73.08418273925781,
        "model.layers.1.input_layernorm": 74.83128356933594,
        "model.layers.1.post_attention_layernorm": 22.22141456604004,
        "model.layers.1.pre_feedforward_layernorm": 70.93682861328125,
        "model.layers.1.mlp.gate_proj": 67.10286712646484,
        "model.layers.1.mlp.up_proj": 49.95143508911133,
        "model.layers.1.mlp.down_proj": 4.762905597686768,
        "model.layers.1.mlp": 4.762905597686768,
        "model.layers.1.post_feedforward_layernorm": 30.86229705810547,
        "model.layers.1": 71.3963394165039,
        "model.layers.2.input_layernorm": 68.1181640625,
        "model.layers.2.post_attention_layernorm": 24.3447322845459,
        "model.layers.2.pre_feedforward_layernorm": 56.58272171020508
      }
    },
    {
      "sequence": "What are investment strategies? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.205253994983174,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 41.267311096191406,
        "model.layers.0.pre_feedforward_layernorm": 84.80732727050781,
        "model.layers.0.mlp.gate_proj": 67.47624969482422,
        "model.layers.0.mlp.up_proj": 53.54490661621094,
        "model.layers.0.mlp.down_proj": 10.76478385925293,
        "model.layers.0.mlp": 10.76478385925293,
        "model.layers.0.post_feedforward_layernorm": 53.90428161621094,
        "model.layers.0": 73.85413360595703,
        "model.layers.1.input_layernorm": 74.6424789428711,
        "model.layers.1.post_attention_layernorm": 22.56250762939453,
        "model.layers.1.pre_feedforward_layernorm": 70.65498352050781,
        "model.layers.1.mlp.gate_proj": 68.6938247680664,
        "model.layers.1.mlp.up_proj": 48.542842864990234,
        "model.layers.1.mlp.down_proj": 4.634326457977295,
        "model.layers.1.mlp": 4.634326457977295,
        "model.layers.1.post_feedforward_layernorm": 31.644872665405273,
        "model.layers.1": 70.62986755371094,
        "model.layers.2.input_layernorm": 67.32675170898438,
        "model.layers.2.post_attention_layernorm": 26.937633514404297,
        "model.layers.2.pre_feedforward_layernorm": 56.41564178466797
      }
    },
    {
      "sequence": "What is version control?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.25037259640901,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.8937160968780518,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 30.722686767578125,
        "model.layers.0.pre_feedforward_layernorm": 82.44342041015625,
        "model.layers.0.mlp.gate_proj": 71.90408325195312,
        "model.layers.0.mlp.up_proj": 59.001163482666016,
        "model.layers.0.mlp.down_proj": 12.780120849609375,
        "model.layers.0.mlp": 12.780120849609375,
        "model.layers.0.post_feedforward_layernorm": 58.650962829589844,
        "model.layers.0": 89.14080810546875,
        "model.layers.1.input_layernorm": 81.41825866699219,
        "model.layers.1.post_attention_layernorm": 16.464927673339844,
        "model.layers.1.pre_feedforward_layernorm": 74.64871215820312,
        "model.layers.1.mlp.gate_proj": 61.59687042236328,
        "model.layers.1.mlp.up_proj": 54.38923645019531,
        "model.layers.1.mlp.down_proj": 4.952042102813721,
        "model.layers.1.mlp": 4.952042102813721,
        "model.layers.1.post_feedforward_layernorm": 30.432849884033203,
        "model.layers.1": 81.98101806640625,
        "model.layers.2.input_layernorm": 72.8747787475586,
        "model.layers.2.post_attention_layernorm": 27.78050994873047,
        "model.layers.2.pre_feedforward_layernorm": 58.69633483886719
      }
    },
    {
      "sequence": "What is the meaning of life? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.250389534494154,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 41.29558563232422,
        "model.layers.0.pre_feedforward_layernorm": 86.56816101074219,
        "model.layers.0.mlp.gate_proj": 69.87447357177734,
        "model.layers.0.mlp.up_proj": 53.77896499633789,
        "model.layers.0.mlp.down_proj": 10.895523071289062,
        "model.layers.0.mlp": 10.895523071289062,
        "model.layers.0.post_feedforward_layernorm": 54.49451446533203,
        "model.layers.0": 73.07193756103516,
        "model.layers.1.input_layernorm": 75.88382720947266,
        "model.layers.1.post_attention_layernorm": 20.4918212890625,
        "model.layers.1.pre_feedforward_layernorm": 71.99246215820312,
        "model.layers.1.mlp.gate_proj": 67.31195068359375,
        "model.layers.1.mlp.up_proj": 50.44208526611328,
        "model.layers.1.mlp.down_proj": 5.275506973266602,
        "model.layers.1.mlp": 5.275506973266602,
        "model.layers.1.post_feedforward_layernorm": 29.976396560668945,
        "model.layers.1": 69.47425842285156,
        "model.layers.2.input_layernorm": 67.4142837524414,
        "model.layers.2.post_attention_layernorm": 23.672048568725586,
        "model.layers.2.pre_feedforward_layernorm": 56.514869689941406
      }
    },
    {
      "sequence": "What is the Fibonacci sequence? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.35283534423165,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 44.079246520996094,
        "model.layers.0.pre_feedforward_layernorm": 83.964599609375,
        "model.layers.0.mlp.gate_proj": 68.61860656738281,
        "model.layers.0.mlp.up_proj": 54.037811279296875,
        "model.layers.0.mlp.down_proj": 10.86668586730957,
        "model.layers.0.mlp": 10.86668586730957,
        "model.layers.0.post_feedforward_layernorm": 53.981849670410156,
        "model.layers.0": 73.40443420410156,
        "model.layers.1.input_layernorm": 74.5886459350586,
        "model.layers.1.post_attention_layernorm": 21.420019149780273,
        "model.layers.1.pre_feedforward_layernorm": 70.19866180419922,
        "model.layers.1.mlp.gate_proj": 67.40314483642578,
        "model.layers.1.mlp.up_proj": 49.794342041015625,
        "model.layers.1.mlp.down_proj": 4.684544563293457,
        "model.layers.1.mlp": 4.684544563293457,
        "model.layers.1.post_feedforward_layernorm": 32.67604064941406,
        "model.layers.1": 70.69408416748047,
        "model.layers.2.input_layernorm": 67.95515441894531,
        "model.layers.2.post_attention_layernorm": 26.22369384765625,
        "model.layers.2.pre_feedforward_layernorm": 57.28889846801758
      }
    },
    {
      "sequence": "Can you help me understand this? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.43664996520333,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 35.81791305541992,
        "model.layers.0.pre_feedforward_layernorm": 87.06922149658203,
        "model.layers.0.mlp.gate_proj": 69.68399810791016,
        "model.layers.0.mlp.up_proj": 53.5198974609375,
        "model.layers.0.mlp.down_proj": 10.141447067260742,
        "model.layers.0.mlp": 10.141447067260742,
        "model.layers.0.post_feedforward_layernorm": 54.04481887817383,
        "model.layers.0": 75.31588745117188,
        "model.layers.1.input_layernorm": 76.07959747314453,
        "model.layers.1.post_attention_layernorm": 21.66244888305664,
        "model.layers.1.pre_feedforward_layernorm": 72.08827209472656,
        "model.layers.1.mlp.gate_proj": 62.96332931518555,
        "model.layers.1.mlp.up_proj": 50.595611572265625,
        "model.layers.1.mlp.down_proj": 5.378811359405518,
        "model.layers.1.mlp": 5.378811359405518,
        "model.layers.1.post_feedforward_layernorm": 33.248130798339844,
        "model.layers.1": 73.050537109375,
        "model.layers.2.input_layernorm": 69.4633560180664,
        "model.layers.2.post_attention_layernorm": 26.054607391357422,
        "model.layers.2.pre_feedforward_layernorm": 57.185546875
      }
    },
    {
      "sequence": "What is version control? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.468631848045014,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 41.149131774902344,
        "model.layers.0.pre_feedforward_layernorm": 84.28659057617188,
        "model.layers.0.mlp.gate_proj": 67.48657989501953,
        "model.layers.0.mlp.up_proj": 53.11238098144531,
        "model.layers.0.mlp.down_proj": 10.22549057006836,
        "model.layers.0.mlp": 10.22549057006836,
        "model.layers.0.post_feedforward_layernorm": 54.7288818359375,
        "model.layers.0": 74.61660766601562,
        "model.layers.1.input_layernorm": 75.55630493164062,
        "model.layers.1.post_attention_layernorm": 22.19847869873047,
        "model.layers.1.pre_feedforward_layernorm": 71.05694580078125,
        "model.layers.1.mlp.gate_proj": 65.82089233398438,
        "model.layers.1.mlp.up_proj": 49.97423553466797,
        "model.layers.1.mlp.down_proj": 5.303340911865234,
        "model.layers.1.mlp": 5.303340911865234,
        "model.layers.1.post_feedforward_layernorm": 31.236736297607422,
        "model.layers.1": 71.5615005493164,
        "model.layers.2.input_layernorm": 69.04151153564453,
        "model.layers.2.post_attention_layernorm": 28.666112899780273,
        "model.layers.2.pre_feedforward_layernorm": 58.21097183227539
      }
    },
    {
      "sequence": "What was the Industrial Revolution?          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.63334877594657,
      "layer_variances": {
        "model.embed_tokens": 2.0919339656829834,
        "model.rotary_emb": 2.518237829208374,
        "model.layers.0.input_layernorm": 71.16197204589844,
        "model.layers.0.post_attention_layernorm": 30.401315689086914,
        "model.layers.0.pre_feedforward_layernorm": 85.63516998291016,
        "model.layers.0.mlp.gate_proj": 73.3731460571289,
        "model.layers.0.mlp.up_proj": 60.42169189453125,
        "model.layers.0.mlp.down_proj": 13.44297981262207,
        "model.layers.0.mlp": 13.44297981262207,
        "model.layers.0.post_feedforward_layernorm": 59.13103485107422,
        "model.layers.0": 88.38074493408203,
        "model.layers.1.input_layernorm": 82.92887115478516,
        "model.layers.1.post_attention_layernorm": 16.94545555114746,
        "model.layers.1.pre_feedforward_layernorm": 74.34133911132812,
        "model.layers.1.mlp.gate_proj": 62.36722946166992,
        "model.layers.1.mlp.up_proj": 53.979957580566406,
        "model.layers.1.mlp.down_proj": 4.657183647155762,
        "model.layers.1.mlp": 4.657183647155762,
        "model.layers.1.post_feedforward_layernorm": 34.049442291259766,
        "model.layers.1": 82.51934051513672,
        "model.layers.2.input_layernorm": 72.77249908447266,
        "model.layers.2.post_attention_layernorm": 25.87942886352539,
        "model.layers.2.pre_feedforward_layernorm": 57.4678840637207
      }
    },
    {
      "sequence": "What caused World War II? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.65293886350549,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 39.8693733215332,
        "model.layers.0.pre_feedforward_layernorm": 88.96057891845703,
        "model.layers.0.mlp.gate_proj": 70.8191146850586,
        "model.layers.0.mlp.up_proj": 54.92746353149414,
        "model.layers.0.mlp.down_proj": 10.505473136901855,
        "model.layers.0.mlp": 10.505473136901855,
        "model.layers.0.post_feedforward_layernorm": 54.893798828125,
        "model.layers.0": 72.1629638671875,
        "model.layers.1.input_layernorm": 74.74038696289062,
        "model.layers.1.post_attention_layernorm": 22.494943618774414,
        "model.layers.1.pre_feedforward_layernorm": 75.75511169433594,
        "model.layers.1.mlp.gate_proj": 70.70368957519531,
        "model.layers.1.mlp.up_proj": 50.436527252197266,
        "model.layers.1.mlp.down_proj": 4.935800075531006,
        "model.layers.1.mlp": 4.935800075531006,
        "model.layers.1.post_feedforward_layernorm": 30.38518714904785,
        "model.layers.1": 69.84141540527344,
        "model.layers.2.input_layernorm": 67.567626953125,
        "model.layers.2.post_attention_layernorm": 23.47878074645996,
        "model.layers.2.pre_feedforward_layernorm": 56.41456604003906
      }
    },
    {
      "sequence": "How do you solve quadratic equations? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.73211991268656,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 44.070369720458984,
        "model.layers.0.pre_feedforward_layernorm": 83.17662048339844,
        "model.layers.0.mlp.gate_proj": 68.42198181152344,
        "model.layers.0.mlp.up_proj": 53.81352996826172,
        "model.layers.0.mlp.down_proj": 11.159186363220215,
        "model.layers.0.mlp": 11.159186363220215,
        "model.layers.0.post_feedforward_layernorm": 54.98698425292969,
        "model.layers.0": 74.47248840332031,
        "model.layers.1.input_layernorm": 74.67488098144531,
        "model.layers.1.post_attention_layernorm": 22.74605369567871,
        "model.layers.1.pre_feedforward_layernorm": 71.25096130371094,
        "model.layers.1.mlp.gate_proj": 70.90015411376953,
        "model.layers.1.mlp.up_proj": 49.360565185546875,
        "model.layers.1.mlp.down_proj": 4.504518985748291,
        "model.layers.1.mlp": 4.504518985748291,
        "model.layers.1.post_feedforward_layernorm": 33.931175231933594,
        "model.layers.1": 72.72567749023438,
        "model.layers.2.input_layernorm": 67.7942886352539,
        "model.layers.2.post_attention_layernorm": 25.321313858032227,
        "model.layers.2.pre_feedforward_layernorm": 56.70504379272461
      }
    },
    {
      "sequence": "What is the structure of a sonnet? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.73511959158856,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.063426971435547,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 49.798309326171875,
        "model.layers.0.pre_feedforward_layernorm": 80.18794250488281,
        "model.layers.0.mlp.gate_proj": 67.00283813476562,
        "model.layers.0.mlp.up_proj": 53.642242431640625,
        "model.layers.0.mlp.down_proj": 10.959200859069824,
        "model.layers.0.mlp": 10.959200859069824,
        "model.layers.0.post_feedforward_layernorm": 54.139801025390625,
        "model.layers.0": 71.85591888427734,
        "model.layers.1.input_layernorm": 74.71793365478516,
        "model.layers.1.post_attention_layernorm": 22.607528686523438,
        "model.layers.1.pre_feedforward_layernorm": 71.76349639892578,
        "model.layers.1.mlp.gate_proj": 74.25035858154297,
        "model.layers.1.mlp.up_proj": 50.2176399230957,
        "model.layers.1.mlp.down_proj": 4.461338996887207,
        "model.layers.1.mlp": 4.461338996887207,
        "model.layers.1.post_feedforward_layernorm": 32.99208068847656,
        "model.layers.1": 71.23652648925781,
        "model.layers.2.input_layernorm": 67.63192749023438,
        "model.layers.2.post_attention_layernorm": 25.679122924804688,
        "model.layers.2.pre_feedforward_layernorm": 57.3407096862793
      }
    },
    {
      "sequence": "What was the Industrial Revolution? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.92207641186921,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 42.249671936035156,
        "model.layers.0.pre_feedforward_layernorm": 86.42552185058594,
        "model.layers.0.mlp.gate_proj": 72.1814193725586,
        "model.layers.0.mlp.up_proj": 55.074005126953125,
        "model.layers.0.mlp.down_proj": 10.828680992126465,
        "model.layers.0.mlp": 10.828680992126465,
        "model.layers.0.post_feedforward_layernorm": 54.6387939453125,
        "model.layers.0": 73.19244384765625,
        "model.layers.1.input_layernorm": 75.47644805908203,
        "model.layers.1.post_attention_layernorm": 22.247615814208984,
        "model.layers.1.pre_feedforward_layernorm": 73.78925323486328,
        "model.layers.1.mlp.gate_proj": 72.65193939208984,
        "model.layers.1.mlp.up_proj": 50.12614822387695,
        "model.layers.1.mlp.down_proj": 4.481243133544922,
        "model.layers.1.mlp": 4.481243133544922,
        "model.layers.1.post_feedforward_layernorm": 31.350576400756836,
        "model.layers.1": 70.65856170654297,
        "model.layers.2.input_layernorm": 67.2651596069336,
        "model.layers.2.post_attention_layernorm": 25.81342315673828,
        "model.layers.2.pre_feedforward_layernorm": 56.76340866088867
      }
    },
    {
      "sequence": "What are the applications of deep learning? !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 46.988397867783256,
      "layer_variances": {
        "model.embed_tokens": 2.199815273284912,
        "model.rotary_emb": 5.063426971435547,
        "model.layers.0.input_layernorm": 111.73905181884766,
        "model.layers.0.post_attention_layernorm": 43.99165725708008,
        "model.layers.0.pre_feedforward_layernorm": 85.18576049804688,
        "model.layers.0.mlp.gate_proj": 74.22129821777344,
        "model.layers.0.mlp.up_proj": 54.33427810668945,
        "model.layers.0.mlp.down_proj": 10.313669204711914,
        "model.layers.0.mlp": 10.313669204711914,
        "model.layers.0.post_feedforward_layernorm": 54.64327621459961,
        "model.layers.0": 73.73307800292969,
        "model.layers.1.input_layernorm": 75.4490737915039,
        "model.layers.1.post_attention_layernorm": 22.918691635131836,
        "model.layers.1.pre_feedforward_layernorm": 73.392822265625,
        "model.layers.1.mlp.gate_proj": 69.47676849365234,
        "model.layers.1.mlp.up_proj": 50.67484664916992,
        "model.layers.1.mlp.down_proj": 5.21338415145874,
        "model.layers.1.mlp": 5.21338415145874,
        "model.layers.1.post_feedforward_layernorm": 31.059358596801758,
        "model.layers.1": 72.11312103271484,
        "model.layers.2.input_layernorm": 67.82201385498047,
        "model.layers.2.post_attention_layernorm": 25.279430389404297,
        "model.layers.2.pre_feedforward_layernorm": 56.38127517700195
      }
    },
    {
      "sequence": "Repeat this word: hello hello hello hello !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 49.09604267452074,
      "layer_variances": {
        "model.embed_tokens": 2.1338348388671875,
        "model.rotary_emb": 5.063426971435547,
        "model.layers.0.input_layernorm": 77.11831665039062,
        "model.layers.0.post_attention_layernorm": 18.190006256103516,
        "model.layers.0.pre_feedforward_layernorm": 79.42194366455078,
        "model.layers.0.mlp.gate_proj": 63.007484436035156,
        "model.layers.0.mlp.up_proj": 59.18971633911133,
        "model.layers.0.mlp.down_proj": 6.633962154388428,
        "model.layers.0.mlp": 6.633962154388428,
        "model.layers.0.post_feedforward_layernorm": 46.92295455932617,
        "model.layers.0": 101.60824584960938,
        "model.layers.1.input_layernorm": 97.69483947753906,
        "model.layers.1.post_attention_layernorm": 22.84647560119629,
        "model.layers.1.pre_feedforward_layernorm": 82.37650299072266,
        "model.layers.1.mlp.gate_proj": 65.15945434570312,
        "model.layers.1.mlp.up_proj": 61.55071258544922,
        "model.layers.1.mlp.down_proj": 4.054780960083008,
        "model.layers.1.mlp": 4.054780960083008,
        "model.layers.1.post_feedforward_layernorm": 35.0675163269043,
        "model.layers.1": 97.99826049804688,
        "model.layers.2.input_layernorm": 88.21597290039062,
        "model.layers.2.post_attention_layernorm": 39.176292419433594,
        "model.layers.2.pre_feedforward_layernorm": 65.08953857421875
      }
    },
    {
      "sequence": "Describe time management methods. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 49.89508671345918,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 49.5418586730957,
        "model.layers.0.pre_feedforward_layernorm": 96.2482681274414,
        "model.layers.0.mlp.gate_proj": 80.9826889038086,
        "model.layers.0.mlp.up_proj": 60.056640625,
        "model.layers.0.mlp.down_proj": 11.049097061157227,
        "model.layers.0.mlp": 11.049097061157227,
        "model.layers.0.post_feedforward_layernorm": 55.763614654541016,
        "model.layers.0": 82.71922302246094,
        "model.layers.1.input_layernorm": 81.6341323852539,
        "model.layers.1.post_attention_layernorm": 21.293437957763672,
        "model.layers.1.pre_feedforward_layernorm": 70.0141830444336,
        "model.layers.1.mlp.gate_proj": 61.35988998413086,
        "model.layers.1.mlp.up_proj": 49.59720230102539,
        "model.layers.1.mlp.down_proj": 4.4138007164001465,
        "model.layers.1.mlp": 4.4138007164001465,
        "model.layers.1.post_feedforward_layernorm": 32.190277099609375,
        "model.layers.1": 76.59908294677734,
        "model.layers.2.input_layernorm": 75.14356994628906,
        "model.layers.2.post_attention_layernorm": 27.972753524780273,
        "model.layers.2.pre_feedforward_layernorm": 58.96250915527344
      }
    },
    {
      "sequence": "Explain how neural networks work. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 49.956723845523335,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 52.03644561767578,
        "model.layers.0.pre_feedforward_layernorm": 95.74779510498047,
        "model.layers.0.mlp.gate_proj": 82.00274658203125,
        "model.layers.0.mlp.up_proj": 60.66732406616211,
        "model.layers.0.mlp.down_proj": 11.789407730102539,
        "model.layers.0.mlp": 11.789407730102539,
        "model.layers.0.post_feedforward_layernorm": 55.67617416381836,
        "model.layers.0": 82.2226333618164,
        "model.layers.1.input_layernorm": 81.84358978271484,
        "model.layers.1.post_attention_layernorm": 20.231050491333008,
        "model.layers.1.pre_feedforward_layernorm": 69.09017181396484,
        "model.layers.1.mlp.gate_proj": 62.353515625,
        "model.layers.1.mlp.up_proj": 49.57141876220703,
        "model.layers.1.mlp.down_proj": 4.23590087890625,
        "model.layers.1.mlp": 4.23590087890625,
        "model.layers.1.post_feedforward_layernorm": 32.03253936767578,
        "model.layers.1": 76.93113708496094,
        "model.layers.2.input_layernorm": 74.41565704345703,
        "model.layers.2.post_attention_layernorm": 27.581146240234375,
        "model.layers.2.pre_feedforward_layernorm": 58.30230712890625
      }
    },
    {
      "sequence": "Explain quantum computing principles. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.00671702882518,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 51.683650970458984,
        "model.layers.0.pre_feedforward_layernorm": 95.77357482910156,
        "model.layers.0.mlp.gate_proj": 82.51138305664062,
        "model.layers.0.mlp.up_proj": 61.02463150024414,
        "model.layers.0.mlp.down_proj": 11.70851993560791,
        "model.layers.0.mlp": 11.70851993560791,
        "model.layers.0.post_feedforward_layernorm": 56.05546188354492,
        "model.layers.0": 82.54423522949219,
        "model.layers.1.input_layernorm": 81.75638580322266,
        "model.layers.1.post_attention_layernorm": 20.52410125732422,
        "model.layers.1.pre_feedforward_layernorm": 69.42313385009766,
        "model.layers.1.mlp.gate_proj": 62.11091613769531,
        "model.layers.1.mlp.up_proj": 49.63103485107422,
        "model.layers.1.mlp.down_proj": 4.306325435638428,
        "model.layers.1.mlp": 4.306325435638428,
        "model.layers.1.post_feedforward_layernorm": 30.827533721923828,
        "model.layers.1": 77.45063018798828,
        "model.layers.2.input_layernorm": 73.69776916503906,
        "model.layers.2.post_attention_layernorm": 28.58649444580078,
        "model.layers.2.pre_feedforward_layernorm": 57.94199752807617
      }
    },
    {
      "sequence": "Explain cooking techniques. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.01615009100541,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 51.13014602661133,
        "model.layers.0.pre_feedforward_layernorm": 95.01698303222656,
        "model.layers.0.mlp.gate_proj": 81.11149597167969,
        "model.layers.0.mlp.up_proj": 60.368221282958984,
        "model.layers.0.mlp.down_proj": 11.3883056640625,
        "model.layers.0.mlp": 11.3883056640625,
        "model.layers.0.post_feedforward_layernorm": 54.92497634887695,
        "model.layers.0": 82.60246276855469,
        "model.layers.1.input_layernorm": 81.45690155029297,
        "model.layers.1.post_attention_layernorm": 21.527389526367188,
        "model.layers.1.pre_feedforward_layernorm": 70.7056884765625,
        "model.layers.1.mlp.gate_proj": 62.00424575805664,
        "model.layers.1.mlp.up_proj": 49.87368392944336,
        "model.layers.1.mlp.down_proj": 4.63247013092041,
        "model.layers.1.mlp": 4.63247013092041,
        "model.layers.1.post_feedforward_layernorm": 32.51662826538086,
        "model.layers.1": 77.22067260742188,
        "model.layers.2.input_layernorm": 75.01467895507812,
        "model.layers.2.post_attention_layernorm": 27.28493881225586,
        "model.layers.2.pre_feedforward_layernorm": 58.6871452331543
      }
    },
    {
      "sequence": "Explain database normalization. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.14211545819821,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 53.249290466308594,
        "model.layers.0.pre_feedforward_layernorm": 93.9050064086914,
        "model.layers.0.mlp.gate_proj": 83.69561004638672,
        "model.layers.0.mlp.up_proj": 60.73515319824219,
        "model.layers.0.mlp.down_proj": 11.594452857971191,
        "model.layers.0.mlp": 11.594452857971191,
        "model.layers.0.post_feedforward_layernorm": 55.629905700683594,
        "model.layers.0": 83.52503967285156,
        "model.layers.1.input_layernorm": 81.73412322998047,
        "model.layers.1.post_attention_layernorm": 21.419017791748047,
        "model.layers.1.pre_feedforward_layernorm": 68.95073699951172,
        "model.layers.1.mlp.gate_proj": 59.932655334472656,
        "model.layers.1.mlp.up_proj": 49.39825439453125,
        "model.layers.1.mlp.down_proj": 4.220114707946777,
        "model.layers.1.mlp": 4.220114707946777,
        "model.layers.1.post_feedforward_layernorm": 32.62350082397461,
        "model.layers.1": 78.33768463134766,
        "model.layers.2.input_layernorm": 74.96930694580078,
        "model.layers.2.post_attention_layernorm": 28.571468353271484,
        "model.layers.2.pre_feedforward_layernorm": 58.079124450683594
      }
    },
    {
      "sequence": "Explain ethical dilemmas. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.221869707107544,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 51.56964874267578,
        "model.layers.0.pre_feedforward_layernorm": 93.79047393798828,
        "model.layers.0.mlp.gate_proj": 80.36344146728516,
        "model.layers.0.mlp.up_proj": 60.32186508178711,
        "model.layers.0.mlp.down_proj": 11.296381950378418,
        "model.layers.0.mlp": 11.296381950378418,
        "model.layers.0.post_feedforward_layernorm": 54.69534683227539,
        "model.layers.0": 81.61466979980469,
        "model.layers.1.input_layernorm": 81.27151489257812,
        "model.layers.1.post_attention_layernorm": 22.467451095581055,
        "model.layers.1.pre_feedforward_layernorm": 72.00584411621094,
        "model.layers.1.mlp.gate_proj": 67.47498321533203,
        "model.layers.1.mlp.up_proj": 49.61550521850586,
        "model.layers.1.mlp.down_proj": 4.392950534820557,
        "model.layers.1.mlp": 4.392950534820557,
        "model.layers.1.post_feedforward_layernorm": 32.91753005981445,
        "model.layers.1": 76.2965087890625,
        "model.layers.2.input_layernorm": 75.33114624023438,
        "model.layers.2.post_attention_layernorm": 27.649478912353516,
        "model.layers.2.pre_feedforward_layernorm": 59.45528793334961
      }
    },
    {
      "sequence": "Compare and contrast these concepts. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.22813868522644,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 49.480743408203125,
        "model.layers.0.pre_feedforward_layernorm": 95.92581176757812,
        "model.layers.0.mlp.gate_proj": 82.25897979736328,
        "model.layers.0.mlp.up_proj": 60.33439254760742,
        "model.layers.0.mlp.down_proj": 11.640828132629395,
        "model.layers.0.mlp": 11.640828132629395,
        "model.layers.0.post_feedforward_layernorm": 55.79887390136719,
        "model.layers.0": 81.74492645263672,
        "model.layers.1.input_layernorm": 82.14602661132812,
        "model.layers.1.post_attention_layernorm": 20.66936683654785,
        "model.layers.1.pre_feedforward_layernorm": 72.12042999267578,
        "model.layers.1.mlp.gate_proj": 62.80788803100586,
        "model.layers.1.mlp.up_proj": 51.20327377319336,
        "model.layers.1.mlp.down_proj": 5.047555446624756,
        "model.layers.1.mlp": 5.047555446624756,
        "model.layers.1.post_feedforward_layernorm": 31.768022537231445,
        "model.layers.1": 76.59597778320312,
        "model.layers.2.input_layernorm": 75.94882202148438,
        "model.layers.2.post_attention_layernorm": 27.55893325805664,
        "model.layers.2.pre_feedforward_layernorm": 59.25957489013672
      }
    },
    {
      "sequence": "Explain film editing principles. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.27095430830251,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 52.42451858520508,
        "model.layers.0.pre_feedforward_layernorm": 95.02445983886719,
        "model.layers.0.mlp.gate_proj": 82.64981079101562,
        "model.layers.0.mlp.up_proj": 60.91044235229492,
        "model.layers.0.mlp.down_proj": 11.405794143676758,
        "model.layers.0.mlp": 11.405794143676758,
        "model.layers.0.post_feedforward_layernorm": 55.398502349853516,
        "model.layers.0": 81.53985595703125,
        "model.layers.1.input_layernorm": 81.78282165527344,
        "model.layers.1.post_attention_layernorm": 21.6590518951416,
        "model.layers.1.pre_feedforward_layernorm": 71.217529296875,
        "model.layers.1.mlp.gate_proj": 64.07868957519531,
        "model.layers.1.mlp.up_proj": 50.008907318115234,
        "model.layers.1.mlp.down_proj": 4.252344608306885,
        "model.layers.1.mlp": 4.252344608306885,
        "model.layers.1.post_feedforward_layernorm": 32.94621658325195,
        "model.layers.1": 76.75481414794922,
        "model.layers.2.input_layernorm": 75.16356658935547,
        "model.layers.2.post_attention_layernorm": 27.690279006958008,
        "model.layers.2.pre_feedforward_layernorm": 59.0843391418457
      }
    },
    {
      "sequence": "Describe the water cycle. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.301465459491894,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 50.92665481567383,
        "model.layers.0.pre_feedforward_layernorm": 96.12401580810547,
        "model.layers.0.mlp.gate_proj": 81.86759185791016,
        "model.layers.0.mlp.up_proj": 60.39980697631836,
        "model.layers.0.mlp.down_proj": 11.561702728271484,
        "model.layers.0.mlp": 11.561702728271484,
        "model.layers.0.post_feedforward_layernorm": 55.701900482177734,
        "model.layers.0": 82.46903991699219,
        "model.layers.1.input_layernorm": 81.99830627441406,
        "model.layers.1.post_attention_layernorm": 20.291519165039062,
        "model.layers.1.pre_feedforward_layernorm": 69.89363098144531,
        "model.layers.1.mlp.gate_proj": 62.67300796508789,
        "model.layers.1.mlp.up_proj": 49.81866455078125,
        "model.layers.1.mlp.down_proj": 4.769697189331055,
        "model.layers.1.mlp": 4.769697189331055,
        "model.layers.1.post_feedforward_layernorm": 33.63082504272461,
        "model.layers.1": 76.7231216430664,
        "model.layers.2.input_layernorm": 75.4148941040039,
        "model.layers.2.post_attention_layernorm": 29.27921485900879,
        "model.layers.2.pre_feedforward_layernorm": 60.476844787597656
      }
    },
    {
      "sequence": "Describe API design principles. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.31954675135405,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 49.88192367553711,
        "model.layers.0.pre_feedforward_layernorm": 97.27239227294922,
        "model.layers.0.mlp.gate_proj": 82.86402130126953,
        "model.layers.0.mlp.up_proj": 60.97261047363281,
        "model.layers.0.mlp.down_proj": 11.249954223632812,
        "model.layers.0.mlp": 11.249954223632812,
        "model.layers.0.post_feedforward_layernorm": 56.52286148071289,
        "model.layers.0": 82.49977111816406,
        "model.layers.1.input_layernorm": 82.36756896972656,
        "model.layers.1.post_attention_layernorm": 21.382606506347656,
        "model.layers.1.pre_feedforward_layernorm": 70.8839340209961,
        "model.layers.1.mlp.gate_proj": 63.13338088989258,
        "model.layers.1.mlp.up_proj": 50.195098876953125,
        "model.layers.1.mlp.down_proj": 4.264218807220459,
        "model.layers.1.mlp": 4.264218807220459,
        "model.layers.1.post_feedforward_layernorm": 31.06691551208496,
        "model.layers.1": 77.32830047607422,
        "model.layers.2.input_layernorm": 74.86067962646484,
        "model.layers.2.post_attention_layernorm": 28.918701171875,
        "model.layers.2.pre_feedforward_layernorm": 59.58859634399414
      }
    },
    {
      "sequence": "Explain calculus concepts. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.44525112276492,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 50.11882781982422,
        "model.layers.0.pre_feedforward_layernorm": 96.85161590576172,
        "model.layers.0.mlp.gate_proj": 81.51129913330078,
        "model.layers.0.mlp.up_proj": 61.006690979003906,
        "model.layers.0.mlp.down_proj": 11.772345542907715,
        "model.layers.0.mlp": 11.772345542907715,
        "model.layers.0.post_feedforward_layernorm": 55.787322998046875,
        "model.layers.0": 82.87439727783203,
        "model.layers.1.input_layernorm": 82.55577850341797,
        "model.layers.1.post_attention_layernorm": 21.719934463500977,
        "model.layers.1.pre_feedforward_layernorm": 71.00971221923828,
        "model.layers.1.mlp.gate_proj": 64.08033752441406,
        "model.layers.1.mlp.up_proj": 49.92377471923828,
        "model.layers.1.mlp.down_proj": 4.739063262939453,
        "model.layers.1.mlp": 4.739063262939453,
        "model.layers.1.post_feedforward_layernorm": 32.70564270019531,
        "model.layers.1": 78.15103912353516,
        "model.layers.2.input_layernorm": 75.54950714111328,
        "model.layers.2.post_attention_layernorm": 27.965307235717773,
        "model.layers.2.pre_feedforward_layernorm": 58.523128509521484
      }
    },
    {
      "sequence": "Please explain step by step. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.4574714017951,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 50.457847595214844,
        "model.layers.0.pre_feedforward_layernorm": 97.20339965820312,
        "model.layers.0.mlp.gate_proj": 84.74891662597656,
        "model.layers.0.mlp.up_proj": 61.178749084472656,
        "model.layers.0.mlp.down_proj": 11.762504577636719,
        "model.layers.0.mlp": 11.762504577636719,
        "model.layers.0.post_feedforward_layernorm": 56.66135025024414,
        "model.layers.0": 82.48263549804688,
        "model.layers.1.input_layernorm": 83.3671875,
        "model.layers.1.post_attention_layernorm": 21.40781021118164,
        "model.layers.1.pre_feedforward_layernorm": 69.42154693603516,
        "model.layers.1.mlp.gate_proj": 63.5876579284668,
        "model.layers.1.mlp.up_proj": 50.14405822753906,
        "model.layers.1.mlp.down_proj": 5.073610782623291,
        "model.layers.1.mlp": 5.073610782623291,
        "model.layers.1.post_feedforward_layernorm": 31.81524085998535,
        "model.layers.1": 78.03209686279297,
        "model.layers.2.input_layernorm": 75.24407958984375,
        "model.layers.2.post_attention_layernorm": 27.7796630859375,
        "model.layers.2.pre_feedforward_layernorm": 57.068992614746094
      }
    },
    {
      "sequence": "I need detailed information about this topic. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.50886611316515,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.063426971435547,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 49.75717544555664,
        "model.layers.0.pre_feedforward_layernorm": 97.66046142578125,
        "model.layers.0.mlp.gate_proj": 84.3730697631836,
        "model.layers.0.mlp.up_proj": 60.95002365112305,
        "model.layers.0.mlp.down_proj": 11.530440330505371,
        "model.layers.0.mlp": 11.530440330505371,
        "model.layers.0.post_feedforward_layernorm": 56.15534973144531,
        "model.layers.0": 82.21087646484375,
        "model.layers.1.input_layernorm": 83.34666442871094,
        "model.layers.1.post_attention_layernorm": 19.68401336669922,
        "model.layers.1.pre_feedforward_layernorm": 72.185546875,
        "model.layers.1.mlp.gate_proj": 67.49458312988281,
        "model.layers.1.mlp.up_proj": 51.092384338378906,
        "model.layers.1.mlp.down_proj": 5.143531799316406,
        "model.layers.1.mlp": 5.143531799316406,
        "model.layers.1.post_feedforward_layernorm": 31.430591583251953,
        "model.layers.1": 76.6297607421875,
        "model.layers.2.input_layernorm": 75.25926208496094,
        "model.layers.2.post_attention_layernorm": 26.51538848876953,
        "model.layers.2.pre_feedforward_layernorm": 57.043670654296875
      }
    },
    {
      "sequence": "Translate: Bonjour means hello in French. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.55693017918131,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.063426971435547,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 56.02366638183594,
        "model.layers.0.pre_feedforward_layernorm": 93.66730499267578,
        "model.layers.0.mlp.gate_proj": 90.01528930664062,
        "model.layers.0.mlp.up_proj": 61.98350143432617,
        "model.layers.0.mlp.down_proj": 11.977677345275879,
        "model.layers.0.mlp": 11.977677345275879,
        "model.layers.0.post_feedforward_layernorm": 55.639366149902344,
        "model.layers.0": 81.14938354492188,
        "model.layers.1.input_layernorm": 83.23713684082031,
        "model.layers.1.post_attention_layernorm": 19.40328025817871,
        "model.layers.1.pre_feedforward_layernorm": 71.48443603515625,
        "model.layers.1.mlp.gate_proj": 66.80037689208984,
        "model.layers.1.mlp.up_proj": 51.934295654296875,
        "model.layers.1.mlp.down_proj": 4.498624324798584,
        "model.layers.1.mlp": 4.498624324798584,
        "model.layers.1.post_feedforward_layernorm": 29.640254974365234,
        "model.layers.1": 75.64373016357422,
        "model.layers.2.input_layernorm": 73.92475128173828,
        "model.layers.2.post_attention_layernorm": 26.452423095703125,
        "model.layers.2.pre_feedforward_layernorm": 56.29043960571289
      }
    },
    {
      "sequence": "Describe free will vs determinism. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.61267945040827,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 47.76301193237305,
        "model.layers.0.pre_feedforward_layernorm": 100.2714614868164,
        "model.layers.0.mlp.gate_proj": 83.50997924804688,
        "model.layers.0.mlp.up_proj": 61.73421096801758,
        "model.layers.0.mlp.down_proj": 11.861992835998535,
        "model.layers.0.mlp": 11.861992835998535,
        "model.layers.0.post_feedforward_layernorm": 56.83656692504883,
        "model.layers.0": 81.8054428100586,
        "model.layers.1.input_layernorm": 82.37889099121094,
        "model.layers.1.post_attention_layernorm": 23.258251190185547,
        "model.layers.1.pre_feedforward_layernorm": 71.55740356445312,
        "model.layers.1.mlp.gate_proj": 66.35413360595703,
        "model.layers.1.mlp.up_proj": 50.90516662597656,
        "model.layers.1.mlp.down_proj": 4.626423358917236,
        "model.layers.1.mlp": 4.626423358917236,
        "model.layers.1.post_feedforward_layernorm": 31.81709098815918,
        "model.layers.1": 76.44794464111328,
        "model.layers.2.input_layernorm": 73.96541595458984,
        "model.layers.2.post_attention_layernorm": 27.587146759033203,
        "model.layers.2.pre_feedforward_layernorm": 58.19855880737305
      }
    },
    {
      "sequence": "Explain probability theory. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.65666487942571,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.3799147605896,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 50.58782958984375,
        "model.layers.0.pre_feedforward_layernorm": 96.48207092285156,
        "model.layers.0.mlp.gate_proj": 82.15975189208984,
        "model.layers.0.mlp.up_proj": 60.859039306640625,
        "model.layers.0.mlp.down_proj": 11.994682312011719,
        "model.layers.0.mlp": 11.994682312011719,
        "model.layers.0.post_feedforward_layernorm": 55.87010192871094,
        "model.layers.0": 85.00408935546875,
        "model.layers.1.input_layernorm": 82.91394805908203,
        "model.layers.1.post_attention_layernorm": 23.647558212280273,
        "model.layers.1.pre_feedforward_layernorm": 70.57752990722656,
        "model.layers.1.mlp.gate_proj": 62.92729187011719,
        "model.layers.1.mlp.up_proj": 49.74103927612305,
        "model.layers.1.mlp.down_proj": 4.318140029907227,
        "model.layers.1.mlp": 4.318140029907227,
        "model.layers.1.post_feedforward_layernorm": 32.266387939453125,
        "model.layers.1": 79.2247085571289,
        "model.layers.2.input_layernorm": 75.22723388671875,
        "model.layers.2.post_attention_layernorm": 28.965185165405273,
        "model.layers.2.pre_feedforward_layernorm": 59.14023971557617
      }
    },
    {
      "sequence": "Describe impressionist painting techniques. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.680225507072784,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 4.744651794433594,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 50.87143325805664,
        "model.layers.0.pre_feedforward_layernorm": 96.81636047363281,
        "model.layers.0.mlp.gate_proj": 83.701904296875,
        "model.layers.0.mlp.up_proj": 61.07972717285156,
        "model.layers.0.mlp.down_proj": 11.923079490661621,
        "model.layers.0.mlp": 11.923079490661621,
        "model.layers.0.post_feedforward_layernorm": 55.801902770996094,
        "model.layers.0": 81.84947967529297,
        "model.layers.1.input_layernorm": 81.8950424194336,
        "model.layers.1.post_attention_layernorm": 21.525075912475586,
        "model.layers.1.pre_feedforward_layernorm": 73.14781951904297,
        "model.layers.1.mlp.gate_proj": 65.95256805419922,
        "model.layers.1.mlp.up_proj": 50.79054641723633,
        "model.layers.1.mlp.down_proj": 4.511044979095459,
        "model.layers.1.mlp": 4.511044979095459,
        "model.layers.1.post_feedforward_layernorm": 32.16200256347656,
        "model.layers.1": 76.13508605957031,
        "model.layers.2.input_layernorm": 75.47269439697266,
        "model.layers.2.post_attention_layernorm": 28.392372131347656,
        "model.layers.2.pre_feedforward_layernorm": 60.93454360961914
      }
    },
    {
      "sequence": "Explain the entire process of how a computer processes a program from source code to execution, including compilation, memory management, and CPU operations. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.88401069848434,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 4.8316240310668945,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 55.40847396850586,
        "model.layers.0.pre_feedforward_layernorm": 94.6397705078125,
        "model.layers.0.mlp.gate_proj": 91.0676498413086,
        "model.layers.0.mlp.up_proj": 62.30634307861328,
        "model.layers.0.mlp.down_proj": 13.08297061920166,
        "model.layers.0.mlp": 13.08297061920166,
        "model.layers.0.post_feedforward_layernorm": 55.09064483642578,
        "model.layers.0": 81.29888916015625,
        "model.layers.1.input_layernorm": 83.22296142578125,
        "model.layers.1.post_attention_layernorm": 20.39895248413086,
        "model.layers.1.pre_feedforward_layernorm": 71.63226318359375,
        "model.layers.1.mlp.gate_proj": 68.46765899658203,
        "model.layers.1.mlp.up_proj": 52.441246032714844,
        "model.layers.1.mlp.down_proj": 4.537328720092773,
        "model.layers.1.mlp": 4.537328720092773,
        "model.layers.1.post_feedforward_layernorm": 27.62811279296875,
        "model.layers.1": 76.60055541992188,
        "model.layers.2.input_layernorm": 73.55271911621094,
        "model.layers.2.post_attention_layernorm": 26.786916732788086,
        "model.layers.2.pre_feedforward_layernorm": 58.213138580322266
      }
    },
    {
      "sequence": "Describe the process of machine learning. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.90347233025924,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 52.09492111206055,
        "model.layers.0.pre_feedforward_layernorm": 97.13301086425781,
        "model.layers.0.mlp.gate_proj": 86.70314025878906,
        "model.layers.0.mlp.up_proj": 61.14596939086914,
        "model.layers.0.mlp.down_proj": 11.776065826416016,
        "model.layers.0.mlp": 11.776065826416016,
        "model.layers.0.post_feedforward_layernorm": 56.177818298339844,
        "model.layers.0": 82.42455291748047,
        "model.layers.1.input_layernorm": 82.58541107177734,
        "model.layers.1.post_attention_layernorm": 21.935462951660156,
        "model.layers.1.pre_feedforward_layernorm": 71.12438201904297,
        "model.layers.1.mlp.gate_proj": 65.48027801513672,
        "model.layers.1.mlp.up_proj": 50.818172454833984,
        "model.layers.1.mlp.down_proj": 4.89539098739624,
        "model.layers.1.mlp": 4.89539098739624,
        "model.layers.1.post_feedforward_layernorm": 31.644479751586914,
        "model.layers.1": 77.25926208496094,
        "model.layers.2.input_layernorm": 75.11051940917969,
        "model.layers.2.post_attention_layernorm": 29.398597717285156,
        "model.layers.2.pre_feedforward_layernorm": 59.67685317993164
      }
    },
    {
      "sequence": "Describe the complete lifecycle of a star from formation to death, including all stages and physical processes involved. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 50.94507282713185,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.0698676109313965,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 53.547183990478516,
        "model.layers.0.pre_feedforward_layernorm": 95.06329345703125,
        "model.layers.0.mlp.gate_proj": 87.30756378173828,
        "model.layers.0.mlp.up_proj": 61.86058044433594,
        "model.layers.0.mlp.down_proj": 12.981707572937012,
        "model.layers.0.mlp": 12.981707572937012,
        "model.layers.0.post_feedforward_layernorm": 55.50069808959961,
        "model.layers.0": 81.38174438476562,
        "model.layers.1.input_layernorm": 82.6040267944336,
        "model.layers.1.post_attention_layernorm": 20.441640853881836,
        "model.layers.1.pre_feedforward_layernorm": 73.7620620727539,
        "model.layers.1.mlp.gate_proj": 70.86344146728516,
        "model.layers.1.mlp.up_proj": 52.78679656982422,
        "model.layers.1.mlp.down_proj": 4.874359130859375,
        "model.layers.1.mlp": 4.874359130859375,
        "model.layers.1.post_feedforward_layernorm": 28.10926628112793,
        "model.layers.1": 76.63365936279297,
        "model.layers.2.input_layernorm": 74.02017974853516,
        "model.layers.2.post_attention_layernorm": 26.649139404296875,
        "model.layers.2.pre_feedforward_layernorm": 58.91967010498047
      }
    },
    {
      "sequence": "Prove the Pythagorean theorem. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 51.02997411852298,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 53.366310119628906,
        "model.layers.0.pre_feedforward_layernorm": 95.157958984375,
        "model.layers.0.mlp.gate_proj": 84.88919067382812,
        "model.layers.0.mlp.up_proj": 61.773193359375,
        "model.layers.0.mlp.down_proj": 12.4671049118042,
        "model.layers.0.mlp": 12.4671049118042,
        "model.layers.0.post_feedforward_layernorm": 55.576412200927734,
        "model.layers.0": 83.47824096679688,
        "model.layers.1.input_layernorm": 83.57945251464844,
        "model.layers.1.post_attention_layernorm": 21.83527374267578,
        "model.layers.1.pre_feedforward_layernorm": 70.96713256835938,
        "model.layers.1.mlp.gate_proj": 66.81563568115234,
        "model.layers.1.mlp.up_proj": 50.035369873046875,
        "model.layers.1.mlp.down_proj": 4.141941070556641,
        "model.layers.1.mlp": 4.141941070556641,
        "model.layers.1.post_feedforward_layernorm": 32.52845001220703,
        "model.layers.1": 78.5522232055664,
        "model.layers.2.input_layernorm": 75.55655670166016,
        "model.layers.2.post_attention_layernorm": 29.07870864868164,
        "model.layers.2.pre_feedforward_layernorm": 60.699337005615234
      }
    },
    {
      "sequence": "Describe ancient Egyptian civilization. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 51.05706842049308,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 51.083251953125,
        "model.layers.0.pre_feedforward_layernorm": 97.54165649414062,
        "model.layers.0.mlp.gate_proj": 85.38601684570312,
        "model.layers.0.mlp.up_proj": 61.76727294921875,
        "model.layers.0.mlp.down_proj": 12.062966346740723,
        "model.layers.0.mlp": 12.062966346740723,
        "model.layers.0.post_feedforward_layernorm": 55.990386962890625,
        "model.layers.0": 81.92221069335938,
        "model.layers.1.input_layernorm": 82.13178253173828,
        "model.layers.1.post_attention_layernorm": 22.204145431518555,
        "model.layers.1.pre_feedforward_layernorm": 74.18131256103516,
        "model.layers.1.mlp.gate_proj": 69.29954528808594,
        "model.layers.1.mlp.up_proj": 51.05837631225586,
        "model.layers.1.mlp.down_proj": 4.452459335327148,
        "model.layers.1.mlp": 4.452459335327148,
        "model.layers.1.post_feedforward_layernorm": 32.860084533691406,
        "model.layers.1": 76.6852798461914,
        "model.layers.2.input_layernorm": 74.57252502441406,
        "model.layers.2.post_attention_layernorm": 28.505220413208008,
        "model.layers.2.pre_feedforward_layernorm": 59.51078796386719
      }
    },
    {
      "sequence": "Explain the Renaissance period. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 51.08661826797154,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.078139305114746,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 50.232295989990234,
        "model.layers.0.pre_feedforward_layernorm": 97.93717956542969,
        "model.layers.0.mlp.gate_proj": 83.03245544433594,
        "model.layers.0.mlp.up_proj": 61.53581237792969,
        "model.layers.0.mlp.down_proj": 11.866710662841797,
        "model.layers.0.mlp": 11.866710662841797,
        "model.layers.0.post_feedforward_layernorm": 55.9005241394043,
        "model.layers.0": 81.78253936767578,
        "model.layers.1.input_layernorm": 82.45792388916016,
        "model.layers.1.post_attention_layernorm": 23.290468215942383,
        "model.layers.1.pre_feedforward_layernorm": 73.89208221435547,
        "model.layers.1.mlp.gate_proj": 71.14376831054688,
        "model.layers.1.mlp.up_proj": 50.84477615356445,
        "model.layers.1.mlp.down_proj": 4.408309459686279,
        "model.layers.1.mlp": 4.408309459686279,
        "model.layers.1.post_feedforward_layernorm": 33.06112289428711,
        "model.layers.1": 77.08778381347656,
        "model.layers.2.input_layernorm": 75.82366943359375,
        "model.layers.2.post_attention_layernorm": 27.649972915649414,
        "model.layers.2.pre_feedforward_layernorm": 60.18793869018555
      }
    },
    {
      "sequence": "Write Python code to sort a list. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 51.09190199686133,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.063426971435547,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 50.090999603271484,
        "model.layers.0.pre_feedforward_layernorm": 100.08230590820312,
        "model.layers.0.mlp.gate_proj": 86.05056762695312,
        "model.layers.0.mlp.up_proj": 62.59425735473633,
        "model.layers.0.mlp.down_proj": 12.2605562210083,
        "model.layers.0.mlp": 12.2605562210083,
        "model.layers.0.post_feedforward_layernorm": 57.39374542236328,
        "model.layers.0": 82.49258422851562,
        "model.layers.1.input_layernorm": 84.96588897705078,
        "model.layers.1.post_attention_layernorm": 19.105609893798828,
        "model.layers.1.pre_feedforward_layernorm": 70.58842468261719,
        "model.layers.1.mlp.gate_proj": 68.3852310180664,
        "model.layers.1.mlp.up_proj": 51.23243713378906,
        "model.layers.1.mlp.down_proj": 4.695850849151611,
        "model.layers.1.mlp": 4.695850849151611,
        "model.layers.1.post_feedforward_layernorm": 31.799617767333984,
        "model.layers.1": 78.84353637695312,
        "model.layers.2.input_layernorm": 75.74824523925781,
        "model.layers.2.post_attention_layernorm": 27.443645477294922,
        "model.layers.2.pre_feedforward_layernorm": 57.816680908203125
      }
    },
    {
      "sequence": "Analyze Shakespeare's writing style. !!! ???",
      "variation_type": "special_chars",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 51.15855499972468,
      "layer_variances": {
        "model.embed_tokens": 2.8002970218658447,
        "model.rotary_emb": 5.220391273498535,
        "model.layers.0.input_layernorm": 128.70343017578125,
        "model.layers.0.post_attention_layernorm": 54.68638229370117,
        "model.layers.0.pre_feedforward_layernorm": 94.90824890136719,
        "model.layers.0.mlp.gate_proj": 84.55770874023438,
        "model.layers.0.mlp.up_proj": 61.507537841796875,
        "model.layers.0.mlp.down_proj": 11.9484224319458,
        "model.layers.0.mlp": 11.9484224319458,
        "model.layers.0.post_feedforward_layernorm": 55.04463577270508,
        "model.layers.0": 80.89911651611328,
        "model.layers.1.input_layernorm": 83.32328033447266,
        "model.layers.1.post_attention_layernorm": 22.088607788085938,
        "model.layers.1.pre_feedforward_layernorm": 74.31497955322266,
        "model.layers.1.mlp.gate_proj": 71.78492736816406,
        "model.layers.1.mlp.up_proj": 52.359012603759766,
        "model.layers.1.mlp.down_proj": 4.539404392242432,
        "model.layers.1.mlp": 4.539404392242432,
        "model.layers.1.post_feedforward_layernorm": 30.807445526123047,
        "model.layers.1": 76.92772674560547,
        "model.layers.2.input_layernorm": 76.14506530761719,
        "model.layers.2.post_attention_layernorm": 27.60304069519043,
        "model.layers.2.pre_feedforward_layernorm": 59.98927688598633
      }
    },
    {
      "sequence": "Repeat this word: hello hello hello hello          ",
      "variation_type": "extra_spaces",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 51.855467931084014,
      "layer_variances": {
        "model.embed_tokens": 2.375624895095825,
        "model.rotary_emb": 2.8216638565063477,
        "model.layers.0.input_layernorm": 101.31523132324219,
        "model.layers.0.post_attention_layernorm": 35.092227935791016,
        "model.layers.0.pre_feedforward_layernorm": 83.44630432128906,
        "model.layers.0.mlp.gate_proj": 71.96513366699219,
        "model.layers.0.mlp.up_proj": 60.753082275390625,
        "model.layers.0.mlp.down_proj": 10.921857833862305,
        "model.layers.0.mlp": 10.921857833862305,
        "model.layers.0.post_feedforward_layernorm": 54.74716567993164,
        "model.layers.0": 106.51406860351562,
        "model.layers.1.input_layernorm": 97.3046646118164,
        "model.layers.1.post_attention_layernorm": 16.9438419342041,
        "model.layers.1.pre_feedforward_layernorm": 79.89678192138672,
        "model.layers.1.mlp.gate_proj": 62.84914016723633,
        "model.layers.1.mlp.up_proj": 61.2128791809082,
        "model.layers.1.mlp.down_proj": 4.442221641540527,
        "model.layers.1.mlp": 4.442221641540527,
        "model.layers.1.post_feedforward_layernorm": 38.96506118774414,
        "model.layers.1": 102.52376556396484,
        "model.layers.2.input_layernorm": 86.62895965576172,
        "model.layers.2.post_attention_layernorm": 34.84634780883789,
        "model.layers.2.pre_feedforward_layernorm": 61.74565887451172
      }
    },
    {
      "sequence": "Repeat this word: hello hello hello hello Please respond carefully.",
      "variation_type": "suffix_noise",
      "target_layers": [
        "model.embed_tokens",
        "model.rotary_emb",
        "model.layers.0.input_layernorm",
        "model.layers.0.post_attention_layernorm",
        "model.layers.0.pre_feedforward_layernorm",
        "model.layers.0.mlp.gate_proj",
        "model.layers.0.mlp.up_proj",
        "model.layers.0.mlp.down_proj",
        "model.layers.0.mlp",
        "model.layers.0.post_feedforward_layernorm",
        "model.layers.0",
        "model.layers.1.input_layernorm",
        "model.layers.1.post_attention_layernorm",
        "model.layers.1.pre_feedforward_layernorm",
        "model.layers.1.mlp.gate_proj",
        "model.layers.1.mlp.up_proj",
        "model.layers.1.mlp.down_proj",
        "model.layers.1.mlp",
        "model.layers.1.post_feedforward_layernorm",
        "model.layers.1",
        "model.layers.2.input_layernorm",
        "model.layers.2.post_attention_layernorm",
        "model.layers.2.pre_feedforward_layernorm"
      ],
      "variance": 52.86950689813365,
      "layer_variances": {
        "model.embed_tokens": 3.0209407806396484,
        "model.rotary_emb": 7.646179676055908,
        "model.layers.0.input_layernorm": 125.04186248779297,
        "model.layers.0.post_attention_layernorm": 51.663780212402344,
        "model.layers.0.pre_feedforward_layernorm": 96.61446380615234,
        "model.layers.0.mlp.gate_proj": 80.43171691894531,
        "model.layers.0.mlp.up_proj": 62.84410858154297,
        "model.layers.0.mlp.down_proj": 11.427106857299805,
        "model.layers.0.mlp": 11.427106857299805,
        "model.layers.0.post_feedforward_layernorm": 54.22392654418945,
        "model.layers.0": 98.03169250488281,
        "model.layers.1.input_layernorm": 92.251220703125,
        "model.layers.1.post_attention_layernorm": 20.875988006591797,
        "model.layers.1.pre_feedforward_layernorm": 73.09109497070312,
        "model.layers.1.mlp.gate_proj": 63.01865768432617,
        "model.layers.1.mlp.up_proj": 54.66446304321289,
        "model.layers.1.mlp.down_proj": 5.458554744720459,
        "model.layers.1.mlp": 5.458554744720459,
        "model.layers.1.post_feedforward_layernorm": 34.35140609741211,
        "model.layers.1": 89.71622467041016,
        "model.layers.2.input_layernorm": 81.91309356689453,
        "model.layers.2.post_attention_layernorm": 36.602474212646484,
        "model.layers.2.pre_feedforward_layernorm": 56.22404098510742
      }
    }
  ],
  "target_layers": [
    "model.embed_tokens",
    "model.rotary_emb",
    "model.layers.0.input_layernorm",
    "model.layers.0.post_attention_layernorm",
    "model.layers.0.pre_feedforward_layernorm",
    "model.layers.0.mlp.gate_proj",
    "model.layers.0.mlp.up_proj",
    "model.layers.0.mlp.down_proj",
    "model.layers.0.mlp",
    "model.layers.0.post_feedforward_layernorm",
    "model.layers.0",
    "model.layers.1.input_layernorm",
    "model.layers.1.post_attention_layernorm",
    "model.layers.1.pre_feedforward_layernorm",
    "model.layers.1.mlp.gate_proj",
    "model.layers.1.mlp.up_proj",
    "model.layers.1.mlp.down_proj",
    "model.layers.1.mlp",
    "model.layers.1.post_feedforward_layernorm",
    "model.layers.1",
    "model.layers.2.input_layernorm",
    "model.layers.2.post_attention_layernorm",
    "model.layers.2.pre_feedforward_layernorm"
  ]
}